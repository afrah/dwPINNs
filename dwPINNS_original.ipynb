{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_16603/3478387952.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n",
      "xxxxxxxxxxxxxx\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 20)           80          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20)           420         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 20)           420         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 20)           420         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 20)           420         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 20)           420         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 20)           420         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 3)            63          dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1)]          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 1)]          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 1)]          0           dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,663\n",
      "Trainable params: 2,663\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 14:50:10.109471: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-29 14:50:10.133362: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-11-29 14:50:10.134132: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562451f2c7e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-29 14:50:10.134183: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-11-29 14:50:10.136483: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Oct 24 13:02:32 2021\n",
    "\n",
    "@author: lenovo\n",
    "\"\"\"\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "import math\n",
    "import matplotlib.gridspec as gridspec\n",
    "from plotting import newfig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import layers, activations\n",
    "from scipy.interpolate import griddata\n",
    "from eager_lbfgs import lbfgs, Struct\n",
    "from pyDOE import lhs\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "layer_sizes = [3, 20, 20, 20, 20, 20, 20, 20, 3]\n",
    "sizes_w = []\n",
    "sizes_b = []\n",
    "for i, width in enumerate(layer_sizes):\n",
    "    if i != 1:\n",
    "        sizes_w.append(int(width * layer_sizes[1]))\n",
    "        sizes_b.append(int(width if i != 0 else layer_sizes[1]))\n",
    "\n",
    "\n",
    "# L-BFGS weight getting and setting from https://github.com/pierremtb/PINNs-TF2.0\n",
    "\n",
    "def set_weights(model, w, sizes_w, sizes_b):  # 重新设置参数\n",
    "\n",
    "    for i, layer in enumerate(model.layers[1:len(sizes_w) + 1]):\n",
    "        start_weights = sum(sizes_w[:i]) + sum(sizes_b[:i])\n",
    "        end_weights = sum(sizes_w[:i + 1]) + sum(sizes_b[:i])\n",
    "        weights = w[start_weights:end_weights]\n",
    "        w_div = int(sizes_w[i] / sizes_b[i])\n",
    "        weights = tf.reshape(weights, [w_div, sizes_b[i]])\n",
    "        biases = w[end_weights:end_weights + sizes_b[i]]\n",
    "        weights_biases = [weights, biases]\n",
    "        layer.set_weights(weights_biases)\n",
    "\n",
    "\n",
    "def get_weights(model):\n",
    "    w = []\n",
    "    for layer in model.layers[1:len(sizes_w) + 1]:\n",
    "        weights_biases = layer.get_weights()\n",
    "        weights = weights_biases[0].flatten()\n",
    "        biases = weights_biases[1]\n",
    "        w.extend(weights)\n",
    "        w.extend(biases)\n",
    "    w = tf.convert_to_tensor(w)\n",
    "    return w\n",
    "\n",
    "def xavier_init(layer_sizes):\n",
    "    in_dim = layer_sizes[0]\n",
    "    out_dim = layer_sizes[1]\n",
    "    xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "    return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "\n",
    "def neural_net(layer_sizes):\n",
    "\n",
    "    input_tensor = keras.Input(shape=(layer_sizes[0],))\n",
    "\n",
    "    hide_layer_list = []\n",
    "    flag = True\n",
    "    for width in layer_sizes[1:-1]:\n",
    "        if flag:\n",
    "            x = layers.Dense(\n",
    "                width, activation=tf.nn.tanh,\n",
    "                kernel_initializer=\"glorot_normal\")(input_tensor)\n",
    "            flag = False\n",
    "        else:\n",
    "            x = layers.Dense(\n",
    "                width, activation=tf.nn.tanh,\n",
    "                kernel_initializer=\"glorot_normal\")(x)\n",
    "    output_tensor = layers.Dense(layer_sizes[-1], activation=None,kernel_initializer=\"glorot_normal\")(x)\n",
    "    print(\"xxxxxxxxxxxxxx\")\n",
    "    output0 = output_tensor[:, 0:1]\n",
    "    output1 = output_tensor[:, 1:2]\n",
    "    output2 = output_tensor[:, 2:3]\n",
    "\n",
    "    model_output = keras.models.Model(input_tensor, [output0, output1, output2])\n",
    "\n",
    "    return model_output\n",
    "\n",
    "# initialize the NN\n",
    "u_model = neural_net(layer_sizes)\n",
    "# view the NN\n",
    "u_model.summary()\n",
    "\n",
    "\n",
    "# define the loss\n",
    "def loss(x_f_batch, y_f_batch, t_f_batch, xb, yb, tb, ub, vb, weight_ub,  weight_fu):\n",
    "\n",
    "    f_u_pred, f_v_pred, div_pred = f_model(x_f_batch, y_f_batch, t_f_batch)\n",
    "\n",
    "\n",
    "    u_pred, v_pred, p_pred = u_model(tf.concat([xb, yb, tb], 1))\n",
    "    mse_b = 100*weight_ub*(tf.reduce_sum(tf.square(u_pred - ub)) + tf.reduce_sum(tf.square(v_pred - vb)))\n",
    "    mse_f = weight_fu*(tf.reduce_sum(tf.square(f_u_pred)) + tf.reduce_sum(tf.square(f_v_pred)) + tf.reduce_sum(tf.square(div_pred)))\n",
    "\n",
    "    return mse_b + mse_f, mse_b, mse_f\n",
    "\n",
    "@tf.function\n",
    "def f_model(x, y, t):\n",
    "    u, v, p = u_model(tf.concat([x, y, t],1))\n",
    "\n",
    "    u_t = tf.gradients(u, t)[0]\n",
    "    u_x = tf.gradients(u, x)[0]\n",
    "    u_y = tf.gradients(u, y)[0]\n",
    "    u_xx = tf.gradients(u_x, x)[0]\n",
    "    u_yy = tf.gradients(u_y, y)[0]\n",
    "\n",
    "    v_t = tf.gradients(v, t)[0]\n",
    "    v_x = tf.gradients(v, x)[0]\n",
    "    v_y = tf.gradients(v, y)[0]\n",
    "    v_xx = tf.gradients(v_x, x)[0]\n",
    "    v_yy = tf.gradients(v_y, y)[0]\n",
    "\n",
    "    p_x = tf.gradients(p, x)[0]\n",
    "    p_y = tf.gradients(p, y)[0]\n",
    "\n",
    "    div = u_x + v_y\n",
    "    c1 = tf.constant(0.01, dtype=tf.float32)\n",
    "    f_u = u_t + u*u_x + v*u_y + p_x -c1*(u_xx + u_yy) - ((np.pi)*tf.cos(np.pi*x)*tf.cos(np.pi*y)*tf.sin(t) - tf.cos(np.pi*y)*(tf.sin(np.pi*x))**2*tf.sin(np.pi*y)*tf.cos(t) + \\\n",
    "    c1*(2*(np.pi)**2*(tf.cos(np.pi*x))**2*tf.cos(np.pi*y)*tf.sin(np.pi*y)*tf.sin(t) - 6*(np.pi)**2*tf.cos(np.pi*y)*(tf.sin(np.pi*x))**2*tf.sin(np.pi*y)*tf.sin(t)) - \\\n",
    "    tf.cos(np.pi*x)*tf.sin(np.pi*x)*(tf.sin(np.pi*y))**2*tf.sin(t)*(np.pi*(tf.cos(np.pi*y))**2*(tf.sin(np.pi*x))**2*tf.sin(t) - np.pi*(tf.sin(np.pi*x))**2*(tf.sin(np.pi*y))**2*tf.sin(t)) +\\\n",
    "    2*np.pi*tf.cos(np.pi*x)*(tf.cos(np.pi*y))**2*(tf.sin(np.pi*x))**3*(tf.sin(np.pi*y))**2*(tf.sin(t))**2)\n",
    "\n",
    "    f_v = v_t + u*v_x + v*v_y + p_y - c1*(v_xx + v_yy) - (tf.cos(np.pi*x)*tf.sin(np.pi*x)*(tf.sin(np.pi*y))**2*tf.cos(t) - np.pi*tf.sin(np.pi*x)*tf.sin(np.pi*y)*tf.sin(t) - \\\n",
    "    c1*(2*(np.pi)**2*tf.cos(np.pi*x)*(tf.cos(np.pi*y))**2*tf.sin(np.pi*x)*tf.sin(t) - 6*(np.pi)**2*tf.cos(np.pi*x)*tf.sin(np.pi*x)*(tf.sin(np.pi*y))**2*tf.sin(t)) -\\\n",
    "    tf.cos(np.pi*y)*(tf.sin(np.pi*x))**2*tf.sin(np.pi*y)*tf.sin(t)*(np.pi*(tf.cos(np.pi*x))**2*(tf.sin(np.pi*y))**2*tf.sin(t) -\\\n",
    "    np.pi*(tf.sin(np.pi*x))**2*(tf.sin(np.pi*y))**2*tf.sin(t)) + 2*np.pi*(tf.cos(np.pi*x))**2*tf.cos(np.pi*y)*(tf.sin(np.pi*x))**2*(tf.sin(np.pi*y))**3*(tf.sin(t))**2)\n",
    "\n",
    "    return f_u, f_v, div\n",
    "\n",
    "@tf.function\n",
    "def u_x_model(x, y, t):\n",
    "    u, v, w = u_model(tf.concat([x, y, t], 1))\n",
    "    return u, v, w\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def grad(u_model, x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch, vb_batch, weight_ub, weight_fu):\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        loss_value, mse_b, mse_f = loss(x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch, vb_batch, weight_ub, weight_fu)\n",
    "        \n",
    "        grads = tape.gradient(loss_value, u_model.trainable_variables)\n",
    "\n",
    "        grads_ub = tape.gradient(loss_value, weight_ub)\n",
    "\n",
    "        grads_fu = tape.gradient(loss_value, weight_fu)\n",
    "\n",
    "    return loss_value, mse_b, mse_f, grads, grads_ub, grads_fu\n",
    "\n",
    "\n",
    "def fit(x_f, y_f, t_f, xb, yb, tb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star, tf_iter, tf_iter2,\n",
    "        newton_iter1, newton_iter2):\n",
    "\n",
    "    batch_sz = N_f\n",
    "    n_batches = N_f // batch_sz\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    tf_optimizer = tf.keras.optimizers.Adam(lr=0.005, beta_1=.99)\n",
    "    tf_optimizer_weights = tf.keras.optimizers.Adam(lr=0.003, beta_1=.99)\n",
    "    tf_optimizer_u = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
    "\n",
    "    print(f\"weight_ub: {weight_ub.numpy()}  weight_fu: {weight_fu.numpy()}\")\n",
    "    print(\"starting Adam training\")\n",
    "\n",
    "    a = np.random.rand(1000)\n",
    "    loss_history = list(a)\n",
    "    MSE_b0 = list(a)\n",
    "    MSE_f0 = list(a)\n",
    "\n",
    "    MSE_b1 = []\n",
    "    MSE_f1 = []\n",
    "\n",
    "    weightu = []\n",
    "    weightf = []\n",
    "    # For mini-batch (if used)\n",
    "    for epoch in range(tf_iter):\n",
    "        for i in range(n_batches):\n",
    "            xb_batch = xb\n",
    "            yb_batch = yb\n",
    "            tb_batch = tb\n",
    "            ub_batch = ub\n",
    "            vb_batch = vb\n",
    "\n",
    "            x_f_batch = x_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
    "            y_f_batch = y_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
    "            t_f_batch = t_f[i * batch_sz:(i * batch_sz + batch_sz), ]\n",
    "\n",
    "            loss_value, mse_b, mse_f, grads, grads_ub, grads_fu = grad(u_model, x_f_batch, y_f_batch, t_f_batch,\n",
    "                                                                       xb_batch, yb_batch,\n",
    "                                                                       tb_batch, ub_batch, vb_batch, weight_ub,\n",
    "                                                                       weight_fu)\n",
    "\n",
    "            tf_optimizer.apply_gradients(zip(grads, u_model.trainable_variables))\n",
    "            MSE_b0.append(mse_b)\n",
    "            MSE_f0.append(mse_f)\n",
    "\n",
    "            loss_history.append(loss_value)\n",
    "            \n",
    "            if loss_history[-1] < loss_history[-2] and loss_history[-2] < loss_history[-3] and loss_history[-1] < loss_history[-10]:\n",
    "                tf_optimizer_weights.apply_gradients(zip([-grads_fu], [weight_fu]))\n",
    "                tf_optimizer_u.apply_gradients(zip([-grads_ub], [weight_ub]))\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            wu = weight_ub.numpy()[0]\n",
    "            wf = weight_fu.numpy()[0]\n",
    "\n",
    "            print('It: %d |loss_value:  %.2f |  mse_b :  %.2f | mse_f: %.2f | Time: %.2f' % ( epoch, loss_value ,wu , wf ,   elapsed))\n",
    "            # tf.print(f\"mse_b  {mse_b}  mse_f: {mse_f}   total loss: {loss_value}\")\n",
    "            print(f\"weight_ub: {weight_ub.numpy()}\")\n",
    "            print(f\" weight_fu: {weight_fu.numpy()}\")\n",
    "\n",
    "            MSE_b1.append(mse_b)\n",
    "            MSE_f1.append(mse_f)\n",
    "\n",
    "            weightu.append(wu)\n",
    "            weightf.append(wf)\n",
    "\n",
    "            start_time = time.time()\n",
    "    u_pred, v_pred, p_pred = predict(X_star)\n",
    "    error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
    "    print('Error u: %e' % (error_u))\n",
    "    error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
    "    print('Error v: %e' % (error_v))\n",
    "    print(\"Starting L-BFGS training\")\n",
    "\n",
    "    loss_and_flat_grad = get_loss_and_flat_grad(x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch,  vb_batch, weight_ub, weight_fu)\n",
    "\n",
    "    lbfgs(loss_and_flat_grad,\n",
    "          get_weights(u_model),\n",
    "          Struct(), maxIter=newton_iter1, learningRate=0.8)\n",
    "\n",
    "    u_pred, v_pred, p_pred = predict(X_star)\n",
    "    error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
    "    print('Error u: %e' % (error_u))\n",
    "    error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
    "    print('Error v: %e' % (error_v))\n",
    "\n",
    "    lbfgs(loss_and_flat_grad,\n",
    "          get_weights(u_model),\n",
    "          Struct(), maxIter=newton_iter2, learningRate=0.8)\n",
    "\n",
    "    return MSE_b1, MSE_f1,  weightu, weightf\n",
    "\n",
    "# L-BFGS implementation from https://github.com/pierremtb/PINNs-TF2.0\n",
    "def get_loss_and_flat_grad(x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch, vb_batch,weight_ub, weight_fu):\n",
    "\n",
    "    def loss_and_flat_grad(w):\n",
    "        with tf.GradientTape() as tape:\n",
    "            set_weights(u_model, w, sizes_w, sizes_b)\n",
    "            loss_value, _, _ = loss(x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch, vb_batch, weight_ub, weight_fu)\n",
    "        grad = tape.gradient(loss_value, u_model.trainable_variables)\n",
    "        grad_flat = []\n",
    "        for g in grad:\n",
    "            grad_flat.append(tf.reshape(g, [-1]))\n",
    "        grad_flat = tf.concat(grad_flat, 0)\n",
    "        # print(loss_value, grad_flat)\n",
    "        return loss_value, grad_flat\n",
    "\n",
    "    return loss_and_flat_grad\n",
    "\n",
    "\n",
    "def predict(X_star):\n",
    "    X_star = tf.convert_to_tensor(X_star, dtype=tf.float32)\n",
    "    u_star, v_star, p_star = u_x_model(X_star[:, 0:1], X_star[:, 1:2], X_star[:, 2:3])\n",
    "    return u_star.numpy(), v_star.numpy(), p_star.numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_f = 10000\n",
    "Nu1 = 200\n",
    "\n",
    "weight_ub = tf.Variable([1.0], dtype=tf.float32)\n",
    "weight_fu = tf.Variable([1.0], dtype=tf.float32)\n",
    "\n",
    "x1 = (np.linspace(0, 1, 32)).flatten()[:, None]\n",
    "y1 = (np.linspace(0, 1, 32)).flatten()[:, None]\n",
    "t1 = (np.linspace(0, 1, 20)).flatten()[:, None]\n",
    "\n",
    "ttt1, ttt0 = np.meshgrid(x1, y1)\n",
    "\n",
    "tt1 = np.concatenate(([ttt1.flatten()[:, None], ttt0.flatten()[:, None], np.zeros((x1.shape[0] * y1.shape[0], 1))]), axis=1)\n",
    "x_1t = np.array([tt1[:, 0]]).T\n",
    "y_1t = np.array([tt1[:, 1]]).T\n",
    "t_1t = np.array([tt1[:, 2]]).T\n",
    "ut1 = -np.sin(t_1t) * np.sin(np.pi * x_1t) * np.sin(np.pi * x_1t) * np.sin(np.pi * y_1t) * np.cos(np.pi * y_1t)\n",
    "vt1 = np.sin(t_1t) * np.sin(np.pi * x_1t) * np.cos(np.pi * x_1t) * np.sin(np.pi * y_1t) * np.sin(np.pi * y_1t)\n",
    "\n",
    "yyy1, yyy0 = np.meshgrid(x1, t1)\n",
    "\n",
    "yy1 = np.concatenate(\n",
    "    ([yyy1.flatten()[:, None], np.min(y1) * np.ones((x1.shape[0] * t1.shape[0], 1)), yyy0.flatten()[:, None]]), axis=1)\n",
    "x_1y = np.array([yy1[:, 0]]).T\n",
    "y_1y = np.array([yy1[:, 1]]).T\n",
    "t_1y = np.array([yy1[:, 2]]).T\n",
    "uy1 = -np.sin(t_1y) * np.sin(np.pi * x_1y) * np.sin(np.pi * x_1y) * np.sin(np.pi * y_1y) * np.cos(np.pi * y_1y)\n",
    "vy1 = np.sin(t_1y) * np.sin(np.pi * x_1y) * np.cos(np.pi * x_1y) * np.sin(np.pi * y_1y) * np.sin(np.pi * y_1y)\n",
    "\n",
    "yy2 = np.concatenate(\n",
    "    ([yyy1.flatten()[:, None], np.max(y1) * np.ones((x1.shape[0] * t1.shape[0], 1)), yyy0.flatten()[:, None]]), axis=1)\n",
    "x_2y = np.array([yy2[:, 0]]).T\n",
    "y_2y = np.array([yy2[:, 1]]).T\n",
    "t_2y = np.array([yy2[:, 2]]).T\n",
    "uy2 = -np.sin(t_2y) * np.sin(np.pi * x_2y) * np.sin(np.pi * x_2y) * np.sin(np.pi * y_2y) * np.cos(np.pi * y_2y)\n",
    "vy2 = np.sin(t_2y) * np.sin(np.pi * x_2y) * np.cos(np.pi * x_2y) * np.sin(np.pi * y_2y) * np.sin(np.pi * y_2y)\n",
    "\n",
    "\n",
    "xxx1, xxx0 = np.meshgrid(y1, t1)\n",
    "\n",
    "xx1 = np.concatenate(\n",
    "    ([np.min(x1) * np.ones((y1.shape[0] * t1.shape[0], 1)), xxx1.flatten()[:, None], xxx0.flatten()[:, None]]), axis=1)\n",
    "x_1x = np.array([xx1[:, 0]]).T\n",
    "y_1x = np.array([xx1[:, 1]]).T\n",
    "t_1x = np.array([xx1[:, 2]]).T\n",
    "ux1 = -np.sin(t_1x) * np.sin(np.pi * x_1x) * np.sin(np.pi * x_1x) * np.sin(np.pi * y_1x) * np.cos(np.pi * y_1x)\n",
    "vx1 = np.sin(t_1x) * np.sin(np.pi * x_1x) * np.cos(np.pi * x_1x) * np.sin(np.pi * y_1x) * np.sin(np.pi * y_1x)\n",
    "\n",
    "xx2 = np.concatenate(\n",
    "    ([np.max(x1) * np.ones((y1.shape[0] * t1.shape[0], 1)), xxx1.flatten()[:, None], xxx0.flatten()[:, None]]), axis=1)\n",
    "x_2x = np.array([xx2[:, 0]]).T\n",
    "y_2x = np.array([xx2[:, 1]]).T\n",
    "t_2x = np.array([xx2[:, 2]]).T\n",
    "ux2 = -np.sin(t_2x) * np.sin(np.pi * x_2x) * np.sin(np.pi * x_2x) * np.sin(np.pi * y_2x) * np.cos(np.pi * y_2x)\n",
    "vx2 = np.sin(t_2x) * np.sin(np.pi * x_2x) * np.cos(np.pi * x_2x) * np.sin(np.pi * y_2x) * np.sin(np.pi * y_2x)\n",
    "\n",
    "X_u1 = np.vstack([tt1, yy1, yy2, xx1, xx2])\n",
    "u1 = np.vstack([ut1, uy1, uy2, ux1, ux2])\n",
    "v1 = np.vstack([vt1, vy1, vy2, vx1, vx2])\n",
    "\n",
    "idx_1 = np.random.choice(X_u1.shape[0], Nu1, replace=False)\n",
    "X_u_train = X_u1[idx_1, :]\n",
    "u_train = u1[idx_1, :]\n",
    "v_train = v1[idx_1, :]\n",
    "\n",
    "X1, Y1, T1 = np.meshgrid(x1, y1, t1)\n",
    "#    Exact = np.sin(np.pi*X)*np.sin(np.pi*T)*np.sin(np.pi*Z)  #100*100*100\n",
    "U_exact1 = -np.sin(T1) * np.sin(np.pi * X1) * np.sin(np.pi * X1) * np.sin(np.pi * Y1) * np.cos(np.pi * Y1)\n",
    "V_exact1 = np.sin(T1) * np.sin(np.pi * X1) * np.cos(np.pi * X1) * np.sin(np.pi * Y1) * np.sin(np.pi * Y1)\n",
    "P_exact1 = np.sin(T1) * np.sin(np.pi * X1) * np.cos(np.pi * Y1)\n",
    "\n",
    "X_star1 = np.hstack((X1.flatten()[:, None], Y1.flatten()[:, None], T1.flatten()[:, None]))\n",
    "x_star1 = np.array([X_star1[:, 0]]).T\n",
    "y_star1 = np.array([X_star1[:, 1]]).T\n",
    "t_star1 = np.array([X_star1[:, 2]]).T\n",
    "\n",
    "u_exact1 = -np.sin(t_star1) * np.sin(np.pi * x_star1) * np.sin(np.pi * x_star1) * np.sin(np.pi * y_star1) * np.cos(np.pi * y_star1)\n",
    "v_exact1 = np.sin(t_star1) * np.sin(np.pi * x_star1) * np.cos(np.pi * x_star1) * np.sin(np.pi * y_star1) * np.sin(np.pi * y_star1)\n",
    "p_exact1 = np.sin(t_star1) * np.sin(np.pi * x_star1) * np.cos(np.pi * y_star1)\n",
    "\n",
    "lb1 = X_star1.min(0)\n",
    "ub1 = X_star1.max(0)\n",
    "\n",
    "X_f_train11 = lb1 + (ub1 - lb1) * lhs(3, N_f)\n",
    "X_f = np.vstack((X_f_train11, X_u_train))\n",
    "\n",
    "xb = tf.cast(X_u_train[:, 0:1], dtype=tf.float32)\n",
    "yb = tf.cast(X_u_train[:, 1:2], dtype=tf.float32)\n",
    "tb = tf.cast(X_u_train[:, 2:3], dtype=tf.float32)\n",
    "ub = tf.cast(u_train[:, 0:1], dtype=tf.float32)\n",
    "vb = tf.cast(v_train[:, 0:1], dtype=tf.float32)\n",
    "\n",
    "\n",
    "lb = X_star1.min(0)\n",
    "rb = X_star1.max(0)\n",
    "\n",
    "x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=tf.float32)\n",
    "y_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=tf.float32)\n",
    "t_f = tf.convert_to_tensor(X_f[:, 2:3], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ub: [1.]  weight_fu: [1.]\n",
      "starting Adam training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16603/3620859216.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mMSE_b1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweightu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweightf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_ub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_fu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_exact1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_exact1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_exact1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_star1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_iter2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewton_iter1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnewton_iter2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16603/3478387952.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(x_f, y_f, t_f, xb, yb, tb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star, tf_iter, tf_iter2, newton_iter1, newton_iter2)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mwf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_fu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'It: %d |loss_value:  %.2f |  mse_b :  %.2f | mse_f: %.2f | Time: %.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mwu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mwf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m   \u001b[0melapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0;31m# tf.print(f\"mse_b  {mse_b}  mse_f: {mse_f}   total loss: {loss_value}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"weight_ub: {weight_ub.numpy()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "MSE_b1, MSE_f1, weightu, weightf = fit(x_f, y_f, t_f, xb, yb, tb, ub, vb, weight_ub, weight_fu, u_exact1, v_exact1, p_exact1, X_star1, tf_iter=10000, tf_iter2=1000, newton_iter1=5000,newton_iter2=15000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "elapsed = time.time() - start_time\n",
    "print('Training time: %.4f' % (elapsed))\n",
    "\n",
    "u_pred, v_pred, p_pred = predict(X_star1)\n",
    "\n",
    "U_pred = u_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
    "V_pred = v_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
    "P_pred = p_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
    "\n",
    "error_uu = np.abs(u_exact1 - u_pred)\n",
    "error_vv = np.abs(v_exact1 - v_pred)\n",
    "error_pp = np.abs(p_exact1 - p_pred)\n",
    "\n",
    "error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
    "print('Error u: %e' % (error_u))\n",
    "\n",
    "error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
    "print('Error v: %e' % (error_v))\n",
    "\n",
    "error_p = np.linalg.norm(p_exact1 - p_pred, 2) / np.linalg.norm(p_exact1, 2)\n",
    "print('Error p: %e' % (error_p))\n",
    "\n",
    "dataNewNS = 'D://NS_hisyory.mat'\n",
    "scipy.io.savemat(dataNewNS, {'w_MSE_b': MSE_b1, 'w_MSE_f': MSE_f1, 'weight_u': weightu, 'weight_f': weightf, 'U_pred': U_pred, 'V_pred': V_pred, 'P_pred': P_pred})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchdiffeq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
