{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_25589/2981199282.py:10: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #155: KMP_AFFINITY: Initial OS proc set respected: 0-15\n",
      "OMP: Info #216: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #157: KMP_AFFINITY: 16 available OS procs\n",
      "OMP: Info #158: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #287: KMP_AFFINITY: topology layer \"LL cache\" is equivalent to \"socket\".\n",
      "OMP: Info #287: KMP_AFFINITY: topology layer \"L3 cache\" is equivalent to \"socket\".\n",
      "OMP: Info #287: KMP_AFFINITY: topology layer \"L2 cache\" is equivalent to \"core\".\n",
      "OMP: Info #287: KMP_AFFINITY: topology layer \"L1 cache\" is equivalent to \"core\".\n",
      "OMP: Info #192: KMP_AFFINITY: 1 socket x 8 cores/socket x 2 threads/core (8 total cores)\n",
      "OMP: Info #218: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 0 maps to socket 0 core 0 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 8 maps to socket 0 core 0 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 1 maps to socket 0 core 1 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 9 maps to socket 0 core 1 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 2 maps to socket 0 core 2 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 10 maps to socket 0 core 2 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 3 maps to socket 0 core 3 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 11 maps to socket 0 core 3 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 4 maps to socket 0 core 4 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 12 maps to socket 0 core 4 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 5 maps to socket 0 core 5 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 13 maps to socket 0 core 5 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 6 maps to socket 0 core 6 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 14 maps to socket 0 core 6 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 7 maps to socket 0 core 7 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 15 maps to socket 0 core 7 thread 1 \n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25589 thread 0 bound to OS proc set 0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Oct 24 13:02:32 2021\n",
    "\n",
    "@author: lenovo\n",
    "\"\"\"\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "import math\n",
    "import matplotlib.gridspec as gridspec\n",
    "from plotting import newfig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import layers, activations\n",
    "from scipy.interpolate import griddata\n",
    "from eager_lbfgs import lbfgs, Struct\n",
    "from pyDOE import lhs\n",
    "\n",
    "from generate_dataset import *\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_net(object):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "\n",
    "        self.layer_sizes = layers\n",
    "        sizes_w = []\n",
    "        sizes_b = []\n",
    "        for i, width in enumerate(self.layer_sizes):\n",
    "            if i != 1:\n",
    "                sizes_w.append(int(width * self.layer_sizes[1]))\n",
    "                sizes_b.append(int(width if i != 0 else self.layer_sizes[1]))\n",
    "\n",
    "\n",
    "        # L-BFGS weight getting and setting from https://github.com/pierremtb/PINNs-TF2.0\n",
    "\n",
    "    def set_weights(self, model, w, sizes_w, sizes_b):  # 重新设置参数\n",
    "\n",
    "        for i, layer in enumerate(model.layers[1:len(sizes_w) + 1]):\n",
    "            start_weights = sum(sizes_w[:i]) + sum(sizes_b[:i])\n",
    "            end_weights = sum(sizes_w[:i + 1]) + sum(sizes_b[:i])\n",
    "            weights = w[start_weights:end_weights]\n",
    "            w_div = int(sizes_w[i] / sizes_b[i])\n",
    "            weights = tf.reshape(weights, [w_div, sizes_b[i]])\n",
    "            biases = w[end_weights:end_weights + sizes_b[i]]\n",
    "            weights_biases = [weights, biases]\n",
    "            layer.set_weights(weights_biases)\n",
    "\n",
    "\n",
    "    def get_weights(self, model):\n",
    "        w = []\n",
    "        for layer in model.layers[1:len(sizes_w) + 1]:\n",
    "            weights_biases = layer.get_weights()\n",
    "            weights = weights_biases[0].flatten()\n",
    "            biases = weights_biases[1]\n",
    "            w.extend(weights)\n",
    "            w.extend(biases)\n",
    "        w = tf.convert_to_tensor(w)\n",
    "        return w\n",
    "\n",
    "    def xavier_init(self, layer_sizes):\n",
    "        in_dim = layer_sizes[0]\n",
    "        out_dim = layer_sizes[1]\n",
    "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "\n",
    "    def model(self):\n",
    "\n",
    "        input_tensor = keras.Input(shape=(self.layer_sizes[0],))\n",
    "\n",
    "        hide_layer_list = []\n",
    "        flag = True\n",
    "        for width in self.layer_sizes[1:-1]:\n",
    "            if flag:\n",
    "                x = layers.Dense(\n",
    "                    width, activation=tf.nn.tanh,\n",
    "                    kernel_initializer=\"glorot_normal\")(input_tensor)\n",
    "                flag = False\n",
    "            else:\n",
    "                x = layers.Dense(\n",
    "                    width, activation=tf.nn.tanh,\n",
    "                    kernel_initializer=\"glorot_normal\")(x)\n",
    "        output_tensor = layers.Dense(self.layer_sizes[-1], activation=None,kernel_initializer=\"glorot_normal\")(x)\n",
    "        print(\"xxxxxxxxxxxxxx\")\n",
    "        output0 = output_tensor[:, 0:1]\n",
    "        output1 = output_tensor[:, 1:2]\n",
    "        output2 = output_tensor[:, 2:3]\n",
    "\n",
    "        model_output = keras.models.Model(input_tensor, [output0, output1, output2])\n",
    "        model_output.summary()\n",
    "\n",
    "        return model_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoD_BFS_Slip_PINN:\n",
    "\n",
    "    def __init__(self, data, layers  , activFun , mode , starter_learning_rate = 1.0e-3 , ExistModel=0):\n",
    "\n",
    "\n",
    "        self.layers = layers # \n",
    "\n",
    "\n",
    "        # initialize the NN\n",
    "        self.net_cuvwp = neural_net(layers).model()\n",
    "        self.weight_initial = []\n",
    "        self.weightf = []\n",
    "\n",
    "    @tf.function\n",
    "    def f_model(self,t ,  x, y):\n",
    "\n",
    "        mu = 0.00345\n",
    "        density = 1056\n",
    "\n",
    "        u, v, p = self.net_cuvwp(tf.concat([t, x , y],1))\n",
    "\n",
    "        \n",
    "        u_t = tf.gradients(u, t)[0]\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        u_y = tf.gradients(u, y)[0]\n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "        u_yy = tf.gradients(u_y, y)[0]\n",
    "\n",
    "        v_t = tf.gradients(v, t)[0]\n",
    "        v_x = tf.gradients(v, x)[0]\n",
    "        v_y = tf.gradients(v, y)[0]\n",
    "        v_xx = tf.gradients(v_x, x)[0]\n",
    "        v_yy = tf.gradients(v_y, y)[0]\n",
    "\n",
    "        p_x = tf.gradients(p, x)[0]\n",
    "        p_y = tf.gradients(p, y)[0]\n",
    "\n",
    "        f_u = u_t + (u * u_x + v * u_y ) + 1.0/density * p_x - mu *(u_xx + u_yy )\n",
    "        f_v = v_t + (u * v_x + v * v_y ) + 1.0/density * p_y - mu *(v_xx + v_yy )\n",
    "        div = u_x + v_y \n",
    "            \n",
    "\n",
    "        return f_u, f_v, div\n",
    "\n",
    "\n",
    "    # define the loss\n",
    "    def loss_funct(self ,  t_f_batch , x_f_batch, y_f_batch, t_WALL_batch , x_WALL_batch, y_WALL_batch, p_WALL_batch ,\n",
    "                    tb , xb, yb, ub, vb, pb, weight_WALL ,  weight_ub,  weight_fu):\n",
    "\n",
    "\n",
    "        f_u_pred, f_v_pred, div_pred = self.f_model( t_f_batch , x_f_batch, y_f_batch)\n",
    "\n",
    "\n",
    "        u_pred, v_pred, p_pred = self.net_cuvwp(tf.concat([tb , xb, yb], 1))\n",
    "        u_WALL_pred, v_WALL_pred, _ = self.net_cuvwp(tf.concat([t_WALL_batch , x_WALL_batch, y_WALL_batch], 1))\n",
    "\n",
    "        loss_INITIAL = weight_ub*(tf.reduce_mean(tf.square(u_pred - ub)) + tf.reduce_mean(tf.square(v_pred - vb))+ tf.reduce_mean(tf.square(p_pred - pb)))\n",
    "        loss_WALL =   weight_WALL *  tf.reduce_mean(tf.square(u_WALL_pred ))  + 1.0* tf.reduce_mean(tf.square(v_WALL_pred))\n",
    "\n",
    "        loss_res = weight_fu*(tf.reduce_mean(tf.square(f_u_pred)) + tf.reduce_mean(tf.square(f_v_pred)) + tf.reduce_mean(tf.square(div_pred)))\n",
    "\n",
    "        return loss_WALL , loss_INITIAL, loss_res\n",
    "    \n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def grad(self , t_f_batch , x_f_batch, y_f_batch, t_WALL_batch , x_WALL_batch, y_WALL_batch, p_WALL_batch , tb_batch ,  xb_batch, yb_batch, ub_batch, vb_batch, pb_batch, weight_ub, weight_fu):\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            loss_WALL, loss_INITIAL, loss_res = self.loss_funct( t_f_batch , x_f_batch, y_f_batch, \n",
    "                                                           t_WALL_batch , x_WALL_batch, y_WALL_batch, p_WALL_batch , \n",
    "                                                           tb_batch , xb_batch, yb_batch , ub_batch, vb_batch , pb_batch, \n",
    "                                                           weight_ub, weight_fu)\n",
    "            \n",
    "            loss = loss_WALL + loss_INITIAL + loss_res\n",
    "            grads = tape.gradient(loss, self.net_cuvwp.trainable_variables)\n",
    "\n",
    "            grads_ub = tape.gradient(loss, weight_ub)\n",
    "\n",
    "            grads_fu = tape.gradient(loss, weight_fu)\n",
    "\n",
    "        return  loss_INITIAL, loss_res, grads, grads_ub, grads_fu\n",
    "\n",
    "\n",
    "\n",
    "#model.train(collo[::10,:], inlet, outlet, wall, initial,  weight_ub, weight_fu, tf_iter=1000,   tf_iter2=100, newton_iter2=1500)\n",
    "    def train( self , collo, inlet, outlet, wall, initial , weight_initial, weight_fu , tf_iter, tf_iter2, newton_iter2):\n",
    "\n",
    "        batch_size =  120\n",
    "        n_batches =  collo.shape[0] // batch_size\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        tf_optimizer = tf.keras.optimizers.Adam(lr=0.005, beta_1=.99)\n",
    "        tf_optimizer_weights = tf.keras.optimizers.Adam(lr=0.003, beta_1=.99)\n",
    "        tf_optimizer_initial = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
    "\n",
    "        tf.print(f\"weight_initial: {weight_initial}  weight_fu: {weight_fu}\")\n",
    "        print(\"starting Adam training\")\n",
    "\n",
    "        a = np.random.rand(1000)\n",
    "        loss_history = list(a)\n",
    "        MSE_b0 = list(a)\n",
    "        MSE_f0 = list(a)\n",
    "\n",
    "        MSE_b1 = []\n",
    "        MSE_f1 = []\n",
    "\n",
    "\n",
    "        # For mini-batch (if used)\n",
    "        for epoch in range(tf_iter):\n",
    "            for i in range(n_batches):\n",
    "\n",
    "                idx_WALL = np.random.choice(wall[:, 0:1].shape[0], np.min([wall[:, 0:1].shape[0],batch_size]), replace=False)\n",
    "\n",
    "                xb_batch = initial[:, 1:2]\n",
    "                yb_batch = initial[:, 2:3]\n",
    "                tb_batch = initial[:, 0:1]\n",
    "                ub_batch = initial[:, 3:4]\n",
    "                vb_batch = initial[:, 4:5]\n",
    "                pb_batch = initial[:, 5:6]\n",
    "\n",
    "                (t_WALL_batch,\n",
    "                x_WALL_batch,\n",
    "                y_WALL_batch,\n",
    "                p_WALL_batch) = (wall[:, 0:1][idx_WALL],\n",
    "                                wall[:, 1:2][idx_WALL],\n",
    "                                wall[:, 2:3][idx_WALL],\n",
    "                                wall[:, 5:6][idx_WALL])\n",
    "                \n",
    "                x_f_batch = collo[:, 1:2][i * batch_size:(i * batch_size + batch_size), ]\n",
    "                y_f_batch = collo[:, 2:3][i * batch_size:(i * batch_size + batch_size), ]\n",
    "                t_f_batch = collo[:, 0:1][i * batch_size:(i * batch_size + batch_size), ]\n",
    "\n",
    "                loss_value , grads, grads_ub, grads_fu = self.grad( t_f_batch , x_f_batch, y_f_batch, \n",
    "                                                                    t_WALL_batch , x_WALL_batch, y_WALL_batch, p_WALL_batch , \n",
    "                                                                      tb_batch , xb_batch, yb_batch , ub_batch, vb_batch, pb_batch,\n",
    "                                                                          weight_initial, weight_fu)\n",
    "                \n",
    "\n",
    "                # print(\"starting Adam training\")\n",
    "\n",
    "                tf_optimizer.apply_gradients(zip(grads, self.net_cuvwp.trainable_variables))\n",
    "\n",
    "                loss_history.append(loss_value)\n",
    "                \n",
    "                if loss_history[-1] < loss_history[-2] and loss_history[-2] < loss_history[-3] and loss_history[-1] < loss_history[-10]:\n",
    "                    tf_optimizer_weights.apply_gradients(zip([-grads_fu], [weight_fu]))\n",
    "                    tf_optimizer_initial.apply_gradients(zip([-grads_ub], [weight_initial]))\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print('It: %d, Time: %.2f' % (epoch, elapsed))\n",
    "                tf.print(f\"mse_b  {mse_b}  mse_f: {mse_f}   total loss: {loss_value}\")\n",
    "\n",
    "                wu = weight_initial.numpy()\n",
    "                wf = weight_fu.numpy()\n",
    "\n",
    "                MSE_b1.append(mse_b)\n",
    "                MSE_f1.append(mse_f)\n",
    "\n",
    "\n",
    "\n",
    "                start_time = time.time()\n",
    "    \n",
    "\n",
    "        return MSE_b1, MSE_f1,  weightu, weightf\n",
    "\n",
    "\n",
    "    # L-BFGS implementation from https://github.com/pierremtb/PINNs-TF2.0\n",
    "    def get_loss_and_flat_grad ( self , t_f_batch , x_f_batch, y_f_batch, xb_batch, yb_batch, tb_batch, ub_batch, vb_batch,weight_ub, weight_fu):\n",
    "        def loss_and_flat_grad(w):\n",
    "            with tf.GradientTape() as tape:\n",
    "                self.net_cuvwp.set_weights( w, self.net_cuvwp.sizes_w, self.net_cuvwp.sizes_b)\n",
    "                loss_value, _, _ = self.loss_funct(x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch, vb_batch, weight_ub, weight_fu)\n",
    "            grad = tape.gradient(loss_value, self.net_cuvwp.trainable_variables)\n",
    "            grad_flat = []\n",
    "            for g in grad:\n",
    "                grad_flat.append(tf.reshape(g, [-1]))\n",
    "            grad_flat = tf.concat(grad_flat, 0)\n",
    "            # print(loss_value, grad_flat)\n",
    "            return loss_value, grad_flat\n",
    "\n",
    "        return loss_and_flat_grad\n",
    "\n",
    "\n",
    "    def predict(self , X_star):\n",
    "        X_star = tf.convert_to_tensor(X_star, dtype=tf.float32)\n",
    "\n",
    "        u, v, p = self.net_cuvwp(tf.concat([ X_star[:, 0:1] , X_star[:, 1:2], X_star[:, 2:3]], 1))\n",
    "\n",
    "        return u.numpy(), v.numpy(), p.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "\n",
    "def get_training_dataset(path , pINLET , pOUTLET , pWALL , pDomain , pInitial , dist):\n",
    "    \n",
    "    data = h5py.File(path , 'r')  # load dataset from matlab\n",
    "    WALL = np.transpose(data['noslipwall_02_Z0slice_2D_wall'], axes=range(len(data['noslipwall_02_Z0slice_2D_wall'].shape) - 1,-1, -1)).astype(np.float32)\n",
    "    \n",
    "    WALL = np.delete(WALL , np.where(WALL[:,0] == WALL[:,0].min())[0] , 0)  \n",
    "    # WALL = np.delete(WALL , np.where(WALL[:,1] <= 0.2)[0] , 0)  \n",
    "\n",
    "\n",
    "    domain = np.transpose(data['noslipwall_02_Z0slice_2D_innerdomain'], axes=range(len(data['noslipwall_02_Z0slice_2D_innerdomain'].shape) - 1,-1, -1)).astype(np.float32)\n",
    "    \n",
    "    domain = np.delete(domain , np.where(domain[:,0] == domain[:,0].min())[0] , 0)  \n",
    "    # domain = np.delete(domain , np.where(domain[:,1] <= 0.2)[0] , 0)  \n",
    "\n",
    "\n",
    "    # INLET = np.transpose(data['noslipwall_02_Z0slice_2D_inlet'],  axes=range(len(data['noslipwall_02_Z0slice_2D_inlet'].shape) - 1,-1, -1)).astype(np.float32)\n",
    "        \n",
    "    INLET = domain[np.where(domain[:,1] == domain[:,1].min())[0],:] #np.delete(INLET , np.where(INLET[:,0] == INLET[:,0].min())[0] , 0)  \n",
    "\n",
    "    OUTLET = np.transpose(data['noslipwall_02_Z0slice_2D_outlet'], axes=range(len(data['noslipwall_02_Z0slice_2D_outlet'].shape) - 1,-1, -1)).astype(np.float32)\n",
    "    \n",
    "    OUTLET = np.delete(OUTLET , np.where(OUTLET[:,0] == OUTLET[:,0].min())[0] , 0)  \n",
    "\n",
    "    total = INLET.shape[0] + OUTLET.shape[0] + domain.shape[0] +  WALL.shape[0] \n",
    "    \n",
    "    np.random.seed(1234)\n",
    "\n",
    "    # initial domain ux is 0.2 and other values are zero. FOr wall, all values are zero\n",
    "    \n",
    "    INITIALd = domain[np.where(domain[:,0] == domain[:,0].min())[0],:] # initial doamin corressponds to all values where t is zero\n",
    "\n",
    "    INITIALw = WALL[np.where(WALL[:,0] == WALL[:,0].min())[0],:] # initial doamin corressponds to all values where t is zero\n",
    "\n",
    "    INITIALi = INLET[np.where(INLET[:,0] == INLET[:,0].min())[0],:] # initial doamin corressponds to all values where t is zero\n",
    "\n",
    "    INITIALo = OUTLET[np.where(OUTLET[:,0] == OUTLET[:,0].min())[0],:] # initial doamin corressponds to all values where t is zero\n",
    "\n",
    "    #random selection of training data\n",
    "    INITIAL = np.concatenate([INITIALd , INITIALo],0)\n",
    "    \n",
    "    # domain = np.concatenate([domain , WALL , INLET , OUTLET],0)\n",
    "\n",
    "    #INLET = np.delete(INLET , idx_initi , 0)  \n",
    "    #OUTLET = np.delete(OUTLET , idx_inito  , 0)  \n",
    "    #domain = np.delete(domain , idx_initd , 0)  \n",
    "    #WALL = np.delete(WALL , idx_initw , 0)  \n",
    "    \n",
    "    if dist == \"Sobol\":\n",
    "        idxi = generate_sobol_sequence(0 , INLET.shape[0] ,  int(INLET.shape[0] * pINLET)) \n",
    "        INLET = INLET[idxi, :]\n",
    "        idxi = generate_sobol_sequence(0 , OUTLET.shape[0] ,  int(OUTLET.shape[0] * pOUTLET)) \n",
    "        OUTLET = OUTLET[idxi, :]\n",
    "        idxi = generate_sobol_sequence(0 , INITIAL.shape[0] ,  int(INITIAL.shape[0] * pInitial)) \n",
    "        INITIAL = INITIAL[idxi, :]\n",
    "        idxi = generate_sobol_sequence(0 , WALL.shape[0] ,  int(WALL.shape[0] * pWALL)) \n",
    "        WALL = WALL[idxi, :]\n",
    "        idxi = generate_sobol_sequence(0 , domain.shape[0] ,  int(domain.shape[0] * pDomain)) \n",
    "        domain = domain[idxi, :]\n",
    "    else:\n",
    "        idxi = np.random.choice(INLET.shape[0], int(INLET.shape[0] * pINLET), replace=False)\n",
    "        INLET = INLET[idxi, :]\n",
    "        idxi = np.random.choice(OUTLET.shape[0], int(OUTLET.shape[0] * pOUTLET), replace=False)\n",
    "        OUTLET = OUTLET[idxi, :]\n",
    "        idxi = np.random.choice(INITIAL.shape[0], int(INITIAL.shape[0] * pInitial), replace=False)\n",
    "        INITIAL = INITIAL[idxi, :]\n",
    "        idxi = np.random.choice(WALL.shape[0], int(WALL.shape[0] * pWALL), replace=False)\n",
    "        WALL = WALL[idxi, :]\n",
    "        idxi = np.random.choice(domain.shape[0], int(domain.shape[0] * pDomain), replace=False)\n",
    "        domain = domain[idxi, :]\n",
    "\n",
    "    #########################################\n",
    "    return [domain , INLET , OUTLET, WALL, INITIAL , total]\n",
    "################################################################\n",
    "\n",
    "\n",
    "\n",
    "def plot_result(model ,path ):\n",
    "\n",
    "\n",
    "    ######################################\n",
    "    model.save_NN()\n",
    "\n",
    "    list_ = [ \"noslipwall_02_Z0slice_2D_wall\", \"noslipwall_02_Z0slice_2D_innerdomain\" ]\n",
    "    N_data = [ 1600 , 20800 ]\n",
    "\n",
    "    peotDic = dict((key,value) for key,value in zip(list_,N_data))\n",
    "\n",
    "    tstep = [ 100 , 100 , ]\n",
    "    tstepList = dict((key,value) for key,value in zip(list_,tstep))\n",
    "\n",
    "    for key,value in peotDic.items():\n",
    "        # print(key, value)\n",
    "        test_data_inlet = get_testing_dataset(path   , part=key)\n",
    "        test_data_inlet = test_data_inlet[np.argsort(test_data_inlet[:, 0])]\n",
    "\n",
    "        [_ ,xf , _ , ufa , vfa  , pfa, u_pred , v_pred, p_pred]  = predict_result(model , test_data_inlet ,  value ,  tstep  ,  text=key , stm = False)\n",
    "        peotDic[key] =  error_over_time(tstepList[key] ,value , ufa , vfa , pfa , u_pred , v_pred , p_pred )\n",
    "        l1l2Erorr(key , u_pred ,v_pred , p_pred , ufa , vfa , pfa , model)\n",
    "\n",
    "    draw_error_over_time(peotDic , model.dirname)\n",
    "\n",
    "        ## drawing velocity profile\n",
    "    xValues = [5.000e-04 , 0.2275, 9.995e-01]\n",
    "    # UVelocity_profile(model.dirname , 0.2 , xValues , xf , u_pred)\n",
    "\n",
    "    \n",
    "    #def predict_result(model , test_data , N_data , tstep  , text = 'all'  , stm = False):\n",
    "    part = 'noslipwall_02_Z0slice_2D_innerdomain'\n",
    "    test_data_innerdomain = get_testing_dataset(path    ,  part=part)\n",
    "    [tf , xf , yf , ufa , vfa  , pfa, u_pred , v_pred, p_pred]   = predict_result(model , test_data_innerdomain , N_data ,  tstep ,  text='domain' ,  stm = False)\n",
    "\n",
    "    \n",
    "\n",
    "    tstep =100\n",
    "    N_data = 20800\n",
    "    x = xf.reshape(tstep,N_data)[0,:]\n",
    "    y = yf.reshape(tstep,N_data)[0,:]\n",
    "    t = tf.reshape(tstep,N_data)[:,0].T\n",
    "    ufa = ufa.reshape(tstep,N_data)\n",
    "    vfa = vfa.reshape(tstep,N_data)\n",
    "    pfa = pfa.reshape(tstep,N_data)\n",
    "    u_pred = u_pred.reshape(tstep,N_data)\n",
    "    v_pred = v_pred.reshape(tstep,N_data)\n",
    "    p_pred = p_pred.reshape(tstep,N_data)\n",
    "    error_u =  np.abs(ufa - u_pred) # (u - u_pred) / u # np.abs(u - u_pred)\n",
    "    error_v =  np.abs(vfa - v_pred) # (v - v_pred) / v # np.abs(v - v_pred)\n",
    "    error_p =  np.abs(pfa - p_pred) # (p - p_pred) / p # np.abs(p - p_pred)\n",
    "\n",
    "    data = [u_pred, v_pred , p_pred , ufa, vfa , pfa , error_u ,  error_v ,error_p ]\n",
    "\n",
    "    plot_time_profile(model.dirname , x , y , t , ufa , u_pred , '$u$')\n",
    "    plot_time_profile(model.dirname , x , y , t , vfa , v_pred , '$v$')\n",
    "    plot_time_profile(model.dirname , x , y , t , pfa , p_pred , '$p$')\n",
    "\n",
    "    draw_contourf(t , x , y , data , 1.0 ,10 , model.dirname , 5 , fontsize=17.5 , labelsize=12.5 , axes_pad=1.2)\n",
    "\n",
    "    model.weight_change_per_layer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxxxxxx\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 20)           80          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20)           420         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 20)           420         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            63          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1)]          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 1)]          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 1)]          0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 983\n",
      "Trainable params: 983\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from generate_dataset import * \n",
    "# path = '/okyanus/users/afarea/dataDir/noslipwall_02_Z0slice_2D/noslipwall_02_Z0slice_2D.mat'\n",
    "path = '/media/afrah2/MyWork/files2023/dataset/noslipwall_02_Z0slice_2D.mat'\n",
    "\n",
    "modelPath = ''\n",
    "\n",
    "\n",
    "pINLET = 1\n",
    "pOUTLET= 0.3\n",
    "pWALL =  0.006\n",
    "pDomain = 0.0007\n",
    "pInitial = 0.07\n",
    "\n",
    "dist =  \"Sobol\"\n",
    "activFun = 'hard_swish'\n",
    "\n",
    "[collo , inlet , outlet, wall, initial , total] = get_training_dataset(path , pINLET , pOUTLET , pWALL , pDomain , pInitial , dist)\n",
    "\n",
    "XY_c = np.concatenate([collo , wall, inlet , outlet], 0) \n",
    "\n",
    "\n",
    "\n",
    "# # Define model\n",
    "mode = 'M1'\n",
    "\n",
    "input_dimension = 3\n",
    "output_dimension = 3\n",
    "n_hidden_layers = 3\n",
    "neurons = 20 # [3, 20, 20, 20, 20, 20, 20, 20, 3]\n",
    "\n",
    "starter_learning_rate = float(0.005)\n",
    "epochs = 5\n",
    "model_layers = [input_dimension] + n_hidden_layers*[neurons] + [output_dimension]\n",
    "\n",
    "batch_size= 300\n",
    "\n",
    "iterations = 40000\n",
    "\n",
    "method  = \"mini_batch\"\n",
    "\n",
    "\n",
    "###############################################################\n",
    "model = TwoD_BFS_Slip_PINN(XY_c , model_layers  , activFun , mode , starter_learning_rate , ExistModel = modelPath)\n",
    "\n",
    "\n",
    "# model.print(\"Using mode: \" , model.mode)\n",
    "# model.print(\"neural network: \" , model.layers )\n",
    "# model.print(\"collocation Training data size : \" ,collo.shape[0], \" (\" ,str(pDomain*100) , \"%)\")\n",
    "# model.print(\"Wall Training data size: \" ,wall.shape[0], \" (\" ,str(pWALL*100) , \"%)\")\n",
    "# model.print(\"Initial Training data size: \" ,initial.shape[0], \" (\" ,str(pInitial*100) , \"%)\")\n",
    "# model.print(\"INLET Training data size: \" ,inlet.shape[0], \" (\" ,str(pINLET*100) , \"%)\")\n",
    "# model.print(\"OUTLET Training data size: \" ,outlet.shape[0], \" (\" ,str(pOUTLET*100) , \"%)\")\n",
    "\n",
    "# model.print(\"Total training datset size: \" ,XY_c.shape[0], \" (\" ,str((XY_c.shape[0] / total)*100) , \"%)\")\n",
    "# model.print(\"Activation function: \" , activFun)\n",
    "# model.print(\"number of iterations: \" , iterations)\n",
    "\n",
    "# model.print(\"Method desciption : learning rates: Exponential decay  with initial value: \", starter_learning_rate)\n",
    "\n",
    "\n",
    "# model.print(\"File directory: \" , model.dirname)\n",
    "\n",
    "# #save_data_to_matlab(os.path.join(model.dirname,'slipwall_02_Z0slice_2D_training.mat') ,coll , INLET , OUTLET , WALL ,INITIAL )\n",
    "\n",
    "# plot_dataset( model.dirname , wall , inlet , outlet , collo , initial , dist)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_f = XY_c\n",
    "X_u_train = wall\n",
    "\n",
    "\n",
    "\n",
    "X_star1 = collo\n",
    "\n",
    "\n",
    "b = X_star1.min(0)\n",
    "rb = X_star1.max(0)\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ub: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>  weight_fu: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>\n",
      "starting Adam training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25616 thread 1 bound to OS proc set 1\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25637 thread 2 bound to OS proc set 2\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25640 thread 5 bound to OS proc set 5\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25639 thread 4 bound to OS proc set 4\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25638 thread 3 bound to OS proc set 3\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25641 thread 6 bound to OS proc set 6\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25642 thread 7 bound to OS proc set 7\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25643 thread 8 bound to OS proc set 8\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25644 thread 9 bound to OS proc set 9\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25645 thread 10 bound to OS proc set 10\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25646 thread 11 bound to OS proc set 11\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25647 thread 12 bound to OS proc set 12\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25649 thread 14 bound to OS proc set 14\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25648 thread 13 bound to OS proc set 13\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25651 thread 16 bound to OS proc set 0\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25650 thread 15 bound to OS proc set 15\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25617 thread 17 bound to OS proc set 1\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25652 thread 18 bound to OS proc set 2\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25653 thread 19 bound to OS proc set 3\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25655 thread 21 bound to OS proc set 5\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25656 thread 22 bound to OS proc set 6\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25654 thread 20 bound to OS proc set 4\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25657 thread 23 bound to OS proc set 7\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25659 thread 25 bound to OS proc set 9\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25658 thread 24 bound to OS proc set 8\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25660 thread 26 bound to OS proc set 10\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25662 thread 28 bound to OS proc set 12\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25661 thread 27 bound to OS proc set 11\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25663 thread 29 bound to OS proc set 13\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25665 thread 31 bound to OS proc set 15\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25666 thread 32 bound to OS proc set 0\n",
      "OMP: Info #254: KMP_AFFINITY: pid 25589 tid 25664 thread 30 bound to OS proc set 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 0, Time: 14.46\n",
      "mse_b  [30155.94]  mse_f: [17.521324]   total loss: [30173.46]\n",
      "It: 10, Time: 0.43\n",
      "mse_b  [4424.8906]  mse_f: [4.8558607]   total loss: [4429.7466]\n",
      "It: 20, Time: 0.27\n",
      "mse_b  [4600.787]  mse_f: [4.5006948]   total loss: [4605.2876]\n",
      "It: 30, Time: 0.98\n",
      "mse_b  [966.13116]  mse_f: [4.3892517]   total loss: [970.5204]\n",
      "It: 40, Time: 0.66\n",
      "mse_b  [2345.0269]  mse_f: [3.0802488]   total loss: [2348.1072]\n",
      "It: 50, Time: 0.25\n",
      "mse_b  [2062.1064]  mse_f: [3.9237363]   total loss: [2066.0303]\n",
      "It: 60, Time: 0.77\n",
      "mse_b  [1244.8021]  mse_f: [3.7409902]   total loss: [1248.5431]\n",
      "It: 70, Time: 0.26\n",
      "mse_b  [1521.5153]  mse_f: [2.2677171]   total loss: [1523.783]\n",
      "It: 80, Time: 0.71\n",
      "mse_b  [874.7628]  mse_f: [1.0880482]   total loss: [875.8509]\n",
      "It: 90, Time: 0.35\n",
      "mse_b  [1184.888]  mse_f: [0.6953928]   total loss: [1185.5834]\n",
      "It: 100, Time: 0.67\n",
      "mse_b  [1173.1166]  mse_f: [0.76007694]   total loss: [1173.8767]\n",
      "It: 110, Time: 0.29\n",
      "mse_b  [642.6582]  mse_f: [0.8639137]   total loss: [643.5221]\n",
      "It: 120, Time: 0.71\n",
      "mse_b  [810.05865]  mse_f: [0.80134386]   total loss: [810.86]\n",
      "It: 130, Time: 0.24\n",
      "mse_b  [1264.3156]  mse_f: [0.7333612]   total loss: [1265.049]\n",
      "It: 140, Time: 0.76\n",
      "mse_b  [711.3341]  mse_f: [0.75505596]   total loss: [712.0892]\n",
      "It: 150, Time: 0.27\n",
      "mse_b  [419.5661]  mse_f: [0.7992439]   total loss: [420.36536]\n",
      "It: 160, Time: 0.64\n",
      "mse_b  [809.5977]  mse_f: [0.6913146]   total loss: [810.289]\n",
      "It: 170, Time: 0.26\n",
      "mse_b  [908.0071]  mse_f: [0.5427325]   total loss: [908.5498]\n",
      "It: 180, Time: 0.60\n",
      "mse_b  [477.97177]  mse_f: [0.49128184]   total loss: [478.46304]\n",
      "It: 190, Time: 0.33\n",
      "mse_b  [545.983]  mse_f: [0.46824694]   total loss: [546.45123]\n",
      "It: 200, Time: 0.22\n",
      "mse_b  [736.96204]  mse_f: [0.39231175]   total loss: [737.3544]\n",
      "It: 210, Time: 0.64\n",
      "mse_b  [710.6571]  mse_f: [0.3240684]   total loss: [710.9812]\n",
      "It: 220, Time: 0.30\n",
      "mse_b  [508.51157]  mse_f: [0.2980922]   total loss: [508.80966]\n",
      "It: 230, Time: 0.74\n",
      "mse_b  [587.30334]  mse_f: [0.268534]   total loss: [587.5719]\n",
      "It: 240, Time: 0.25\n",
      "mse_b  [625.43896]  mse_f: [0.22401462]   total loss: [625.66296]\n",
      "It: 250, Time: 0.68\n",
      "mse_b  [579.9298]  mse_f: [0.18993385]   total loss: [580.11975]\n",
      "It: 260, Time: 0.28\n",
      "mse_b  [519.22644]  mse_f: [0.19051853]   total loss: [519.41693]\n",
      "It: 270, Time: 0.60\n",
      "mse_b  [589.36005]  mse_f: [0.20501564]   total loss: [589.56506]\n",
      "It: 280, Time: 0.28\n",
      "mse_b  [590.06696]  mse_f: [0.16967292]   total loss: [590.23663]\n",
      "It: 290, Time: 0.39\n",
      "mse_b  [538.8468]  mse_f: [0.13386083]   total loss: [538.98065]\n",
      "It: 300, Time: 0.59\n",
      "mse_b  [548.719]  mse_f: [0.0961516]   total loss: [548.8151]\n",
      "It: 310, Time: 0.23\n",
      "mse_b  [550.444]  mse_f: [0.06134652]   total loss: [550.5053]\n",
      "It: 320, Time: 0.68\n",
      "mse_b  [560.6131]  mse_f: [0.05164165]   total loss: [560.66473]\n",
      "It: 330, Time: 0.28\n",
      "mse_b  [541.2328]  mse_f: [0.03938523]   total loss: [541.27216]\n",
      "It: 340, Time: 0.78\n",
      "mse_b  [537.6348]  mse_f: [0.04235974]   total loss: [537.6772]\n",
      "It: 350, Time: 0.24\n",
      "mse_b  [545.78406]  mse_f: [0.04661428]   total loss: [545.8307]\n",
      "It: 360, Time: 0.73\n",
      "mse_b  [545.79486]  mse_f: [0.05039546]   total loss: [545.8453]\n",
      "It: 370, Time: 0.22\n",
      "mse_b  [549.25244]  mse_f: [0.04692559]   total loss: [549.2994]\n",
      "It: 380, Time: 0.76\n",
      "mse_b  [532.63654]  mse_f: [0.04175157]   total loss: [532.6783]\n",
      "It: 390, Time: 0.20\n",
      "mse_b  [539.5498]  mse_f: [0.05027007]   total loss: [539.6001]\n",
      "It: 400, Time: 0.44\n",
      "mse_b  [546.1187]  mse_f: [0.05089427]   total loss: [546.1696]\n",
      "It: 410, Time: 0.34\n",
      "mse_b  [535.7054]  mse_f: [0.05264254]   total loss: [535.758]\n",
      "It: 420, Time: 0.22\n",
      "mse_b  [542.99976]  mse_f: [0.05311042]   total loss: [543.05286]\n",
      "It: 430, Time: 0.65\n",
      "mse_b  [548.9276]  mse_f: [0.0508346]   total loss: [548.97845]\n",
      "It: 440, Time: 0.28\n",
      "mse_b  [543.61426]  mse_f: [0.05270994]   total loss: [543.667]\n",
      "It: 450, Time: 0.56\n",
      "mse_b  [540.3678]  mse_f: [0.05708686]   total loss: [540.42487]\n",
      "It: 460, Time: 0.28\n",
      "mse_b  [555.12585]  mse_f: [0.05948141]   total loss: [555.18536]\n",
      "It: 470, Time: 0.22\n",
      "mse_b  [555.447]  mse_f: [0.06167308]   total loss: [555.50867]\n",
      "It: 480, Time: 0.78\n",
      "mse_b  [546.85974]  mse_f: [0.06233513]   total loss: [546.92206]\n",
      "It: 490, Time: 0.21\n",
      "mse_b  [558.61865]  mse_f: [0.06213777]   total loss: [558.6808]\n",
      "It: 500, Time: 0.60\n",
      "mse_b  [567.6474]  mse_f: [0.06251163]   total loss: [567.7099]\n",
      "It: 510, Time: 0.28\n",
      "mse_b  [559.8046]  mse_f: [0.06550281]   total loss: [559.8701]\n",
      "It: 520, Time: 0.30\n",
      "mse_b  [565.7133]  mse_f: [0.06838131]   total loss: [565.7817]\n",
      "It: 530, Time: 0.52\n",
      "mse_b  [574.348]  mse_f: [0.06665599]   total loss: [574.4147]\n",
      "It: 540, Time: 0.27\n",
      "mse_b  [568.4944]  mse_f: [0.06516065]   total loss: [568.5596]\n",
      "It: 550, Time: 0.64\n",
      "mse_b  [573.12683]  mse_f: [0.0631574]   total loss: [573.19]\n",
      "It: 560, Time: 0.23\n",
      "mse_b  [581.74835]  mse_f: [0.06174709]   total loss: [581.8101]\n",
      "It: 570, Time: 0.33\n",
      "mse_b  [575.64404]  mse_f: [0.06289619]   total loss: [575.7069]\n",
      "It: 580, Time: 0.54\n",
      "mse_b  [579.04517]  mse_f: [0.06678368]   total loss: [579.11194]\n",
      "It: 590, Time: 0.23\n",
      "mse_b  [586.8067]  mse_f: [0.0674549]   total loss: [586.87415]\n",
      "It: 600, Time: 0.63\n",
      "mse_b  [581.4174]  mse_f: [0.06619719]   total loss: [581.48364]\n",
      "It: 610, Time: 0.24\n",
      "mse_b  [584.6388]  mse_f: [0.061325]   total loss: [584.70013]\n",
      "It: 620, Time: 0.21\n",
      "mse_b  [589.21967]  mse_f: [0.05664672]   total loss: [589.2763]\n",
      "It: 630, Time: 0.68\n",
      "mse_b  [586.0158]  mse_f: [0.05518708]   total loss: [586.071]\n",
      "It: 640, Time: 0.24\n",
      "mse_b  [590.1288]  mse_f: [0.0543405]   total loss: [590.1831]\n",
      "It: 650, Time: 0.63\n",
      "mse_b  [593.0491]  mse_f: [0.05284276]   total loss: [593.1019]\n",
      "It: 660, Time: 0.27\n",
      "mse_b  [592.6028]  mse_f: [0.05141984]   total loss: [592.6542]\n",
      "It: 670, Time: 0.30\n",
      "mse_b  [593.83655]  mse_f: [0.04937724]   total loss: [593.8859]\n",
      "It: 680, Time: 0.50\n",
      "mse_b  [595.4923]  mse_f: [0.04773065]   total loss: [595.54004]\n",
      "It: 690, Time: 0.22\n",
      "mse_b  [595.7545]  mse_f: [0.04690208]   total loss: [595.8014]\n",
      "It: 700, Time: 0.43\n",
      "mse_b  [596.3496]  mse_f: [0.04584902]   total loss: [596.39545]\n",
      "It: 710, Time: 0.37\n",
      "mse_b  [596.468]  mse_f: [0.04456447]   total loss: [596.5126]\n",
      "It: 720, Time: 0.22\n",
      "mse_b  [598.5131]  mse_f: [0.04321408]   total loss: [598.55634]\n",
      "It: 730, Time: 0.57\n",
      "mse_b  [599.06866]  mse_f: [0.04171809]   total loss: [599.1104]\n",
      "It: 740, Time: 0.23\n",
      "mse_b  [599.8253]  mse_f: [0.04046608]   total loss: [599.8658]\n",
      "It: 750, Time: 0.23\n",
      "mse_b  [599.6563]  mse_f: [0.0396741]   total loss: [599.696]\n",
      "It: 760, Time: 0.71\n",
      "mse_b  [601.4129]  mse_f: [0.03879669]   total loss: [601.4517]\n",
      "It: 770, Time: 0.30\n",
      "mse_b  [602.49347]  mse_f: [0.0378435]   total loss: [602.5313]\n",
      "It: 780, Time: 0.71\n",
      "mse_b  [603.6297]  mse_f: [0.03615958]   total loss: [603.66583]\n",
      "It: 790, Time: 0.21\n",
      "mse_b  [605.0027]  mse_f: [0.03466174]   total loss: [605.03735]\n",
      "It: 800, Time: 0.61\n",
      "mse_b  [605.24884]  mse_f: [0.03370437]   total loss: [605.28253]\n",
      "It: 810, Time: 0.43\n",
      "mse_b  [606.2291]  mse_f: [0.03266523]   total loss: [606.2618]\n",
      "It: 820, Time: 0.60\n",
      "mse_b  [607.55225]  mse_f: [0.03167417]   total loss: [607.5839]\n",
      "It: 830, Time: 0.19\n",
      "mse_b  [607.7701]  mse_f: [0.03061877]   total loss: [607.8007]\n",
      "It: 840, Time: 0.20\n",
      "mse_b  [608.8061]  mse_f: [0.02945442]   total loss: [608.8356]\n",
      "It: 850, Time: 0.59\n",
      "mse_b  [610.4894]  mse_f: [0.02857977]   total loss: [610.51794]\n",
      "It: 860, Time: 0.24\n",
      "mse_b  [611.5665]  mse_f: [0.02771186]   total loss: [611.59424]\n",
      "It: 870, Time: 0.21\n",
      "mse_b  [611.41113]  mse_f: [0.0268512]   total loss: [611.438]\n",
      "It: 880, Time: 0.59\n",
      "mse_b  [611.8482]  mse_f: [0.02600627]   total loss: [611.8742]\n",
      "It: 890, Time: 0.21\n",
      "mse_b  [612.74475]  mse_f: [0.02507406]   total loss: [612.76984]\n",
      "It: 900, Time: 0.21\n",
      "mse_b  [612.92346]  mse_f: [0.0243129]   total loss: [612.94775]\n",
      "It: 910, Time: 0.62\n",
      "mse_b  [612.8812]  mse_f: [0.02362791]   total loss: [612.90485]\n",
      "It: 920, Time: 0.23\n",
      "mse_b  [614.0748]  mse_f: [0.022987]   total loss: [614.09784]\n",
      "It: 930, Time: 0.55\n",
      "mse_b  [614.39343]  mse_f: [0.02230193]   total loss: [614.4157]\n",
      "It: 940, Time: 0.36\n",
      "mse_b  [615.4073]  mse_f: [0.0214123]   total loss: [615.4287]\n",
      "It: 950, Time: 0.24\n",
      "mse_b  [615.5638]  mse_f: [0.02067979]   total loss: [615.5845]\n",
      "It: 960, Time: 0.70\n",
      "mse_b  [615.5455]  mse_f: [0.02002669]   total loss: [615.5655]\n",
      "It: 970, Time: 0.23\n",
      "mse_b  [616.76074]  mse_f: [0.01949066]   total loss: [616.7802]\n",
      "It: 980, Time: 0.71\n",
      "mse_b  [616.9728]  mse_f: [0.01906139]   total loss: [616.9918]\n",
      "It: 990, Time: 0.22\n",
      "mse_b  [618.15106]  mse_f: [0.01853175]   total loss: [618.1696]\n"
     ]
    }
   ],
   "source": [
    "N_f = 10000\n",
    "Nu1 = 200\n",
    "\n",
    "weight_initial = tf.Variable([1.0], dtype=tf.float32)\n",
    "weight_fu = tf.Variable([1.0], dtype=tf.float32)\n",
    "\n",
    "MSE_b1, MSE_f1, weightu, weightf = model.train(collo[::10,:], inlet, outlet, wall, initial,  weight_initial, weight_fu, tf_iter=1000,   tf_iter2=100, newton_iter2=1500)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"noslipwall_02_Z0slice_2D_innerdomain\" \n",
    "\n",
    "test_data_innerdomain = get_testing_dataset(path   , part=key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_innerdomain[:, 1:2].shape\n",
    "\n",
    "test = ([test_data_innerdomain[:, 1:2] , test_data_innerdomain[:, 2:3] , test_data_innerdomain[:, 0:1]])\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ub: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([3.8362556], dtype=float32)>  weight_fu: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1.2829995], dtype=float32)>\n",
      "Error u: 5.784612e+02\n",
      "Error v: 1.589013e+04\n",
      "Starting L-BFGS training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_f_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11589/2528267922.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting L-BFGS training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mloss_and_flat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss_and_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_f_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_f_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_f_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mub_batch\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_ub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_fu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlbfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_and_flat_grad\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewton_iter1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_f_batch' is not defined"
     ]
    }
   ],
   "source": [
    "tf.print(f\"weight_ub: {weight_initial}  weight_fu: {weight_fu}\")\n",
    "u_pred, v_pred, p_pred = model.predict(test_data_innerdomain[:,  0:3])\n",
    "\n",
    "\n",
    "\n",
    "# lbfgs(loss_and_flat_grad, get_weights(u_model), Struct(), maxIter=newton_iter2, learningRate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error u: 4.151636e-01\n",
      "Error v: 1.034749e+02\n",
      "Error v: 9.199123e-01\n",
      "Starting L-BFGS training\n"
     ]
    }
   ],
   "source": [
    "error_u = np.linalg.norm(test_data_innerdomain[:,  3:4] - u_pred, 2) / np.linalg.norm(test_data_innerdomain[:,  3:4] , 2)\n",
    "print('Error u: %e' % (error_u))\n",
    "error_v = np.linalg.norm(test_data_innerdomain[:,  4:5] - v_pred, 2) / np.linalg.norm(test_data_innerdomain[:,  4:5], 2)\n",
    "print('Error v: %e' % (error_v))\n",
    "error_v = np.linalg.norm(test_data_innerdomain[:,  5:6] - v_pred, 2) / np.linalg.norm(test_data_innerdomain[:,  5:6], 2)\n",
    "print('Error v: %e' % (error_v))\n",
    "print(\"Starting L-BFGS training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loss_funct() missing 1 required positional argument: 'weight_fu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11589/61811525.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloss_and_flat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss_and_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_star1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_star1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_star1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mub\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvb\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mweight_ub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_fu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlbfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_and_flat_grad\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_innerdomain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/afrah2/MyWork/files2023/SK/dwPINNs/eager_lbfgs.py\u001b[0m in \u001b[0;36mlbfgs\u001b[0;34m(opfunc, x, state, maxIter, learningRate, do_verbose)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mf_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11589/705365439.py\u001b[0m in \u001b[0;36mloss_and_flat_grad\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_funct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_f_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_f_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_f_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mub_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_ub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_fu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mgrad_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: loss_funct() missing 1 required positional argument: 'weight_fu'"
     ]
    }
   ],
   "source": [
    "loss_and_flat_grad = get_loss_and_flat_grad(t_star1 , x_star1, y_star1, tb ,  xb, yb,  ub,  vb , weight_initial, weight_fu)\n",
    "\n",
    "lbfgs(loss_and_flat_grad,  get_weights(u_model), Struct(), maxIter = 500, learningRate=0.8)\n",
    "\n",
    "u_pred, v_pred, p_pred = predict(test_data_innerdomain[:,  0:3])\n",
    "error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
    "print('Error u: %e' % (error_u))\n",
    "error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
    "print('Error v: %e' % (error_v))\n",
    "\n",
    "error_p = np.linalg.norm(p_exact1 - p_pred, 2) / np.linalg.norm(p_exact1, 2)\n",
    "print('Error p: %e' % (error_p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1488.1751\n",
      "Error u: 4.266133e+01\n",
      "Error v: 4.266161e+01\n",
      "Error p: 8.245512e+01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "elapsed = time.time() - start_time\n",
    "print('Training time: %.4f' % (elapsed))\n",
    "\n",
    "u_pred, v_pred, p_pred = predict(X_star1)\n",
    "\n",
    "# U_pred = u_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
    "# V_pred = v_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
    "# P_pred = p_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
    "u_exact1 = np.array([X_star1[:, 3]])\n",
    "v_exact1 = np.array([X_star1[:, 4]])\n",
    "p_exact1 = np.array([X_star1[:, 5]])\n",
    "\n",
    "\n",
    "error_uu = np.abs( u_exact1 - u_pred)\n",
    "error_vv = np.abs(v_exact1 - v_pred)\n",
    "error_pp = np.abs( p_exact1 - p_pred)\n",
    "\n",
    "error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
    "print('Error u: %e' % (error_u))\n",
    "\n",
    "error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
    "print('Error v: %e' % (error_v))\n",
    "\n",
    "error_p = np.linalg.norm(p_exact1 - p_pred, 2) / np.linalg.norm(p_exact1, 2)\n",
    "print('Error p: %e' % (error_p))\n",
    "\n",
    "# dataNewNS = 'NS_hisyory.mat'\n",
    "# scipy.io.savemat(dataNewNS, {'w_MSE_b': MSE_b1, 'w_MSE_f': MSE_f1, 'weight_u': weightu,\n",
    "#                   'weight_f': weightf, 'U_pred': u_pred, 'V_pred': V_pred, 'P_pred': P_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
