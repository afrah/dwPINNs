{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_26015/2981199282.py:10: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #155: KMP_AFFINITY: Initial OS proc set respected: 0-15\n",
      "OMP: Info #216: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #157: KMP_AFFINITY: 16 available OS procs\n",
      "OMP: Info #158: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #287: KMP_AFFINITY: topology layer \"LL cache\" is equivalent to \"socket\".\n",
      "OMP: Info #287: KMP_AFFINITY: topology layer \"L3 cache\" is equivalent to \"socket\".\n",
      "OMP: Info #287: KMP_AFFINITY: topology layer \"L2 cache\" is equivalent to \"core\".\n",
      "OMP: Info #287: KMP_AFFINITY: topology layer \"L1 cache\" is equivalent to \"core\".\n",
      "OMP: Info #192: KMP_AFFINITY: 1 socket x 8 cores/socket x 2 threads/core (8 total cores)\n",
      "OMP: Info #218: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 0 maps to socket 0 core 0 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 8 maps to socket 0 core 0 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 1 maps to socket 0 core 1 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 9 maps to socket 0 core 1 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 2 maps to socket 0 core 2 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 10 maps to socket 0 core 2 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 3 maps to socket 0 core 3 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 11 maps to socket 0 core 3 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 4 maps to socket 0 core 4 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 12 maps to socket 0 core 4 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 5 maps to socket 0 core 5 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 13 maps to socket 0 core 5 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 6 maps to socket 0 core 6 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 14 maps to socket 0 core 6 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 7 maps to socket 0 core 7 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 15 maps to socket 0 core 7 thread 1 \n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26015 thread 0 bound to OS proc set 0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Oct 24 13:02:32 2021\n",
    "\n",
    "@author: lenovo\n",
    "\"\"\"\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "import math\n",
    "import matplotlib.gridspec as gridspec\n",
    "from plotting import newfig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import layers, activations\n",
    "from scipy.interpolate import griddata\n",
    "from eager_lbfgs import lbfgs, Struct\n",
    "from pyDOE import lhs\n",
    "\n",
    "from generate_dataset import *\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "layer_sizes = [3, 20, 20, 20, 20, 20, 20, 20, 3]\n",
    "sizes_w = []\n",
    "sizes_b = []\n",
    "for i, width in enumerate(layer_sizes):\n",
    "    if i != 1:\n",
    "        sizes_w.append(int(width * layer_sizes[1]))\n",
    "        sizes_b.append(int(width if i != 0 else layer_sizes[1]))\n",
    "\n",
    "\n",
    "\n",
    "# L-BFGS weight getting and setting from https://github.com/pierremtb/PINNs-TF2.0\n",
    "\n",
    "def set_weights(model, w, sizes_w, sizes_b):  # Reset parameters\n",
    "\n",
    "    for i, layer in enumerate(model.layers[1:len(sizes_w) + 1]):\n",
    "        start_weights = sum(sizes_w[:i]) + sum(sizes_b[:i])\n",
    "        end_weights = sum(sizes_w[:i + 1]) + sum(sizes_b[:i])\n",
    "        weights = w[start_weights:end_weights]\n",
    "        w_div = int(sizes_w[i] / sizes_b[i])\n",
    "        weights = tf.reshape(weights, [w_div, sizes_b[i]])\n",
    "        biases = w[end_weights:end_weights + sizes_b[i]]\n",
    "        weights_biases = [weights, biases]\n",
    "        layer.set_weights(weights_biases)\n",
    "\n",
    "\n",
    "def get_weights(model):\n",
    "    w = []\n",
    "    for layer in model.layers[1:len(sizes_w) + 1]:\n",
    "        weights_biases = layer.get_weights()\n",
    "        weights = weights_biases[0].flatten()\n",
    "        biases = weights_biases[1]\n",
    "        w.extend(weights)\n",
    "        w.extend(biases)\n",
    "    w = tf.convert_to_tensor(w)\n",
    "    return w\n",
    "\n",
    "# def xavier_init(layer_sizes):\n",
    "#     in_dim = layer_sizes[0]\n",
    "#     out_dim = layer_sizes[1]\n",
    "#     xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "#     return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "\n",
    "def neural_net(layer_sizes):\n",
    "\n",
    "    input_tensor = keras.Input(shape=(layer_sizes[0],))\n",
    "\n",
    "    hide_layer_list = []\n",
    "    flag = True\n",
    "    for width in layer_sizes[1:-1]:\n",
    "        if flag:\n",
    "            x = layers.Dense(\n",
    "                width, activation=tf.nn.tanh,\n",
    "                kernel_initializer=\"glorot_normal\")(input_tensor)\n",
    "            flag = False\n",
    "        else:\n",
    "            x = layers.Dense(\n",
    "                width, activation=tf.nn.tanh,\n",
    "                kernel_initializer=\"glorot_normal\")(x)\n",
    "    output_tensor = layers.Dense(layer_sizes[-1], activation=None,kernel_initializer=\"glorot_normal\")(x)\n",
    "    print(\"xxxxxxxxxxxxxx\")\n",
    "    output0 = output_tensor[:, 0:1]\n",
    "    output1 = output_tensor[:, 1:2]\n",
    "    output2 = output_tensor[:, 2:3]\n",
    "\n",
    "    model_output = keras.models.Model(input_tensor, [output0, output1, output2])\n",
    "\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize the NN\n",
    "u_model = neural_net(layer_sizes)\n",
    "# view the NN\n",
    "u_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def f_model(x, y, t):\n",
    "\n",
    "    mu = 0.00345\n",
    "    density = 1056\n",
    "    u, v, p = u_model(tf.concat([x, y, t],1))\n",
    "\n",
    "    u_t = tf.gradients(u, t)[0]\n",
    "    u_x = tf.gradients(u, x)[0]\n",
    "    u_y = tf.gradients(u, y)[0]\n",
    "    u_xx = tf.gradients(u_x, x)[0]\n",
    "    u_yy = tf.gradients(u_y, y)[0]\n",
    "\n",
    "    v_t = tf.gradients(v, t)[0]\n",
    "    v_x = tf.gradients(v, x)[0]\n",
    "    v_y = tf.gradients(v, y)[0]\n",
    "    v_xx = tf.gradients(v_x, x)[0]\n",
    "    v_yy = tf.gradients(v_y, y)[0]\n",
    "\n",
    "    p_x = tf.gradients(p, x)[0]\n",
    "    p_y = tf.gradients(p, y)[0]\n",
    "\n",
    "    f_u = u_t + (u * u_x + v * u_y ) + 1.0/density * p_x - mu *(u_xx + u_yy )\n",
    "    f_v = v_t + (u * v_x + v * v_y ) + 1.0/density * p_y - mu *(v_xx + v_yy )\n",
    "    div = u_x + v_y \n",
    "        \n",
    "    return f_u, f_v, div\n",
    "\n",
    "@tf.function\n",
    "def u_x_model(x, y, t):\n",
    "    u, v, w = u_model(tf.concat([x, y, t], 1))\n",
    "    return u, v, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxxxxxx\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 20)           80          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20)           420         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 20)           420         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 20)           420         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 20)           420         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 20)           420         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 20)           420         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 3)            63          dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1)]          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 1)]          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 1)]          0           dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,663\n",
      "Trainable params: 2,663\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# define the loss\n",
    "def loss(x_f_batch, y_f_batch, t_f_batch, xb, yb, tb, ub, vb, weight_ub,  weight_fu):\n",
    "\n",
    "    f_u_pred, f_v_pred, div_pred = f_model(x_f_batch, y_f_batch, t_f_batch)\n",
    "\n",
    "\n",
    "    u_pred, v_pred, p_pred = u_model(tf.concat([xb, yb, tb], 1))\n",
    "    mse_b = 100*weight_ub*(tf.reduce_sum(tf.square(u_pred - ub)) + tf.reduce_sum(tf.square(v_pred - vb)))\n",
    "    mse_f = weight_fu*(tf.reduce_sum(tf.square(f_u_pred)) + tf.reduce_sum(tf.square(f_v_pred)) + tf.reduce_sum(tf.square(div_pred)))\n",
    "\n",
    "    return mse_b + mse_f, mse_b, mse_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def grad(u_model, x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch, vb_batch, weight_ub,\n",
    "         weight_fu):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        loss_value, mse_b, mse_f = loss(x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch,\n",
    "                                        vb_batch, weight_ub, weight_fu)\n",
    "        grads = tape.gradient(loss_value, u_model.trainable_variables)\n",
    "\n",
    "        grads_ub = tape.gradient(loss_value, weight_ub)\n",
    "\n",
    "        grads_fu = tape.gradient(loss_value, weight_fu)\n",
    "\n",
    "    return loss_value, mse_b, mse_f, grads, grads_ub, grads_fu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def fit( collo, inlet, outlet, wall, initial , weight_ub, weight_fu , tf_iter, tf_iter2, newton_iter1, newton_iter2):\n",
    "\n",
    "    batch_sz = N_f\n",
    "    n_batches = N_f // batch_sz\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    tf_optimizer = tf.keras.optimizers.Adam(lr=0.005, beta_1=.99)\n",
    "    tf_optimizer_weights = tf.keras.optimizers.Adam(lr=0.003, beta_1=.99)\n",
    "    tf_optimizer_u = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
    "\n",
    "    tf.print(f\"weight_ub: {weight_ub}  weight_fu: {weight_fu}\")\n",
    "    print(\"starting Adam training\")\n",
    "\n",
    "    a = np.random.rand(1000)\n",
    "    loss_history = list(a)\n",
    "    MSE_b0 = list(a)\n",
    "    MSE_f0 = list(a)\n",
    "\n",
    "    MSE_b1 = []\n",
    "    MSE_f1 = []\n",
    "\n",
    "    weightu = []\n",
    "    weightf = []\n",
    "    # For mini-batch (if used)\n",
    "    for epoch in range(tf_iter):\n",
    "        for i in range(n_batches):\n",
    "            xb_batch = initial[:, 1:2]\n",
    "            yb_batch = initial[:, 2:3]\n",
    "            tb_batch = initial[:, 0:1]\n",
    "            ub_batch = initial[:, 3:4]\n",
    "            vb_batch = initial[:, 4:5]\n",
    "            pb_batch = initial[:, 5:6]\n",
    "\n",
    "            x_f_batch = collo[:, 1:2][i * batch_sz:(i * batch_sz + batch_sz), ]\n",
    "            y_f_batch = collo[:, 2:3][i * batch_sz:(i * batch_sz + batch_sz), ]\n",
    "            t_f_batch = collo[:, 0:1][i * batch_sz:(i * batch_sz + batch_sz), ]\n",
    "\n",
    "            loss_value, mse_b, mse_f, grads, grads_ub, grads_fu = grad(u_model, x_f_batch, y_f_batch, t_f_batch,\n",
    "                                                                       xb_batch, yb_batch,\n",
    "                                                                       tb_batch, ub_batch, vb_batch, weight_ub,\n",
    "                                                                       weight_fu)\n",
    "\n",
    "            tf_optimizer.apply_gradients(zip(grads, u_model.trainable_variables))\n",
    "            MSE_b0.append(mse_b)\n",
    "            MSE_f0.append(mse_f)\n",
    "\n",
    "            loss_history.append(loss_value)\n",
    "            \n",
    "            if loss_history[-1] < loss_history[-2] and loss_history[-2] < loss_history[-3] and loss_history[-1] < loss_history[-10]:\n",
    "                tf_optimizer_weights.apply_gradients(zip([-grads_fu], [weight_fu]))\n",
    "                tf_optimizer_u.apply_gradients(zip([-grads_ub], [weight_ub]))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('It: %d, Time: %.2f' % (epoch, elapsed))\n",
    "            tf.print(f\"mse_b  {mse_b}  mse_f: {mse_f}   total loss: {loss_value}\")\n",
    "\n",
    "            wu = weight_ub.numpy()\n",
    "            wf = weight_fu.numpy()\n",
    "\n",
    "            MSE_b1.append(mse_b)\n",
    "            MSE_f1.append(mse_f)\n",
    "\n",
    "            weightu.append(wu)\n",
    "            weightf.append(wf)\n",
    "\n",
    "            start_time = time.time()\n",
    "    tf.print(f\"weight_ub: {weight_ub}  weight_fu: {weight_fu}\")\n",
    "    u_pred, v_pred, p_pred = predict(X_star)\n",
    "    error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
    "    print('Error u: %e' % (error_u))\n",
    "    error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
    "    print('Error v: %e' % (error_v))\n",
    "    print(\"Starting L-BFGS training\")\n",
    "\n",
    "    loss_and_flat_grad = get_loss_and_flat_grad(x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch,  vb_batch, weight_ub, weight_fu)\n",
    "\n",
    "    lbfgs(loss_and_flat_grad,  get_weights(u_model), Struct(), maxIter=newton_iter1, learningRate=0.8)\n",
    "\n",
    "    u_pred, v_pred, p_pred = predict(X_star)\n",
    "    error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
    "    print('Error u: %e' % (error_u))\n",
    "    error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
    "    print('Error v: %e' % (error_v))\n",
    "\n",
    "    error_p = np.linalg.norm(p_exact1 - p_pred, 2) / np.linalg.norm(p_exact1, 2)\n",
    "    print('Error p: %e' % (error_p))\n",
    "\n",
    "\n",
    "    # lbfgs(loss_and_flat_grad, get_weights(u_model), Struct(), maxIter=newton_iter2, learningRate=0.8)\n",
    "\n",
    "    return MSE_b1, MSE_f1,  weightu, weightf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L-BFGS implementation from https://github.com/pierremtb/PINNs-TF2.0\n",
    "def get_loss_and_flat_grad(x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch, vb_batch,weight_ub, weight_fu):\n",
    "    def loss_and_flat_grad(w):\n",
    "        with tf.GradientTape() as tape:\n",
    "            set_weights(u_model, w, sizes_w, sizes_b)\n",
    "            loss_value, _, _ = loss(x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch, vb_batch, weight_ub, weight_fu)\n",
    "        grad = tape.gradient(loss_value, u_model.trainable_variables)\n",
    "        grad_flat = []\n",
    "        for g in grad:\n",
    "            grad_flat.append(tf.reshape(g, [-1]))\n",
    "        grad_flat = tf.concat(grad_flat, 0)\n",
    "        # print(loss_value, grad_flat)\n",
    "        return loss_value, grad_flat\n",
    "\n",
    "    return loss_and_flat_grad\n",
    "\n",
    "\n",
    "def predict(X_star):\n",
    "    X_star = tf.convert_to_tensor(X_star, dtype=tf.float32)\n",
    "    u_star, v_star, p_star = u_x_model(X_star[:, 0:1], X_star[:, 1:2], X_star[:, 2:3])\n",
    "    return u_star.numpy(), v_star.numpy(), p_star.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "\n",
    "def get_training_dataset(path , pINLET , pOUTLET , pWALL , pDomain , pInitial , dist):\n",
    "    \n",
    "    data = h5py.File(path , 'r')  # load dataset from matlab\n",
    "    WALL = np.transpose(data['noslipwall_02_Z0slice_2D_wall'], axes=range(len(data['noslipwall_02_Z0slice_2D_wall'].shape) - 1,-1, -1)).astype(np.float32)\n",
    "    \n",
    "    WALL = np.delete(WALL , np.where(WALL[:,0] == WALL[:,0].min())[0] , 0)  \n",
    "    # WALL = np.delete(WALL , np.where(WALL[:,1] <= 0.2)[0] , 0)  \n",
    "\n",
    "\n",
    "    domain = np.transpose(data['noslipwall_02_Z0slice_2D_innerdomain'], axes=range(len(data['noslipwall_02_Z0slice_2D_innerdomain'].shape) - 1,-1, -1)).astype(np.float32)\n",
    "    \n",
    "    domain = np.delete(domain , np.where(domain[:,0] == domain[:,0].min())[0] , 0)  \n",
    "    # domain = np.delete(domain , np.where(domain[:,1] <= 0.2)[0] , 0)  \n",
    "\n",
    "\n",
    "    # INLET = np.transpose(data['noslipwall_02_Z0slice_2D_inlet'],  axes=range(len(data['noslipwall_02_Z0slice_2D_inlet'].shape) - 1,-1, -1)).astype(np.float32)\n",
    "        \n",
    "    INLET = domain[np.where(domain[:,1] == domain[:,1].min())[0],:] #np.delete(INLET , np.where(INLET[:,0] == INLET[:,0].min())[0] , 0)  \n",
    "\n",
    "    OUTLET = np.transpose(data['noslipwall_02_Z0slice_2D_outlet'], axes=range(len(data['noslipwall_02_Z0slice_2D_outlet'].shape) - 1,-1, -1)).astype(np.float32)\n",
    "    \n",
    "    OUTLET = np.delete(OUTLET , np.where(OUTLET[:,0] == OUTLET[:,0].min())[0] , 0)  \n",
    "\n",
    "    total = INLET.shape[0] + OUTLET.shape[0] + domain.shape[0] +  WALL.shape[0] \n",
    "    \n",
    "    np.random.seed(1234)\n",
    "\n",
    "    # initial domain ux is 0.2 and other values are zero. FOr wall, all values are zero\n",
    "    \n",
    "    INITIALd = domain[np.where(domain[:,0] == domain[:,0].min())[0],:] # initial doamin corressponds to all values where t is zero\n",
    "\n",
    "    INITIALw = WALL[np.where(WALL[:,0] == WALL[:,0].min())[0],:] # initial doamin corressponds to all values where t is zero\n",
    "\n",
    "    INITIALi = INLET[np.where(INLET[:,0] == INLET[:,0].min())[0],:] # initial doamin corressponds to all values where t is zero\n",
    "\n",
    "    INITIALo = OUTLET[np.where(OUTLET[:,0] == OUTLET[:,0].min())[0],:] # initial doamin corressponds to all values where t is zero\n",
    "\n",
    "    #random selection of training data\n",
    "    INITIAL = np.concatenate([INITIALd , INITIALo],0)\n",
    "    \n",
    "    # domain = np.concatenate([domain , WALL , INLET , OUTLET],0)\n",
    "\n",
    "    #INLET = np.delete(INLET , idx_initi , 0)  \n",
    "    #OUTLET = np.delete(OUTLET , idx_inito  , 0)  \n",
    "    #domain = np.delete(domain , idx_initd , 0)  \n",
    "    #WALL = np.delete(WALL , idx_initw , 0)  \n",
    "    \n",
    "    if dist == \"Sobol\":\n",
    "        idxi = generate_sobol_sequence(0 , INLET.shape[0] ,  int(INLET.shape[0] * pINLET)) \n",
    "        INLET = INLET[idxi, :]\n",
    "        idxi = generate_sobol_sequence(0 , OUTLET.shape[0] ,  int(OUTLET.shape[0] * pOUTLET)) \n",
    "        OUTLET = OUTLET[idxi, :]\n",
    "        idxi = generate_sobol_sequence(0 , INITIAL.shape[0] ,  int(INITIAL.shape[0] * pInitial)) \n",
    "        INITIAL = INITIAL[idxi, :]\n",
    "        idxi = generate_sobol_sequence(0 , WALL.shape[0] ,  int(WALL.shape[0] * pWALL)) \n",
    "        WALL = WALL[idxi, :]\n",
    "        idxi = generate_sobol_sequence(0 , domain.shape[0] ,  int(domain.shape[0] * pDomain)) \n",
    "        domain = domain[idxi, :]\n",
    "    else:\n",
    "        idxi = np.random.choice(INLET.shape[0], int(INLET.shape[0] * pINLET), replace=False)\n",
    "        INLET = INLET[idxi, :]\n",
    "        idxi = np.random.choice(OUTLET.shape[0], int(OUTLET.shape[0] * pOUTLET), replace=False)\n",
    "        OUTLET = OUTLET[idxi, :]\n",
    "        idxi = np.random.choice(INITIAL.shape[0], int(INITIAL.shape[0] * pInitial), replace=False)\n",
    "        INITIAL = INITIAL[idxi, :]\n",
    "        idxi = np.random.choice(WALL.shape[0], int(WALL.shape[0] * pWALL), replace=False)\n",
    "        WALL = WALL[idxi, :]\n",
    "        idxi = np.random.choice(domain.shape[0], int(domain.shape[0] * pDomain), replace=False)\n",
    "        domain = domain[idxi, :]\n",
    "\n",
    "    #########################################\n",
    "    return [domain , INLET , OUTLET, WALL, INITIAL , total]\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_dataset import * \n",
    "# path = '/okyanus/users/afarea/dataDir/noslipwall_02_Z0slice_2D/noslipwall_02_Z0slice_2D.mat'\n",
    "path = '/media/afrah2/MyWork/files2023/dataset/noslipwall_02_Z0slice_2D.mat'\n",
    "\n",
    "modelPath = ''\n",
    "\n",
    "\n",
    "pINLET = 1\n",
    "pOUTLET= 0.3\n",
    "pWALL =  0.006\n",
    "pDomain = 0.0007\n",
    "pInitial = 0.07\n",
    "\n",
    "dist =  \"Sobol\"\n",
    "activFun = 'hard_swish'\n",
    "\n",
    "[collo , inlet , outlet, wall, initial , total] = get_training_dataset(path , pINLET , pOUTLET , pWALL , pDomain , pInitial , dist)\n",
    "\n",
    "XY_c = np.concatenate([collo , wall, inlet , outlet], 0) \n",
    "\n",
    "\n",
    "\n",
    "# # Define model\n",
    "mode = 'M1'\n",
    "\n",
    "input_dimension = 3\n",
    "output_dimension = 3\n",
    "n_hidden_layers = 3\n",
    "neurons = 50\n",
    "\n",
    "starter_learning_rate = float(0.005)\n",
    "epochs = 5\n",
    "layers = [input_dimension] + n_hidden_layers*[neurons] + [output_dimension]\n",
    "\n",
    "batch_size= 300\n",
    "\n",
    "iterations = 40000\n",
    "\n",
    "method  = \"mini_batch\"\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# model = TwoD_BFS_Slip_PINN(XY_c , layers  , activFun , mode , starter_learning_rate , ExistModel = modelPath)\n",
    "\n",
    "\n",
    "# model.print(\"Using mode: \" , model.mode)\n",
    "# model.print(\"neural network: \" , model.layers )\n",
    "# model.print(\"collocation Training data size : \" ,collo.shape[0], \" (\" ,str(pDomain*100) , \"%)\")\n",
    "# model.print(\"Wall Training data size: \" ,wall.shape[0], \" (\" ,str(pWALL*100) , \"%)\")\n",
    "# model.print(\"Initial Training data size: \" ,initial.shape[0], \" (\" ,str(pInitial*100) , \"%)\")\n",
    "# model.print(\"INLET Training data size: \" ,inlet.shape[0], \" (\" ,str(pINLET*100) , \"%)\")\n",
    "# model.print(\"OUTLET Training data size: \" ,outlet.shape[0], \" (\" ,str(pOUTLET*100) , \"%)\")\n",
    "\n",
    "# model.print(\"Total training datset size: \" ,XY_c.shape[0], \" (\" ,str((XY_c.shape[0] / total)*100) , \"%)\")\n",
    "# model.print(\"Activation function: \" , activFun)\n",
    "# model.print(\"number of iterations: \" , iterations)\n",
    "\n",
    "# model.print(\"Method desciption : learning rates: Exponential decay  with initial value: \", starter_learning_rate)\n",
    "\n",
    "\n",
    "# model.print(\"File directory: \" , model.dirname)\n",
    "\n",
    "# #save_data_to_matlab(os.path.join(model.dirname,'slipwall_02_Z0slice_2D_training.mat') ,coll , INLET , OUTLET , WALL ,INITIAL )\n",
    "\n",
    "# plot_dataset( model.dirname , wall , inlet , outlet , collo , initial , dist)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_f = XY_c\n",
    "X_u_train = wall\n",
    "\n",
    "\n",
    "\n",
    "X_star1 = collo\n",
    "x_star1 = np.array([X_star1[:, 0]])\n",
    "y_star1 = np.array([X_star1[:, 1]])\n",
    "t_star1 = np.array([X_star1[:, 2]])\n",
    "\n",
    "u_exact1=np.array([X_star1[:, 3]])\n",
    "v_exact1=np.array([X_star1[:, 4]])\n",
    "p_exact1=np.array([X_star1[:, 5]])\n",
    "\n",
    "xb = tf.cast(X_u_train[:, 0:1], dtype=tf.float32)\n",
    "yb = tf.cast(X_u_train[:, 1:2], dtype=tf.float32)\n",
    "tb = tf.cast(X_u_train[:, 2:3], dtype=tf.float32)\n",
    "ub = tf.cast(X_u_train[:, 3:4], dtype=tf.float32)\n",
    "vb = tf.cast(X_u_train[:, 4:5], dtype=tf.float32)\n",
    "\n",
    "\n",
    "x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=tf.float32)\n",
    "y_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=tf.float32)\n",
    "t_f = tf.convert_to_tensor(X_f[:, 2:3], dtype=tf.float32)\n",
    "\n",
    "b = X_star1.min(0)\n",
    "rb = X_star1.max(0)\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ub: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>  weight_fu: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>\n",
      "starting Adam training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26060 thread 1 bound to OS proc set 1\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26070 thread 3 bound to OS proc set 3\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26069 thread 2 bound to OS proc set 2\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26071 thread 4 bound to OS proc set 4\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26073 thread 6 bound to OS proc set 6\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26072 thread 5 bound to OS proc set 5\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26074 thread 7 bound to OS proc set 7\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26075 thread 8 bound to OS proc set 8\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26076 thread 9 bound to OS proc set 9\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26077 thread 10 bound to OS proc set 10\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26079 thread 12 bound to OS proc set 12\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26078 thread 11 bound to OS proc set 11\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26080 thread 13 bound to OS proc set 13\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26081 thread 14 bound to OS proc set 14\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26082 thread 15 bound to OS proc set 15\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26083 thread 16 bound to OS proc set 0\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26059 thread 17 bound to OS proc set 1\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26084 thread 18 bound to OS proc set 2\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26085 thread 19 bound to OS proc set 3\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26086 thread 20 bound to OS proc set 4\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26087 thread 21 bound to OS proc set 5\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26089 thread 23 bound to OS proc set 7\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26088 thread 22 bound to OS proc set 6\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26090 thread 24 bound to OS proc set 8\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26091 thread 25 bound to OS proc set 9\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26093 thread 27 bound to OS proc set 11\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26092 thread 26 bound to OS proc set 10\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26094 thread 28 bound to OS proc set 12\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26095 thread 29 bound to OS proc set 13\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26096 thread 30 bound to OS proc set 14\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26097 thread 31 bound to OS proc set 15\n",
      "OMP: Info #254: KMP_AFFINITY: pid 26015 tid 26098 thread 32 bound to OS proc set 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 0, Time: 30.96\n",
      "mse_b  [16294.189]  mse_f: [82.16595]   total loss: [16376.355]\n",
      "It: 10, Time: 1.71\n",
      "mse_b  [5069.53]  mse_f: [6.1845226]   total loss: [5075.7144]\n",
      "It: 20, Time: 1.36\n",
      "mse_b  [2750.938]  mse_f: [22.093035]   total loss: [2773.031]\n",
      "It: 30, Time: 1.79\n",
      "mse_b  [85.14449]  mse_f: [11.838937]   total loss: [96.98343]\n",
      "It: 40, Time: 1.65\n",
      "mse_b  [1783.4163]  mse_f: [14.218702]   total loss: [1797.635]\n",
      "It: 50, Time: 1.31\n",
      "mse_b  [1726.6447]  mse_f: [6.1706295]   total loss: [1732.8153]\n",
      "It: 60, Time: 1.49\n",
      "mse_b  [98.84054]  mse_f: [12.554775]   total loss: [111.39531]\n",
      "It: 70, Time: 1.52\n",
      "mse_b  [1371.1953]  mse_f: [17.63553]   total loss: [1388.8308]\n",
      "It: 80, Time: 1.31\n",
      "mse_b  [354.00308]  mse_f: [15.53737]   total loss: [369.54047]\n",
      "It: 90, Time: 1.48\n",
      "mse_b  [834.614]  mse_f: [8.127516]   total loss: [842.7415]\n",
      "It: 100, Time: 1.32\n",
      "mse_b  [185.45494]  mse_f: [15.68756]   total loss: [201.1425]\n",
      "It: 110, Time: 1.67\n",
      "mse_b  [775.36694]  mse_f: [17.570923]   total loss: [792.93787]\n",
      "It: 120, Time: 1.33\n",
      "mse_b  [293.4839]  mse_f: [4.3971553]   total loss: [297.88104]\n",
      "It: 130, Time: 1.67\n",
      "mse_b  [206.39499]  mse_f: [2.1571639]   total loss: [208.55215]\n",
      "It: 140, Time: 1.17\n",
      "mse_b  [372.00876]  mse_f: [2.7962682]   total loss: [374.80502]\n",
      "It: 150, Time: 1.36\n",
      "mse_b  [170.57082]  mse_f: [3.4924476]   total loss: [174.06326]\n",
      "It: 160, Time: 1.42\n",
      "mse_b  [338.45856]  mse_f: [5.0692925]   total loss: [343.52786]\n",
      "It: 170, Time: 1.42\n",
      "mse_b  [110.31181]  mse_f: [5.7877746]   total loss: [116.09959]\n",
      "It: 180, Time: 1.72\n",
      "mse_b  [122.58538]  mse_f: [3.1880488]   total loss: [125.77343]\n",
      "It: 190, Time: 1.23\n",
      "mse_b  [226.74042]  mse_f: [1.2188343]   total loss: [227.95926]\n",
      "It: 200, Time: 1.71\n",
      "mse_b  [94.14635]  mse_f: [1.7041875]   total loss: [95.85053]\n",
      "It: 210, Time: 1.31\n",
      "mse_b  [70.07442]  mse_f: [2.6510859]   total loss: [72.7255]\n",
      "It: 220, Time: 1.63\n",
      "mse_b  [143.1961]  mse_f: [1.6592705]   total loss: [144.85538]\n",
      "It: 230, Time: 1.30\n",
      "mse_b  [93.77331]  mse_f: [0.5834977]   total loss: [94.356804]\n",
      "It: 240, Time: 1.77\n",
      "mse_b  [54.40659]  mse_f: [0.6045054]   total loss: [55.011093]\n",
      "It: 250, Time: 1.24\n",
      "mse_b  [82.11639]  mse_f: [0.87324816]   total loss: [82.98963]\n",
      "It: 260, Time: 1.63\n",
      "mse_b  [56.06594]  mse_f: [0.9698214]   total loss: [57.035763]\n",
      "It: 270, Time: 1.43\n",
      "mse_b  [16.23496]  mse_f: [1.1060761]   total loss: [17.341036]\n",
      "It: 280, Time: 1.59\n",
      "mse_b  [45.478878]  mse_f: [1.1804297]   total loss: [46.65931]\n",
      "It: 290, Time: 1.30\n",
      "mse_b  [67.72478]  mse_f: [0.7140522]   total loss: [68.43883]\n",
      "It: 300, Time: 1.67\n",
      "mse_b  [30.938293]  mse_f: [0.44484735]   total loss: [31.38314]\n",
      "It: 310, Time: 1.35\n",
      "mse_b  [13.06264]  mse_f: [0.8984836]   total loss: [13.961123]\n",
      "It: 320, Time: 1.57\n",
      "mse_b  [44.39713]  mse_f: [1.42135]   total loss: [45.818478]\n",
      "It: 330, Time: 1.24\n",
      "mse_b  [47.410595]  mse_f: [1.1288388]   total loss: [48.539433]\n",
      "It: 340, Time: 1.25\n",
      "mse_b  [12.777788]  mse_f: [0.60177505]   total loss: [13.379563]\n",
      "It: 350, Time: 1.74\n",
      "mse_b  [9.374357]  mse_f: [0.31826803]   total loss: [9.692625]\n",
      "It: 360, Time: 1.25\n",
      "mse_b  [32.48119]  mse_f: [0.34400597]   total loss: [32.825195]\n",
      "It: 370, Time: 1.65\n",
      "mse_b  [24.688513]  mse_f: [0.31016648]   total loss: [24.99868]\n",
      "It: 380, Time: 1.40\n",
      "mse_b  [2.9306643]  mse_f: [0.23658164]   total loss: [3.1672459]\n",
      "It: 390, Time: 1.45\n",
      "mse_b  [9.562849]  mse_f: [0.23295619]   total loss: [9.795805]\n",
      "It: 400, Time: 1.40\n",
      "mse_b  [21.26555]  mse_f: [0.29186696]   total loss: [21.557417]\n",
      "It: 410, Time: 1.53\n",
      "mse_b  [12.105002]  mse_f: [0.29998195]   total loss: [12.404984]\n",
      "It: 420, Time: 1.48\n",
      "mse_b  [7.124363]  mse_f: [0.26534256]   total loss: [7.3897057]\n",
      "It: 430, Time: 1.50\n",
      "mse_b  [12.817635]  mse_f: [0.25388438]   total loss: [13.071519]\n",
      "It: 440, Time: 1.40\n",
      "mse_b  [9.528641]  mse_f: [0.2686273]   total loss: [9.797268]\n",
      "It: 450, Time: 1.26\n",
      "mse_b  [6.167366]  mse_f: [0.26780474]   total loss: [6.4351707]\n",
      "It: 460, Time: 1.69\n",
      "mse_b  [11.109862]  mse_f: [0.2296415]   total loss: [11.339504]\n",
      "It: 470, Time: 1.19\n",
      "mse_b  [8.384316]  mse_f: [0.19190921]   total loss: [8.576225]\n",
      "It: 480, Time: 1.70\n",
      "mse_b  [1.8132689]  mse_f: [0.17512383]   total loss: [1.9883927]\n",
      "It: 490, Time: 1.31\n",
      "mse_b  [5.258784]  mse_f: [0.16652767]   total loss: [5.4253116]\n",
      "It: 500, Time: 1.54\n",
      "mse_b  [7.849423]  mse_f: [0.1763807]   total loss: [8.025804]\n",
      "It: 510, Time: 1.33\n",
      "mse_b  [3.0828145]  mse_f: [0.1682397]   total loss: [3.251054]\n",
      "It: 520, Time: 1.19\n",
      "mse_b  [2.5680737]  mse_f: [0.14066513]   total loss: [2.7087388]\n",
      "It: 530, Time: 1.42\n",
      "mse_b  [5.3703537]  mse_f: [0.10383344]   total loss: [5.4741874]\n",
      "It: 540, Time: 1.42\n",
      "mse_b  [2.9962122]  mse_f: [0.06637757]   total loss: [3.06259]\n",
      "It: 550, Time: 1.74\n",
      "mse_b  [1.6097863]  mse_f: [0.05031784]   total loss: [1.6601042]\n",
      "It: 560, Time: 1.17\n",
      "mse_b  [4.2943716]  mse_f: [0.04379067]   total loss: [4.3381624]\n",
      "It: 570, Time: 1.23\n",
      "mse_b  [3.6726155]  mse_f: [0.03825938]   total loss: [3.7108748]\n",
      "It: 580, Time: 1.59\n",
      "mse_b  [1.2063172]  mse_f: [0.03587519]   total loss: [1.2421924]\n",
      "It: 590, Time: 1.26\n",
      "mse_b  [2.2478518]  mse_f: [0.03972209]   total loss: [2.287574]\n",
      "It: 600, Time: 1.22\n",
      "mse_b  [2.759803]  mse_f: [0.04397718]   total loss: [2.8037803]\n",
      "It: 610, Time: 1.78\n",
      "mse_b  [1.1744951]  mse_f: [0.04156962]   total loss: [1.2160647]\n",
      "It: 620, Time: 1.55\n",
      "mse_b  [1.5859673]  mse_f: [0.0337593]   total loss: [1.6197267]\n",
      "It: 630, Time: 1.37\n",
      "mse_b  [2.1302474]  mse_f: [0.02515746]   total loss: [2.1554048]\n",
      "It: 640, Time: 1.91\n",
      "mse_b  [0.64126074]  mse_f: [0.02146469]   total loss: [0.66272545]\n",
      "It: 650, Time: 1.51\n",
      "mse_b  [0.56392807]  mse_f: [0.01900347]   total loss: [0.5829315]\n",
      "It: 660, Time: 1.39\n",
      "mse_b  [1.5553279]  mse_f: [0.01802186]   total loss: [1.5733497]\n",
      "It: 670, Time: 1.31\n",
      "mse_b  [0.80398875]  mse_f: [0.01688077]   total loss: [0.8208695]\n",
      "It: 680, Time: 1.74\n",
      "mse_b  [0.3887215]  mse_f: [0.01440279]   total loss: [0.40312427]\n",
      "It: 690, Time: 1.25\n",
      "mse_b  [1.0352057]  mse_f: [0.01524231]   total loss: [1.0504481]\n",
      "It: 700, Time: 1.63\n",
      "mse_b  [0.6334337]  mse_f: [0.01656043]   total loss: [0.64999413]\n",
      "It: 710, Time: 1.37\n",
      "mse_b  [0.34891123]  mse_f: [0.01876712]   total loss: [0.36767834]\n",
      "It: 720, Time: 1.15\n",
      "mse_b  [0.8786232]  mse_f: [0.01757406]   total loss: [0.89619726]\n",
      "It: 730, Time: 1.62\n",
      "mse_b  [0.53316665]  mse_f: [0.0133486]   total loss: [0.5465152]\n",
      "It: 740, Time: 1.27\n",
      "mse_b  [0.21316099]  mse_f: [0.0107166]   total loss: [0.2238776]\n",
      "It: 750, Time: 1.74\n",
      "mse_b  [0.69537556]  mse_f: [0.00958761]   total loss: [0.70496315]\n",
      "It: 760, Time: 1.37\n",
      "mse_b  [0.54187024]  mse_f: [0.01064567]   total loss: [0.5525159]\n",
      "It: 770, Time: 1.51\n",
      "mse_b  [0.21096893]  mse_f: [0.0097685]   total loss: [0.22073743]\n",
      "It: 780, Time: 1.20\n",
      "mse_b  [0.4050486]  mse_f: [0.00927026]   total loss: [0.41431886]\n",
      "It: 790, Time: 1.71\n",
      "mse_b  [0.23471482]  mse_f: [0.00896151]   total loss: [0.24367633]\n",
      "It: 800, Time: 1.34\n",
      "mse_b  [0.11067771]  mse_f: [0.00875945]   total loss: [0.11943717]\n",
      "It: 810, Time: 1.64\n",
      "mse_b  [0.40050235]  mse_f: [0.00909362]   total loss: [0.40959597]\n",
      "It: 820, Time: 1.28\n",
      "mse_b  [0.26631343]  mse_f: [0.00862451]   total loss: [0.27493793]\n",
      "It: 830, Time: 1.77\n",
      "mse_b  [0.08659411]  mse_f: [0.00873684]   total loss: [0.09533095]\n",
      "It: 840, Time: 1.23\n",
      "mse_b  [0.24378791]  mse_f: [0.0081167]   total loss: [0.2519046]\n",
      "It: 850, Time: 1.74\n",
      "mse_b  [0.15947285]  mse_f: [0.00801871]   total loss: [0.16749157]\n",
      "It: 860, Time: 1.33\n",
      "mse_b  [0.10541881]  mse_f: [0.00750872]   total loss: [0.11292753]\n",
      "It: 870, Time: 1.52\n",
      "mse_b  [0.21645863]  mse_f: [0.00712773]   total loss: [0.22358637]\n",
      "It: 880, Time: 1.24\n",
      "mse_b  [0.0921495]  mse_f: [0.00722519]   total loss: [0.09937468]\n",
      "It: 890, Time: 1.27\n",
      "mse_b  [0.07404735]  mse_f: [0.00786134]   total loss: [0.08190869]\n",
      "It: 900, Time: 1.65\n",
      "mse_b  [0.1886491]  mse_f: [0.00845246]   total loss: [0.19710156]\n",
      "It: 910, Time: 1.30\n",
      "mse_b  [0.10023797]  mse_f: [0.00795907]   total loss: [0.10819704]\n",
      "It: 920, Time: 1.58\n",
      "mse_b  [0.08641554]  mse_f: [0.00675653]   total loss: [0.09317207]\n",
      "It: 930, Time: 1.18\n",
      "mse_b  [0.11658677]  mse_f: [0.00580032]   total loss: [0.1223871]\n",
      "It: 940, Time: 1.30\n",
      "mse_b  [0.03959789]  mse_f: [0.00536983]   total loss: [0.04496772]\n",
      "It: 950, Time: 1.59\n",
      "mse_b  [0.07818785]  mse_f: [0.00530012]   total loss: [0.08348797]\n",
      "It: 960, Time: 1.25\n",
      "mse_b  [0.08479502]  mse_f: [0.00520262]   total loss: [0.08999763]\n",
      "It: 970, Time: 1.67\n",
      "mse_b  [0.01310873]  mse_f: [0.00508611]   total loss: [0.01819484]\n",
      "It: 980, Time: 1.20\n",
      "mse_b  [0.05079687]  mse_f: [0.00495502]   total loss: [0.05575189]\n",
      "It: 990, Time: 1.21\n",
      "mse_b  [0.0473663]  mse_f: [0.00491559]   total loss: [0.05228189]\n",
      "weight_ub: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([2.7848973], dtype=float32)>  weight_fu: <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1.4335109], dtype=float32)>\n",
      "Error u: 4.267285e+01\n",
      "Error v: 4.264223e+01\n",
      "Starting L-BFGS training\n",
      "Step  10 loss 0.01713 \n",
      "Step  20 loss 0.00922 \n",
      "Step  30 loss 0.00740 \n",
      "Step  40 loss 0.00676 \n",
      "Step  50 loss 0.00640 \n",
      "Step  60 loss 0.00606 \n",
      "Step  70 loss 0.00558 \n",
      "Step  80 loss 0.00529 \n",
      "Step  90 loss 0.00515 \n",
      "Step 100 loss 0.00474 \n",
      "Step 110 loss 0.00424 \n",
      "Step 120 loss 0.00379 \n",
      "Step 130 loss 0.00357 \n",
      "Step 140 loss 0.00342 \n",
      "Step 150 loss 0.00582 \n",
      "Step 160 loss 0.00280 \n",
      "Step 170 loss 0.00262 \n",
      "Step 180 loss 0.00203 \n",
      "Step 190 loss 0.00180 \n",
      "Step 200 loss 0.00144 \n",
      "Step 210 loss 0.00137 \n",
      "Step 220 loss 0.00133 \n",
      "Step 230 loss 0.00131 \n",
      "Step 240 loss 0.00124 \n",
      "Step 250 loss 0.00104 \n",
      "Step 260 loss 0.00097 \n",
      "Step 270 loss 0.00096 \n",
      "Step 280 loss 0.00092 \n",
      "Step 290 loss 0.00073 \n",
      "Step 300 loss 0.00068 \n",
      "Step 310 loss 0.00067 \n",
      "Step 320 loss 0.00066 \n",
      "Step 330 loss 0.00059 \n",
      "Step 340 loss 0.00056 \n",
      "Step 350 loss 0.00055 \n",
      "Step 360 loss 0.00055 \n",
      "Step 370 loss 0.00055 \n",
      "Step 380 loss 0.00053 \n",
      "Step 390 loss 0.00047 \n",
      "Step 400 loss 0.00039 \n",
      "Step 410 loss 0.00029 \n",
      "Step 420 loss 0.00029 \n",
      "Step 430 loss 0.00029 \n",
      "Step 440 loss 0.00028 \n",
      "Step 450 loss 0.00027 \n",
      "Step 460 loss 0.00027 \n",
      "Step 470 loss 0.00026 \n",
      "Step 480 loss 0.00025 \n",
      "Step 490 loss 0.00024 \n",
      "Error u: 4.266133e+01\n",
      "Error v: 4.266161e+01\n",
      "Error p: 8.245512e+01\n"
     ]
    }
   ],
   "source": [
    "N_f = 10000\n",
    "Nu1 = 200\n",
    "\n",
    "weight_ub = tf.Variable([1.0], dtype=tf.float32)\n",
    "weight_fu = tf.Variable([1.0], dtype=tf.float32)\n",
    "\n",
    "MSE_b1, MSE_f1, weightu, weightf = fit(collo[::10,:], inlet, outlet, wall, initial,\n",
    "                                       weight_ub, weight_fu,\n",
    "                                         tf_iter=1000, \n",
    "                                         tf_iter2=100, \n",
    "                                         newton_iter1=500,\n",
    "                                         newton_iter2=1500)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1488.1751\n",
      "Error u: 4.266133e+01\n",
      "Error v: 4.266161e+01\n",
      "Error p: 8.245512e+01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "elapsed = time.time() - start_time\n",
    "print('Training time: %.4f' % (elapsed))\n",
    "\n",
    "u_pred, v_pred, p_pred = predict(X_star1)\n",
    "\n",
    "# U_pred = u_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
    "# V_pred = v_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
    "# P_pred = p_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
    "u_exact1 = np.array([X_star1[:, 3]])\n",
    "v_exact1 = np.array([X_star1[:, 4]])\n",
    "p_exact1 = np.array([X_star1[:, 5]])\n",
    "\n",
    "\n",
    "error_uu = np.abs( u_exact1 - u_pred)\n",
    "error_vv = np.abs(v_exact1 - v_pred)\n",
    "error_pp = np.abs( p_exact1 - p_pred)\n",
    "\n",
    "error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
    "print('Error u: %e' % (error_u))\n",
    "\n",
    "error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
    "print('Error v: %e' % (error_v))\n",
    "\n",
    "error_p = np.linalg.norm(p_exact1 - p_pred, 2) / np.linalg.norm(p_exact1, 2)\n",
    "print('Error p: %e' % (error_p))\n",
    "\n",
    "# dataNewNS = 'NS_hisyory.mat'\n",
    "# scipy.io.savemat(dataNewNS, {'w_MSE_b': MSE_b1, 'w_MSE_f': MSE_f1, 'weight_u': weightu,\n",
    "#                   'weight_f': weightf, 'U_pred': u_pred, 'V_pred': V_pred, 'P_pred': P_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
