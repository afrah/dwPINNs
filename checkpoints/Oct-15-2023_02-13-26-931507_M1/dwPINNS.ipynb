{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_24705/3641463767.py:10: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Oct 24 13:02:32 2021\n",
    "\n",
    "@author: lenovo\n",
    "\"\"\"\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "import math\n",
    "import matplotlib.gridspec as gridspec\n",
    "from plotting import newfig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import layers, activations\n",
    "from scipy.interpolate import griddata\n",
    "from eager_lbfgs import lbfgs, Struct\n",
    "from pyDOE import lhs\n",
    "\n",
    "# from generate_dataset import *\n",
    "# from utilities import *\n",
    "import os.path\n",
    "\n",
    "import shutil\n",
    "import datetime\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_net(object):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "\n",
    "        self.layer_sizes = layers\n",
    "        sizes_w = []\n",
    "        sizes_b = []\n",
    "        for i, width in enumerate(self.layer_sizes):\n",
    "            if i != 1:\n",
    "                sizes_w.append(int(width * self.layer_sizes[1]))\n",
    "                sizes_b.append(int(width if i != 0 else self.layer_sizes[1]))\n",
    "\n",
    "\n",
    "        # L-BFGS weight getting and setting from https://github.com/pierremtb/PINNs-TF2.0\n",
    "\n",
    "    def set_weights(self, model, w, sizes_w, sizes_b):  # 重新设置参数\n",
    "\n",
    "        for i, layer in enumerate(model.layers[1:len(sizes_w) + 1]):\n",
    "            start_weights = sum(sizes_w[:i]) + sum(sizes_b[:i])\n",
    "            end_weights = sum(sizes_w[:i + 1]) + sum(sizes_b[:i])\n",
    "            weights = w[start_weights:end_weights]\n",
    "            w_div = int(sizes_w[i] / sizes_b[i])\n",
    "            weights = tf.reshape(weights, [w_div, sizes_b[i]])\n",
    "            biases = w[end_weights:end_weights + sizes_b[i]]\n",
    "            weights_biases = [weights, biases]\n",
    "            layer.set_weights(weights_biases)\n",
    "\n",
    "\n",
    "    def get_weights(self, model):\n",
    "        w = []\n",
    "        for layer in model.layers[1:len(sizes_w) + 1]:\n",
    "            weights_biases = layer.get_weights()\n",
    "            weights = weights_biases[0].flatten()\n",
    "            biases = weights_biases[1]\n",
    "            w.extend(weights)\n",
    "            w.extend(biases)\n",
    "        w = tf.convert_to_tensor(w)\n",
    "        return w\n",
    "\n",
    "    def xavier_init(self, layer_sizes):\n",
    "        in_dim = layer_sizes[0]\n",
    "        out_dim = layer_sizes[1]\n",
    "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "\n",
    "    def model(self):\n",
    "\n",
    "        input_tensor = keras.Input(shape=(self.layer_sizes[0],))\n",
    "\n",
    "        hide_layer_list = []\n",
    "        flag = True\n",
    "        for width in self.layer_sizes[1:-1]:\n",
    "            if flag:\n",
    "                x = layers.Dense(\n",
    "                    width, activation=tf.nn.tanh,\n",
    "                    kernel_initializer=\"glorot_normal\")(input_tensor)\n",
    "                flag = False\n",
    "            else:\n",
    "                x = layers.Dense(\n",
    "                    width, activation=tf.nn.tanh,\n",
    "                    kernel_initializer=\"glorot_normal\")(x)\n",
    "        output_tensor = layers.Dense(self.layer_sizes[-1], activation=None,kernel_initializer=\"glorot_normal\")(x)\n",
    "        print(\"xxxxxxxxxxxxxx\")\n",
    "        output0 = output_tensor[:, 0:1]\n",
    "        output1 = output_tensor[:, 1:2]\n",
    "        output2 = output_tensor[:, 2:3]\n",
    "\n",
    "        model_output = keras.models.Model(input_tensor, [output0, output1, output2])\n",
    "        model_output.summary()\n",
    "\n",
    "        return model_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoD_BFS_Slip_PINN:\n",
    "\n",
    "    def __init__(self, data, layers  , activFun , mode , starter_learning_rate = 1.0e-3 , ExistModel=0):\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "        self.layers = layers # \n",
    "        self.weight_initial = tf.Variable([1.0], dtype=tf.float32)\n",
    "        self.weight_wall = tf.Variable([1.0], dtype=tf.float32)\n",
    "        self.weight_inlet = tf.Variable([1.0], dtype=tf.float32)\n",
    "        self.weight_outlet = tf.Variable([1.0], dtype=tf.float32)\n",
    "\n",
    "        self.weight_fu = tf.Variable([1.0], dtype=tf.float32)\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=0.005, beta_1=.99)\n",
    "        self.optimizer_fu= tf.keras.optimizers.Adam(lr=0.003, beta_1=.99)\n",
    "        self.optimizer_initial = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
    "        self.optimizer_wall = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
    "        self.optimizer_inlet = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
    "        self.optimizer_outlet = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
    "\n",
    "        # initialize the NN\n",
    "        self.net_cuvwp = neural_net(layers).model()\n",
    "\n",
    "        # self.loss_tensor_list = [ self.loss_OUTLET,  self.loss_INLET , self.loss_WALL, self.loss_res, self.loss_INITIAL] \n",
    "        self.loss_list = [\"loss_OUTLET\" , \"loss_INLET\", \"loss_WALL\" , \"loss_Phys\", \"loss_Initial\" ]\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "\n",
    "   \n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        loss_list = [\"loss_OUTLET\" , \"loss_INLET\", \"loss_WALL\" ,\"loss_Initial\"]\n",
    "        ax = plt.subplot(1 , 2 , 1)\n",
    "    #         fig.set_size_inches([15,8])\n",
    "        for key in loss_list:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=7)\n",
    "        ax.set_ylabel(\"loss\", fontsize=7)\n",
    "        ax.set_yscale('log')\n",
    "        ax.tick_params(labelsize=7)\n",
    "        ax.legend()\n",
    "\n",
    "\n",
    "        ax = plt.subplot(1,2,2)\n",
    "        self.print(\"Final loss %s: %e\" % (\"loss_res\", self.loss_history[\"loss_res\"][-1]))\n",
    "        ax.semilogy(self.loss_history[\"loss_res\"], label=\"loss_res\")\n",
    "        ax.set_xlabel(\"epochs\", fontsize=7)\n",
    "        ax.set_ylabel(\"loss\", fontsize=7)\n",
    "        ax.set_yscale('log')\n",
    "        ax.tick_params(labelsize=7)\n",
    "        ax.legend()\n",
    "        plt.savefig(path , bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "\n",
    "    @tf.function\n",
    "    def net_f(self,t ,  x, y):\n",
    "\n",
    "        mu = 0.00345\n",
    "        density = 1056\n",
    "\n",
    "        u, v, p = self.net_cuvwp(tf.concat([t, x , y],1))\n",
    "\n",
    "        \n",
    "        u_t = tf.gradients(u, t)[0]\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        u_y = tf.gradients(u, y)[0]\n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "        u_yy = tf.gradients(u_y, y)[0]\n",
    "\n",
    "        v_t = tf.gradients(v, t)[0]\n",
    "        v_x = tf.gradients(v, x)[0]\n",
    "        v_y = tf.gradients(v, y)[0]\n",
    "        v_xx = tf.gradients(v_x, x)[0]\n",
    "        v_yy = tf.gradients(v_y, y)[0]\n",
    "\n",
    "        p_x = tf.gradients(p, x)[0]\n",
    "        p_y = tf.gradients(p, y)[0]\n",
    "\n",
    "        f_u = u_t + (u * u_x + v * u_y ) + 1.0/density * p_x - mu *(u_xx + u_yy )\n",
    "        f_v = v_t + (u * v_x + v * v_y ) + 1.0/density * p_y - mu *(v_xx + v_yy )\n",
    "        div = u_x + v_y \n",
    "            \n",
    "\n",
    "        return f_u, f_v, div\n",
    "\n",
    "\n",
    "    # define the loss\n",
    "    def loss_funct(self ,t_c_tf , x_c_tf, y_c_tf, t_WALL_tf , x_WALL_tf, y_WALL_tf, p_WALL_tf ,\n",
    "                    t_INLET_tf , x_INLET_tf, y_INLET_tf, u_INLET_tf ,  v_INLET_tf ,t_OUTLET_tf , x_OUTLET_tf, y_OUTLET_tf, u_OUTLET_tf , v_OUTLET_tf,\n",
    "                    t_INITIAL_tf , x_INITIAL_tf, y_INITIAL_tf , u_INITIAL_tf, v_INITIAL_tf, p_INITIAL_tf ):\n",
    "\n",
    "\n",
    "        [f_u, f_v, f_e] = self.net_f(t_c_tf ,x_c_tf ,y_c_tf)\n",
    "    \n",
    "        [u_WALL_pred, v_WALL_pred, p_WALL_pred] = self.net_cuvwp(tf.concat([t_WALL_tf, x_WALL_tf,y_WALL_tf],1))\n",
    "        \n",
    "        [u_INLET_pred, v_INLET_pred, p_INLET_pred] = self.net_cuvwp(tf.concat([t_INLET_tf, x_INLET_tf,y_INLET_tf],1))\n",
    "\n",
    "        [u_OUTLET_pred, v_OUTLET_pred,  p_OUTLET_pred ]= self.net_cuvwp(tf.concat([t_OUTLET_tf,x_OUTLET_tf, y_OUTLET_tf],1))\n",
    "\n",
    "        \n",
    "        [u_INITIAL_pred, v_INITIAL_pred,  p_INITIAL_pred ]= self.net_cuvwp( tf.concat([t_INITIAL_tf, x_INITIAL_tf, y_INITIAL_tf  ],1))\n",
    "\n",
    "        loss_res = self.weight_fu *( tf.reduce_mean(tf.square(f_u)) + 1.0 *  tf.reduce_mean(tf.square(f_v)) + tf.reduce_mean(tf.square(f_e)))\n",
    "        \n",
    "        ##############################################################################\n",
    "        # loss_res = 1.0 *tf.reduce_mean(tf.square(u_c_pred  - u_c_tf)) + 1 * tf.reduce_mean(tf.square(v_c_pred  -v_c_tf )) + 1 * tf.reduce_mean(tf.square(p_c_pred - p_c_tf)) \n",
    "    \n",
    "    # on the Wall :supervised leaning:  U_predict - Uexact , V_predict - V_exact and  P_predict - P_exact and the gradient : P_y = 0  \n",
    "        loss_wall =   self.weight_wall *  (tf.reduce_mean(tf.square(u_WALL_pred ))  + 1.0* tf.reduce_mean(tf.square(v_WALL_pred)))\n",
    "                                                                                                        # + 1000 * tf.reduce_mean(tf.square(p_WALL_pred - p_WALL_tf)) + tf.square(tf.gradients(p_WALL_pred, y_WALL_tf)[0])) \n",
    "        \n",
    "        \n",
    "    # on the INLET :supervised leaning:  U_predict - Uexact , V_predict - V_exact and  P_predict - P_exact and the gradient : P_x = 0  \n",
    "        loss_inlet =   self.weight_inlet  * ( tf.reduce_mean(tf.square(u_INLET_pred - u_INLET_tf))  +  1.0 * tf.reduce_mean(tf.square(v_INLET_pred -v_INLET_tf )))\n",
    "        #  + \\                1000.0 * tf.reduce_mean(tf.square(p_INLET_pred - p_INLET_tf)) #+ tf.square(tf.gradients(p_INLET_pred, x_INLET_tf)[0])) \n",
    "        \n",
    "    # on the OUTLET :supervised leaning:  U_predict - Uexact , V_predict - V_exact and  P_predict - P_exact and the gradient : U_x = 0 and V_x = 0   \n",
    "        loss_outlet =   self.weight_outlet * (tf.reduce_mean(tf.square(p_OUTLET_pred )) )# 50.0  *    tf.reduce_mean(tf.square(u_OUTLET_pred - u_OUTLET_tf) + tf.square(tf.gradients(u_OUTLET_pred , x_OUTLET_tf)[0])) + \\        2000.0 * tf.reduce_mean(tf.square(v_OUTLET_pred - v_OUTLET_tf) + tf.square(tf.gradients(v_OUTLET_pred , x_OUTLET_tf)[0])) +  \n",
    "        \n",
    "\n",
    "    # on the Initial condition :supervised leaning:  U_predict - Uexact , V_predict - V_exact and  P_predict - P_exact \n",
    "        loss_initial = self.weight_initial *  (tf.reduce_mean(tf.square(u_INITIAL_pred - u_INITIAL_tf)) +  tf.reduce_mean(tf.square(v_INITIAL_pred - v_INITIAL_tf)) +  tf.reduce_mean(tf.square(p_INITIAL_pred - p_INITIAL_tf)))\n",
    "\n",
    "        \n",
    "        loss = loss_wall + loss_initial + loss_res + loss_inlet + loss_outlet\n",
    "\n",
    "        return  loss_res, loss_wall , loss_inlet , loss_outlet , loss_initial , loss\n",
    "    \n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def grad(self ,t_c_tf , x_c_tf, y_c_tf,\n",
    "             t_WALL_tf , x_WALL_tf, y_WALL_tf, p_WALL_tf ,\n",
    "               t_INLET_tf , x_INLET_tf, y_INLET_tf, u_INLET_tf ,  v_INLET_tf ,\n",
    "                 t_OUTLET_tf , x_OUTLET_tf, y_OUTLET_tf, u_OUTLET_tf , v_OUTLET_tf,\n",
    "                 t_INITIAL_tf , x_INITIAL_tf, y_INITIAL_tf , u_INITIAL_tf, v_INITIAL_tf, p_INITIAL_tf ):\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            loss_res, loss_wall , loss_inlet , loss_outlet , loss_initial , loss = self.loss_funct(t_c_tf , x_c_tf, y_c_tf, t_WALL_tf , x_WALL_tf, y_WALL_tf, p_WALL_tf ,\n",
    "                                                                                                   t_INLET_tf , x_INLET_tf, y_INLET_tf, u_INLET_tf ,  v_INLET_tf ,t_OUTLET_tf , x_OUTLET_tf, y_OUTLET_tf, u_OUTLET_tf , v_OUTLET_tf,\n",
    "                                                                                                     t_INITIAL_tf , x_INITIAL_tf, y_INITIAL_tf , u_INITIAL_tf, v_INITIAL_tf, p_INITIAL_tf )\n",
    "            grads = tape.gradient(loss, self.net_cuvwp.trainable_variables)\n",
    "\n",
    "            grads_initial = tape.gradient(loss, self.weight_initial)\n",
    "            grads_fu = tape.gradient(loss, self.weight_fu)\n",
    "\n",
    "            grads_wall = tape.gradient(loss, self.weight_wall)\n",
    "            grads_inlet = tape.gradient(loss, self.weight_inlet)\n",
    "\n",
    "            grads_outlet = tape.gradient(loss, self.weight_outlet)\n",
    "\n",
    "\n",
    "        return   loss_res, loss_wall , loss_inlet , loss_outlet , loss_initial , loss, grads , grads_fu , grads_initial, grads_wall , grads_inlet , grads_outlet\n",
    "\n",
    "\n",
    "\n",
    "#model.train(collo[::10,:], inlet, outlet, wall, initial,  weight_ub, weight_fu, tf_iter=1000,   tf_iter2=100, newton_iter2=1500)\n",
    "    def train( self , collo, inlet, outlet, wall, initial ,  tf_iter, tf_iter2, newton_iter2):\n",
    "\n",
    "        batch_size =  120\n",
    "        n_batches =  collo.shape[0] // batch_size\n",
    "        start_time = time.time()\n",
    "\n",
    "        running_time = 0\n",
    "\n",
    "\n",
    "        # tf.print(f\"weight_initial: {self.weight_initial}  weight_fu: {self.weight_fu}\")\n",
    "        print(\"starting Adam training\")\n",
    "\n",
    "        a = np.random.rand(1000)\n",
    "        loss_history = list(a)\n",
    "        MSE_b0 = list(a)\n",
    "        MSE_f0 = list(a)\n",
    "\n",
    "        MSE_b1 = []\n",
    "        MSE_f1 = []\n",
    "        # print(wall)\n",
    "        # For mini-batch (if used)\n",
    "        for epoch in range(tf_iter):\n",
    "            for it in range(n_batches):\n",
    "\n",
    "                t_WALL_tf= wall[: , 0:1]\n",
    "                x_WALL_tf= wall[: , 1:2]\n",
    "                y_WALL_tf= wall[: , 2:3]\n",
    "                p_WALL_tf= wall[: , 5:6]\n",
    "\n",
    "                t_INLET_tf= inlet[: , 0:1]\n",
    "                x_INLET_tf= inlet[: , 1:2]\n",
    "                y_INLET_tf= inlet[: , 2:3]\n",
    "                u_INLET_tf= inlet[: , 3:4]\n",
    "                v_INLET_tf= inlet[: , 4:5]\n",
    "                p_INLET_tf= inlet[: , 5:6]\n",
    "\n",
    "                t_OUTLET_tf= outlet[: , 0:1]\n",
    "                x_OUTLET_tf= outlet[: , 1:2]\n",
    "                y_OUTLET_tf= outlet[: , 2:3]\n",
    "                u_OUTLET_tf= outlet[: , 3:4]\n",
    "                v_OUTLET_tf= outlet[: , 4:5]\n",
    "\n",
    "                t_INITIAL_tf= initial[: , 0:1]\n",
    "                x_INITIAL_tf= initial[: , 1:2]\n",
    "                y_INITIAL_tf= initial[: , 2:3]\n",
    "                u_INITIAL_tf= initial[: , 3:4]\n",
    "                v_INITIAL_tf= initial[: , 4:5]\n",
    "                p_INITIAL_tf= initial[: , 5:6]\n",
    "\n",
    "\n",
    "                x_f_tf = collo[:, 1:2][it * batch_size:(it * batch_size + batch_size), ]\n",
    "                y_f_tf = collo[:, 2:3][it * batch_size:(it * batch_size + batch_size), ]\n",
    "                t_f_tf = collo[:, 0:1][it * batch_size:(it * batch_size + batch_size), ]\n",
    "                \n",
    "                elapsed = time.time() - start_time\n",
    "                running_time += elapsed\n",
    "\n",
    "                loss_res, loss_wall , loss_inlet , loss_outlet , loss_initial , loss,grads , grads_fu , grads_initial, grads_wall , grads_inlet , grads_outlet = self.grad( t_f_tf , x_f_tf, y_f_tf,\n",
    "                                                                                                                                                         t_WALL_tf , x_WALL_tf, y_WALL_tf, p_WALL_tf ,\n",
    "                                                                                                                                                           t_INLET_tf , x_INLET_tf, y_INLET_tf, u_INLET_tf ,  v_INLET_tf ,\n",
    "                                                                                                                                                             t_OUTLET_tf , x_OUTLET_tf, y_OUTLET_tf, u_OUTLET_tf , v_OUTLET_tf,\n",
    "                                                                                                                                                               t_INITIAL_tf , x_INITIAL_tf, y_INITIAL_tf , u_INITIAL_tf, v_INITIAL_tf, p_INITIAL_tf )\n",
    "                \n",
    "                \n",
    "\n",
    "                # print(\"starting Adam training\")\n",
    "        # self.loss_list = [\"loss_OUTLET\" , \"loss_INLET\", \"loss_WALL\" , \"loss_Phys\", \"loss_Initial\" ]\n",
    "\n",
    "                self.optimizer.apply_gradients(zip(grads, self.net_cuvwp.trainable_variables))\n",
    "\n",
    "\n",
    "                batch_losses =[loss_outlet.numpy()[0], loss_inlet.numpy()[0] , loss_wall.numpy()[0] , loss_res.numpy()[0] ,loss_initial.numpy()[0] ]\n",
    "\n",
    "                self.assign_batch_losses(batch_losses)\n",
    "                for key in self.loss_history:\n",
    "                    self.loss_history[key].append(self.epoch_loss[key])\n",
    "                start_time = time.time()\n",
    "                                    \n",
    "                # if loss_history[-1] < loss_history[-2] and loss_history[-2] < loss_history[-3] and loss_history[-1] < loss_history[-10]:\n",
    "                self.optimizer_outlet.tf.apply_gradients(zip([-grads_outlet], [self.weight_outlet]))\n",
    "                self.optimizer_inlet.apply_gradients(zip([-grads_inlet], [self.weight_inlet]))\n",
    "                self.optimizer_wall.apply_gradients(zip([-grads_wall], [self.weight_wall]))\n",
    "                self.optimizer_initial.apply_gradients(zip([-grads_initial], [self.weight_initial]))\n",
    "                self.optimizer_fu.apply_gradients(zip([-grads_fu], [self.weight_fu]))\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                self.print('Epoch: %d| It: %d| Loss: %.3e| physLoss: %.3e | lINITIAL: %.3e | wLoss: %.3e | inLoss: %.3e | outLoss: %.3e |Time: %.2fs | rTime: %.3eh | LR: %.3e'\n",
    "                          %(epoch , it, loss.numpy()[0], loss_res.numpy()[0], loss_initial.numpy()[0] , loss_wall.numpy()[0] , loss_inlet.numpy()[0] , loss_outlet.numpy()[0] , elapsed, running_time, 0.005))\n",
    "\n",
    "                # wu = self.weight_initial.numpy()\n",
    "                # wf = self.weight_fu.numpy()\n",
    "  \n",
    "        \n",
    "\n",
    "        return #MSE_b1, MSE_f1,  weightu, weightf\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "            \n",
    "            \n",
    "\n",
    "    # L-BFGS implementation from https://github.com/pierremtb/PINNs-TF2.0\n",
    "    def get_loss_and_flat_grad ( self , t_f_batch , x_f_batch, y_f_batch, xb_batch, yb_batch, tb_batch, ub_batch, vb_batch,weight_ub, weight_fu):\n",
    "        def loss_and_flat_grad(w):\n",
    "            with tf.GradientTape() as tape:\n",
    "                self.net_cuvwp.set_weights( w, self.net_cuvwp.sizes_w, self.net_cuvwp.sizes_b)\n",
    "                loss_value, _, _ = self.loss_funct(x_f_batch, y_f_batch, t_f_batch, xb_batch, yb_batch, tb_batch, ub_batch, vb_batch, weight_ub, weight_fu)\n",
    "            grad = tape.gradient(loss_value, self.net_cuvwp.trainable_variables)\n",
    "            grad_flat = []\n",
    "            for g in grad:\n",
    "                grad_flat.append(tf.reshape(g, [-1]))\n",
    "            grad_flat = tf.concat(grad_flat, 0)\n",
    "            # print(loss_value, grad_flat)\n",
    "            return loss_value, grad_flat\n",
    "\n",
    "        return loss_and_flat_grad\n",
    "\n",
    "\n",
    "    def predict(self , t , x , y):\n",
    "        X_star = tf.concat([t,x, y],1) # tf.convert_to_tensor(np.array([t , x, y]), dtype=tf.float32)\n",
    "\n",
    "        u, v, p = self.net_cuvwp(tf.concat([ X_star[:, 0:1] , X_star[:, 1:2], X_star[:, 2:3]], 1))\n",
    "\n",
    "        return u.numpy(), v.numpy(), p.numpy()\n",
    "\n",
    "############################################################\n",
    "   ############################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        text = datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode\n",
    "        dirname = os.path.abspath(os.path.join(\"checkpoints\", text))\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        shutil.copyfile('dwPINNS.ipynb', os.path.join(dirname, 'dwPINNS.ipynb'))\n",
    "\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    #  ############################################################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/noslipwall_02_Z0slice_2D/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/noslipwall_02_Z0slice_2D/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/noslipwall_02_Z0slice_2D/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/noslipwall_02_Z0slice_2D/M1.py', os.path.join(dirname, 'M1.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "    \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "\n",
    "def get_training_dataset(path , pINLET , pOUTLET , pWALL , pDomain , pInitial , dist):\n",
    "    \n",
    "    data = h5py.File(path , 'r')  # load dataset from matlab\n",
    "    WALL = np.transpose(data['noslipwall_02_Z0slice_2D_wall'], axes=range(len(data['noslipwall_02_Z0slice_2D_wall'].shape) - 1,-1, -1)).astype(np.float32)\n",
    "    \n",
    "    WALL = np.delete(WALL , np.where(WALL[:,0] == WALL[:,0].min())[0] , 0)  \n",
    "    # WALL = np.delete(WALL , np.where(WALL[:,1] <= 0.2)[0] , 0)  \n",
    "\n",
    "\n",
    "    domain = np.transpose(data['noslipwall_02_Z0slice_2D_innerdomain'], axes=range(len(data['noslipwall_02_Z0slice_2D_innerdomain'].shape) - 1,-1, -1)).astype(np.float32)\n",
    "    \n",
    "    domain = np.delete(domain , np.where(domain[:,0] == domain[:,0].min())[0] , 0)  \n",
    "    # domain = np.delete(domain , np.where(domain[:,1] <= 0.2)[0] , 0)  \n",
    "\n",
    "\n",
    "    # INLET = np.transpose(data['noslipwall_02_Z0slice_2D_inlet'],  axes=range(len(data['noslipwall_02_Z0slice_2D_inlet'].shape) - 1,-1, -1)).astype(np.float32)\n",
    "        \n",
    "    INLET = domain[np.where(domain[:,1] == domain[:,1].min())[0],:] #np.delete(INLET , np.where(INLET[:,0] == INLET[:,0].min())[0] , 0)  \n",
    "\n",
    "    OUTLET = np.transpose(data['noslipwall_02_Z0slice_2D_outlet'], axes=range(len(data['noslipwall_02_Z0slice_2D_outlet'].shape) - 1,-1, -1)).astype(np.float32)\n",
    "    \n",
    "    OUTLET = np.delete(OUTLET , np.where(OUTLET[:,0] == OUTLET[:,0].min())[0] , 0)  \n",
    "\n",
    "    total = INLET.shape[0] + OUTLET.shape[0] + domain.shape[0] +  WALL.shape[0] \n",
    "    \n",
    "    np.random.seed(1234)\n",
    "\n",
    "    # initial domain ux is 0.2 and other values are zero. FOr wall, all values are zero\n",
    "    \n",
    "    INITIALd = domain[np.where(domain[:,0] == domain[:,0].min())[0],:] # initial doamin corressponds to all values where t is zero\n",
    "\n",
    "    INITIALw = WALL[np.where(WALL[:,0] == WALL[:,0].min())[0],:] # initial doamin corressponds to all values where t is zero\n",
    "\n",
    "    INITIALi = INLET[np.where(INLET[:,0] == INLET[:,0].min())[0],:] # initial doamin corressponds to all values where t is zero\n",
    "\n",
    "    INITIALo = OUTLET[np.where(OUTLET[:,0] == OUTLET[:,0].min())[0],:] # initial doamin corressponds to all values where t is zero\n",
    "\n",
    "    #random selection of training data\n",
    "    INITIAL = np.concatenate([INITIALd , INITIALo],0)\n",
    "    \n",
    "    # domain = np.concatenate([domain , WALL , INLET , OUTLET],0)\n",
    "\n",
    "    #INLET = np.delete(INLET , idx_initi , 0)  \n",
    "    #OUTLET = np.delete(OUTLET , idx_inito  , 0)  \n",
    "    #domain = np.delete(domain , idx_initd , 0)  \n",
    "    #WALL = np.delete(WALL , idx_initw , 0)  \n",
    "    \n",
    "    if dist == \"Sobol\":\n",
    "        idxi = generate_sobol_sequence(0 , INLET.shape[0] ,  int(INLET.shape[0] * pINLET)) \n",
    "        INLET = INLET[idxi, :]\n",
    "        idxi = generate_sobol_sequence(0 , OUTLET.shape[0] ,  int(OUTLET.shape[0] * pOUTLET)) \n",
    "        OUTLET = OUTLET[idxi, :]\n",
    "        idxi = generate_sobol_sequence(0 , INITIAL.shape[0] ,  int(INITIAL.shape[0] * pInitial)) \n",
    "        INITIAL = INITIAL[idxi, :]\n",
    "        idxi = generate_sobol_sequence(0 , WALL.shape[0] ,  int(WALL.shape[0] * pWALL)) \n",
    "        WALL = WALL[idxi, :]\n",
    "        idxi = generate_sobol_sequence(0 , domain.shape[0] ,  int(domain.shape[0] * pDomain)) \n",
    "        domain = domain[idxi, :]\n",
    "    else:\n",
    "        idxi = np.random.choice(INLET.shape[0], int(INLET.shape[0] * pINLET), replace=False)\n",
    "        INLET = INLET[idxi, :]\n",
    "        idxi = np.random.choice(OUTLET.shape[0], int(OUTLET.shape[0] * pOUTLET), replace=False)\n",
    "        OUTLET = OUTLET[idxi, :]\n",
    "        idxi = np.random.choice(INITIAL.shape[0], int(INITIAL.shape[0] * pInitial), replace=False)\n",
    "        INITIAL = INITIAL[idxi, :]\n",
    "        idxi = np.random.choice(WALL.shape[0], int(WALL.shape[0] * pWALL), replace=False)\n",
    "        WALL = WALL[idxi, :]\n",
    "        idxi = np.random.choice(domain.shape[0], int(domain.shape[0] * pDomain), replace=False)\n",
    "        domain = domain[idxi, :]\n",
    "\n",
    "    #########################################\n",
    "    return [domain , INLET , OUTLET, WALL, INITIAL , total]\n",
    "################################################################\n",
    "\n",
    "\n",
    "\n",
    "def plot_result2(model ,path ):\n",
    "\n",
    "\n",
    "    ######################################\n",
    "    # model.save_NN()\n",
    "\n",
    "    # list_ = [ \"noslipwall_02_Z0slice_2D_wall\", \"noslipwall_02_Z0slice_2D_innerdomain\" ]\n",
    "    # N_data = [ 1600 , 20800 ]\n",
    "\n",
    "    # peotDic = dict((key,value) for key,value in zip(list_,N_data))\n",
    "\n",
    "    # tstep = [ 100 , 100 , ]\n",
    "    # tstepList = dict((key,value) for key,value in zip(list_,tstep))\n",
    "\n",
    "    # for key,value in peotDic.items():\n",
    "    #     # print(key, value)\n",
    "    #     test_data_inlet = get_testing_dataset(path   , part=key)\n",
    "    #     test_data_inlet = test_data_inlet[np.argsort(test_data_inlet[:, 0])]\n",
    "\n",
    "    #     [_ ,xf , _ , ufa , vfa  , pfa, u_pred , v_pred, p_pred]  = predict_result(model , test_data_inlet ,  value ,  tstep  ,  text=key , stm = False)\n",
    "    #     peotDic[key] =  error_over_time(tstepList[key] ,value , ufa , vfa , pfa , u_pred , v_pred , p_pred )\n",
    "    #     l1l2Erorr(key , u_pred ,v_pred , p_pred , ufa , vfa , pfa , model)\n",
    "\n",
    "    # draw_error_over_time(peotDic , model.dirname)\n",
    "\n",
    "    #     ## drawing velocity profile\n",
    "    # xValues = [5.000e-04 , 0.2275, 9.995e-01]\n",
    "    # # UVelocity_profile(model.dirname , 0.2 , xValues , xf , u_pred)\n",
    "\n",
    "    \n",
    "    tstep =100\n",
    "    N_data = 20800\n",
    "    #def predict_result(model , test_data , N_data , tstep  , text = 'all'  , stm = False):\n",
    "    part = 'noslipwall_02_Z0slice_2D_innerdomain'\n",
    "    test_data_innerdomain = get_testing_dataset(path    ,  part=part)\n",
    "    [tf , xf , yf , ufa , vfa  , pfa, u_pred , v_pred, p_pred]   = predict_result(model , test_data_innerdomain , N_data ,  tstep ,  text='domain' ,  stm = False)\n",
    "\n",
    "    \n",
    "\n",
    "    x = xf.reshape(tstep,N_data)[0,:]\n",
    "    y = yf.reshape(tstep,N_data)[0,:]\n",
    "    t = tf.reshape(tstep,N_data)[:,0].T\n",
    "    ufa = ufa.reshape(tstep,N_data)\n",
    "    vfa = vfa.reshape(tstep,N_data)\n",
    "    pfa = pfa.reshape(tstep,N_data)\n",
    "    u_pred = u_pred.reshape(tstep,N_data)\n",
    "    v_pred = v_pred.reshape(tstep,N_data)\n",
    "    p_pred = p_pred.reshape(tstep,N_data)\n",
    "    error_u =  np.abs(ufa - u_pred) # (u - u_pred) / u # np.abs(u - u_pred)\n",
    "    error_v =  np.abs(vfa - v_pred) # (v - v_pred) / v # np.abs(v - v_pred)\n",
    "    error_p =  np.abs(pfa - p_pred) # (p - p_pred) / p # np.abs(p - p_pred)\n",
    "\n",
    "    data = [u_pred, v_pred , p_pred , ufa, vfa , pfa , error_u ,  error_v ,error_p ]\n",
    "\n",
    "    plot_time_profile(model.dirname , x , y , t , ufa , u_pred , '$u$')\n",
    "    plot_time_profile(model.dirname , x , y , t , vfa , v_pred , '$v$')\n",
    "    plot_time_profile(model.dirname , x , y , t , pfa , p_pred , '$p$')\n",
    "\n",
    "    draw_contourf(t , x , y , data , 1.0 ,10 , model.dirname , 5 , fontsize=17.5 , labelsize=12.5 , axes_pad=1.2)\n",
    "\n",
    "    model.weight_change_per_layer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxxxxxx\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 20)           80          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 20)           420         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 20)           420         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 3)            63          dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [(None, 1)]          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 1)]          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [(None, 1)]          0           dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 983\n",
      "Trainable params: 983\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from generate_dataset import * \n",
    "# path = '/okyanus/users/afarea/dataDir/noslipwall_02_Z0slice_2D/noslipwall_02_Z0slice_2D.mat'\n",
    "path = '/media/afrah2/MyWork/files2023/dataset/noslipwall_02_Z0slice_2D.mat'\n",
    "\n",
    "modelPath = ''\n",
    "\n",
    "\n",
    "pINLET = 1\n",
    "pOUTLET= 0.3\n",
    "pWALL =  0.006\n",
    "pDomain = 0.0007\n",
    "pInitial = 0.07\n",
    "\n",
    "dist =  \"Sobol\"\n",
    "activFun = 'hard_swish'\n",
    "\n",
    "[collo , inlet , outlet, wall, initial , total] = get_training_dataset(path , pINLET , pOUTLET , pWALL , pDomain , pInitial , dist)\n",
    "\n",
    "XY_c = np.concatenate([collo , wall, inlet , outlet], 0) \n",
    "\n",
    "\n",
    "\n",
    "# # Define model\n",
    "mode = 'M1'\n",
    "\n",
    "input_dimension = 3\n",
    "output_dimension = 3\n",
    "n_hidden_layers = 3\n",
    "neurons = 20 # [3, 20, 20, 20, 20, 20, 20, 20, 3]\n",
    "\n",
    "starter_learning_rate = float(0.005)\n",
    "epochs = 5\n",
    "model_layers = [input_dimension] + n_hidden_layers*[neurons] + [output_dimension]\n",
    "\n",
    "batch_size= 300\n",
    "\n",
    "iterations = 40000\n",
    "\n",
    "method  = \"mini_batch\"\n",
    "\n",
    "\n",
    "###############################################################\n",
    "model = TwoD_BFS_Slip_PINN(XY_c , model_layers  , activFun , mode , starter_learning_rate , ExistModel = modelPath)\n",
    "\n",
    "\n",
    "# model.print(\"Using mode: \" , model.mode)\n",
    "# model.print(\"neural network: \" , model.layers )\n",
    "# model.print(\"collocation Training data size : \" ,collo.shape[0], \" (\" ,str(pDomain*100) , \"%)\")\n",
    "# model.print(\"Wall Training data size: \" ,wall.shape[0], \" (\" ,str(pWALL*100) , \"%)\")\n",
    "# model.print(\"Initial Training data size: \" ,initial.shape[0], \" (\" ,str(pInitial*100) , \"%)\")\n",
    "# model.print(\"INLET Training data size: \" ,inlet.shape[0], \" (\" ,str(pINLET*100) , \"%)\")\n",
    "# model.print(\"OUTLET Training data size: \" ,outlet.shape[0], \" (\" ,str(pOUTLET*100) , \"%)\")\n",
    "\n",
    "# model.print(\"Total training datset size: \" ,XY_c.shape[0], \" (\" ,str((XY_c.shape[0] / total)*100) , \"%)\")\n",
    "# model.print(\"Activation function: \" , activFun)\n",
    "# model.print(\"number of iterations: \" , iterations)\n",
    "\n",
    "# model.print(\"Method desciption : learning rates: Exponential decay  with initial value: \", starter_learning_rate)\n",
    "\n",
    "\n",
    "# model.print(\"File directory: \" , model.dirname)\n",
    "\n",
    "# #save_data_to_matlab(os.path.join(model.dirname,'slipwall_02_Z0slice_2D_training.mat') ,coll , INLET , OUTLET , WALL ,INITIAL )\n",
    "\n",
    "# plot_dataset( model.dirname , wall , inlet , outlet , collo , initial , dist)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Adam training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0| It: 0| Loss: 8.228e-01| physLoss: 1.049e-01 | lINITIAL: 3.266e-01 | wLoss: 1.807e-01 | inLoss: 1.543e-01 | outLoss: 5.616e-02 |Time: 0.00s | rTime: 3.893e-04h | LR: 5.000e-03\n",
      "Epoch: 0| It: 0| Loss: 8.228e-01| physLoss: 1.049e-01 | lINITIAL: 3.266e-01 | wLoss: 1.807e-01 | inLoss: 1.543e-01 | outLoss: 5.616e-02 |Time: 0.00s | rTime: 3.893e-04h | LR: 5.000e-03\n",
      "Epoch: 10| It: 0| Loss: 1.871e-01| physLoss: 3.630e-02 | lINITIAL: 9.229e-02 | wLoss: 4.965e-02 | inLoss: 6.766e-03 | outLoss: 2.129e-03 |Time: 0.00s | rTime: 1.569e-03h | LR: 5.000e-03\n",
      "Epoch: 10| It: 0| Loss: 1.871e-01| physLoss: 3.630e-02 | lINITIAL: 9.229e-02 | wLoss: 4.965e-02 | inLoss: 6.766e-03 | outLoss: 2.129e-03 |Time: 0.00s | rTime: 1.569e-03h | LR: 5.000e-03\n",
      "Epoch: 20| It: 0| Loss: 7.611e-02| physLoss: 5.664e-03 | lINITIAL: 4.071e-02 | wLoss: 2.018e-02 | inLoss: 8.673e-03 | outLoss: 8.900e-04 |Time: 0.00s | rTime: 2.655e-03h | LR: 5.000e-03\n",
      "Epoch: 20| It: 0| Loss: 7.611e-02| physLoss: 5.664e-03 | lINITIAL: 4.071e-02 | wLoss: 2.018e-02 | inLoss: 8.673e-03 | outLoss: 8.900e-04 |Time: 0.00s | rTime: 2.655e-03h | LR: 5.000e-03\n",
      "Epoch: 30| It: 0| Loss: 6.867e-02| physLoss: 6.538e-03 | lINITIAL: 3.345e-02 | wLoss: 1.633e-02 | inLoss: 7.317e-03 | outLoss: 5.030e-03 |Time: 0.00s | rTime: 3.656e-03h | LR: 5.000e-03\n",
      "Epoch: 30| It: 0| Loss: 6.867e-02| physLoss: 6.538e-03 | lINITIAL: 3.345e-02 | wLoss: 1.633e-02 | inLoss: 7.317e-03 | outLoss: 5.030e-03 |Time: 0.00s | rTime: 3.656e-03h | LR: 5.000e-03\n",
      "Epoch: 40| It: 0| Loss: 8.218e-02| physLoss: 2.680e-03 | lINITIAL: 4.931e-02 | wLoss: 8.334e-03 | inLoss: 6.718e-03 | outLoss: 1.514e-02 |Time: 0.00s | rTime: 4.637e-03h | LR: 5.000e-03\n",
      "Epoch: 40| It: 0| Loss: 8.218e-02| physLoss: 2.680e-03 | lINITIAL: 4.931e-02 | wLoss: 8.334e-03 | inLoss: 6.718e-03 | outLoss: 1.514e-02 |Time: 0.00s | rTime: 4.637e-03h | LR: 5.000e-03\n",
      "Epoch: 50| It: 0| Loss: 4.750e-02| physLoss: 1.903e-03 | lINITIAL: 2.353e-02 | wLoss: 1.837e-02 | inLoss: 2.607e-03 | outLoss: 1.090e-03 |Time: 0.00s | rTime: 5.650e-03h | LR: 5.000e-03\n",
      "Epoch: 50| It: 0| Loss: 4.750e-02| physLoss: 1.903e-03 | lINITIAL: 2.353e-02 | wLoss: 1.837e-02 | inLoss: 2.607e-03 | outLoss: 1.090e-03 |Time: 0.00s | rTime: 5.650e-03h | LR: 5.000e-03\n",
      "Epoch: 60| It: 0| Loss: 4.098e-02| physLoss: 6.059e-03 | lINITIAL: 1.564e-03 | wLoss: 1.175e-02 | inLoss: 1.857e-02 | outLoss: 3.042e-03 |Time: 0.00s | rTime: 6.632e-03h | LR: 5.000e-03\n",
      "Epoch: 60| It: 0| Loss: 4.098e-02| physLoss: 6.059e-03 | lINITIAL: 1.564e-03 | wLoss: 1.175e-02 | inLoss: 1.857e-02 | outLoss: 3.042e-03 |Time: 0.00s | rTime: 6.632e-03h | LR: 5.000e-03\n",
      "Epoch: 70| It: 0| Loss: 4.943e-02| physLoss: 6.489e-03 | lINITIAL: 1.168e-02 | wLoss: 1.803e-02 | inLoss: 6.689e-03 | outLoss: 6.542e-03 |Time: 0.00s | rTime: 7.624e-03h | LR: 5.000e-03\n",
      "Epoch: 70| It: 0| Loss: 4.943e-02| physLoss: 6.489e-03 | lINITIAL: 1.168e-02 | wLoss: 1.803e-02 | inLoss: 6.689e-03 | outLoss: 6.542e-03 |Time: 0.00s | rTime: 7.624e-03h | LR: 5.000e-03\n",
      "Epoch: 80| It: 0| Loss: 4.418e-02| physLoss: 7.913e-03 | lINITIAL: 9.662e-03 | wLoss: 1.491e-02 | inLoss: 4.942e-03 | outLoss: 6.756e-03 |Time: 0.00s | rTime: 8.830e-03h | LR: 5.000e-03\n",
      "Epoch: 80| It: 0| Loss: 4.418e-02| physLoss: 7.913e-03 | lINITIAL: 9.662e-03 | wLoss: 1.491e-02 | inLoss: 4.942e-03 | outLoss: 6.756e-03 |Time: 0.00s | rTime: 8.830e-03h | LR: 5.000e-03\n",
      "Epoch: 90| It: 0| Loss: 4.042e-02| physLoss: 9.379e-03 | lINITIAL: 6.882e-03 | wLoss: 7.977e-03 | inLoss: 1.146e-02 | outLoss: 4.724e-03 |Time: 0.00s | rTime: 9.790e-03h | LR: 5.000e-03\n",
      "Epoch: 90| It: 0| Loss: 4.042e-02| physLoss: 9.379e-03 | lINITIAL: 6.882e-03 | wLoss: 7.977e-03 | inLoss: 1.146e-02 | outLoss: 4.724e-03 |Time: 0.00s | rTime: 9.790e-03h | LR: 5.000e-03\n",
      "Epoch: 100| It: 0| Loss: 2.617e-02| physLoss: 3.904e-03 | lINITIAL: 4.989e-03 | wLoss: 1.344e-02 | inLoss: 3.430e-03 | outLoss: 4.108e-04 |Time: 0.00s | rTime: 1.073e-02h | LR: 5.000e-03\n",
      "Epoch: 100| It: 0| Loss: 2.617e-02| physLoss: 3.904e-03 | lINITIAL: 4.989e-03 | wLoss: 1.344e-02 | inLoss: 3.430e-03 | outLoss: 4.108e-04 |Time: 0.00s | rTime: 1.073e-02h | LR: 5.000e-03\n",
      "Epoch: 110| It: 0| Loss: 2.673e-02| physLoss: 1.881e-03 | lINITIAL: 4.813e-03 | wLoss: 1.445e-02 | inLoss: 3.842e-03 | outLoss: 1.741e-03 |Time: 0.00s | rTime: 1.176e-02h | LR: 5.000e-03\n",
      "Epoch: 110| It: 0| Loss: 2.673e-02| physLoss: 1.881e-03 | lINITIAL: 4.813e-03 | wLoss: 1.445e-02 | inLoss: 3.842e-03 | outLoss: 1.741e-03 |Time: 0.00s | rTime: 1.176e-02h | LR: 5.000e-03\n",
      "Epoch: 120| It: 0| Loss: 3.372e-02| physLoss: 3.771e-03 | lINITIAL: 7.986e-03 | wLoss: 9.593e-03 | inLoss: 1.088e-02 | outLoss: 1.484e-03 |Time: 0.00s | rTime: 1.269e-02h | LR: 5.000e-03\n",
      "Epoch: 120| It: 0| Loss: 3.372e-02| physLoss: 3.771e-03 | lINITIAL: 7.986e-03 | wLoss: 9.593e-03 | inLoss: 1.088e-02 | outLoss: 1.484e-03 |Time: 0.00s | rTime: 1.269e-02h | LR: 5.000e-03\n",
      "Epoch: 130| It: 0| Loss: 3.431e-02| physLoss: 4.382e-03 | lINITIAL: 8.608e-03 | wLoss: 1.314e-02 | inLoss: 5.943e-03 | outLoss: 2.229e-03 |Time: 0.00s | rTime: 1.357e-02h | LR: 5.000e-03\n",
      "Epoch: 130| It: 0| Loss: 3.431e-02| physLoss: 4.382e-03 | lINITIAL: 8.608e-03 | wLoss: 1.314e-02 | inLoss: 5.943e-03 | outLoss: 2.229e-03 |Time: 0.00s | rTime: 1.357e-02h | LR: 5.000e-03\n",
      "Epoch: 140| It: 0| Loss: 2.979e-02| physLoss: 2.496e-03 | lINITIAL: 5.207e-03 | wLoss: 1.421e-02 | inLoss: 4.650e-03 | outLoss: 3.228e-03 |Time: 0.00s | rTime: 1.464e-02h | LR: 5.000e-03\n",
      "Epoch: 140| It: 0| Loss: 2.979e-02| physLoss: 2.496e-03 | lINITIAL: 5.207e-03 | wLoss: 1.421e-02 | inLoss: 4.650e-03 | outLoss: 3.228e-03 |Time: 0.00s | rTime: 1.464e-02h | LR: 5.000e-03\n",
      "Epoch: 150| It: 0| Loss: 2.813e-02| physLoss: 1.651e-03 | lINITIAL: 5.961e-03 | wLoss: 9.449e-03 | inLoss: 7.230e-03 | outLoss: 3.843e-03 |Time: 0.00s | rTime: 1.558e-02h | LR: 5.000e-03\n",
      "Epoch: 150| It: 0| Loss: 2.813e-02| physLoss: 1.651e-03 | lINITIAL: 5.961e-03 | wLoss: 9.449e-03 | inLoss: 7.230e-03 | outLoss: 3.843e-03 |Time: 0.00s | rTime: 1.558e-02h | LR: 5.000e-03\n",
      "Epoch: 160| It: 0| Loss: 2.666e-02| physLoss: 2.852e-03 | lINITIAL: 5.198e-03 | wLoss: 1.258e-02 | inLoss: 3.993e-03 | outLoss: 2.042e-03 |Time: 0.00s | rTime: 1.649e-02h | LR: 5.000e-03\n",
      "Epoch: 160| It: 0| Loss: 2.666e-02| physLoss: 2.852e-03 | lINITIAL: 5.198e-03 | wLoss: 1.258e-02 | inLoss: 3.993e-03 | outLoss: 2.042e-03 |Time: 0.00s | rTime: 1.649e-02h | LR: 5.000e-03\n",
      "Epoch: 170| It: 0| Loss: 2.690e-02| physLoss: 5.061e-03 | lINITIAL: 5.306e-03 | wLoss: 1.026e-02 | inLoss: 5.011e-03 | outLoss: 1.266e-03 |Time: 0.00s | rTime: 1.739e-02h | LR: 5.000e-03\n",
      "Epoch: 170| It: 0| Loss: 2.690e-02| physLoss: 5.061e-03 | lINITIAL: 5.306e-03 | wLoss: 1.026e-02 | inLoss: 5.011e-03 | outLoss: 1.266e-03 |Time: 0.00s | rTime: 1.739e-02h | LR: 5.000e-03\n",
      "Epoch: 180| It: 0| Loss: 2.898e-02| physLoss: 5.315e-03 | lINITIAL: 7.113e-03 | wLoss: 1.022e-02 | inLoss: 5.629e-03 | outLoss: 7.058e-04 |Time: 0.00s | rTime: 1.843e-02h | LR: 5.000e-03\n",
      "Epoch: 180| It: 0| Loss: 2.898e-02| physLoss: 5.315e-03 | lINITIAL: 7.113e-03 | wLoss: 1.022e-02 | inLoss: 5.629e-03 | outLoss: 7.058e-04 |Time: 0.00s | rTime: 1.843e-02h | LR: 5.000e-03\n",
      "Epoch: 190| It: 0| Loss: 2.604e-02| physLoss: 3.365e-03 | lINITIAL: 5.426e-03 | wLoss: 1.250e-02 | inLoss: 4.433e-03 | outLoss: 3.187e-04 |Time: 0.00s | rTime: 1.938e-02h | LR: 5.000e-03\n",
      "Epoch: 190| It: 0| Loss: 2.604e-02| physLoss: 3.365e-03 | lINITIAL: 5.426e-03 | wLoss: 1.250e-02 | inLoss: 4.433e-03 | outLoss: 3.187e-04 |Time: 0.00s | rTime: 1.938e-02h | LR: 5.000e-03\n",
      "Epoch: 200| It: 0| Loss: 2.459e-02| physLoss: 2.276e-03 | lINITIAL: 4.085e-03 | wLoss: 1.076e-02 | inLoss: 5.539e-03 | outLoss: 1.935e-03 |Time: 0.00s | rTime: 2.034e-02h | LR: 5.000e-03\n",
      "Epoch: 200| It: 0| Loss: 2.459e-02| physLoss: 2.276e-03 | lINITIAL: 4.085e-03 | wLoss: 1.076e-02 | inLoss: 5.539e-03 | outLoss: 1.935e-03 |Time: 0.00s | rTime: 2.034e-02h | LR: 5.000e-03\n",
      "Epoch: 210| It: 0| Loss: 2.476e-02| physLoss: 2.568e-03 | lINITIAL: 4.415e-03 | wLoss: 9.080e-03 | inLoss: 8.493e-03 | outLoss: 2.014e-04 |Time: 0.00s | rTime: 2.130e-02h | LR: 5.000e-03\n",
      "Epoch: 210| It: 0| Loss: 2.476e-02| physLoss: 2.568e-03 | lINITIAL: 4.415e-03 | wLoss: 9.080e-03 | inLoss: 8.493e-03 | outLoss: 2.014e-04 |Time: 0.00s | rTime: 2.130e-02h | LR: 5.000e-03\n",
      "Epoch: 220| It: 0| Loss: 2.409e-02| physLoss: 2.735e-03 | lINITIAL: 3.811e-03 | wLoss: 1.314e-02 | inLoss: 4.337e-03 | outLoss: 6.811e-05 |Time: 0.00s | rTime: 2.227e-02h | LR: 5.000e-03\n",
      "Epoch: 220| It: 0| Loss: 2.409e-02| physLoss: 2.735e-03 | lINITIAL: 3.811e-03 | wLoss: 1.314e-02 | inLoss: 4.337e-03 | outLoss: 6.811e-05 |Time: 0.00s | rTime: 2.227e-02h | LR: 5.000e-03\n",
      "Epoch: 230| It: 0| Loss: 2.186e-02| physLoss: 2.568e-03 | lINITIAL: 3.162e-03 | wLoss: 1.156e-02 | inLoss: 4.461e-03 | outLoss: 1.066e-04 |Time: 0.00s | rTime: 2.326e-02h | LR: 5.000e-03\n",
      "Epoch: 230| It: 0| Loss: 2.186e-02| physLoss: 2.568e-03 | lINITIAL: 3.162e-03 | wLoss: 1.156e-02 | inLoss: 4.461e-03 | outLoss: 1.066e-04 |Time: 0.00s | rTime: 2.326e-02h | LR: 5.000e-03\n",
      "Epoch: 240| It: 0| Loss: 2.195e-02| physLoss: 2.245e-03 | lINITIAL: 4.118e-03 | wLoss: 8.398e-03 | inLoss: 7.028e-03 | outLoss: 1.603e-04 |Time: 0.00s | rTime: 2.423e-02h | LR: 5.000e-03\n",
      "Epoch: 240| It: 0| Loss: 2.195e-02| physLoss: 2.245e-03 | lINITIAL: 4.118e-03 | wLoss: 8.398e-03 | inLoss: 7.028e-03 | outLoss: 1.603e-04 |Time: 0.00s | rTime: 2.423e-02h | LR: 5.000e-03\n",
      "Epoch: 250| It: 0| Loss: 2.069e-02| physLoss: 2.028e-03 | lINITIAL: 3.935e-03 | wLoss: 1.185e-02 | inLoss: 2.738e-03 | outLoss: 1.331e-04 |Time: 0.00s | rTime: 2.530e-02h | LR: 5.000e-03\n",
      "Epoch: 250| It: 0| Loss: 2.069e-02| physLoss: 2.028e-03 | lINITIAL: 3.935e-03 | wLoss: 1.185e-02 | inLoss: 2.738e-03 | outLoss: 1.331e-04 |Time: 0.00s | rTime: 2.530e-02h | LR: 5.000e-03\n",
      "Epoch: 260| It: 0| Loss: 2.120e-02| physLoss: 2.850e-03 | lINITIAL: 3.894e-03 | wLoss: 1.058e-02 | inLoss: 2.886e-03 | outLoss: 9.906e-04 |Time: 0.00s | rTime: 2.633e-02h | LR: 5.000e-03\n",
      "Epoch: 260| It: 0| Loss: 2.120e-02| physLoss: 2.850e-03 | lINITIAL: 3.894e-03 | wLoss: 1.058e-02 | inLoss: 2.886e-03 | outLoss: 9.906e-04 |Time: 0.00s | rTime: 2.633e-02h | LR: 5.000e-03\n",
      "Epoch: 270| It: 0| Loss: 2.131e-02| physLoss: 3.390e-03 | lINITIAL: 4.473e-03 | wLoss: 9.480e-03 | inLoss: 3.654e-03 | outLoss: 3.147e-04 |Time: 0.00s | rTime: 2.751e-02h | LR: 5.000e-03\n",
      "Epoch: 270| It: 0| Loss: 2.131e-02| physLoss: 3.390e-03 | lINITIAL: 4.473e-03 | wLoss: 9.480e-03 | inLoss: 3.654e-03 | outLoss: 3.147e-04 |Time: 0.00s | rTime: 2.751e-02h | LR: 5.000e-03\n",
      "Epoch: 280| It: 0| Loss: 2.118e-02| physLoss: 3.100e-03 | lINITIAL: 4.353e-03 | wLoss: 9.479e-03 | inLoss: 4.055e-03 | outLoss: 1.902e-04 |Time: 0.00s | rTime: 2.842e-02h | LR: 5.000e-03\n",
      "Epoch: 280| It: 0| Loss: 2.118e-02| physLoss: 3.100e-03 | lINITIAL: 4.353e-03 | wLoss: 9.479e-03 | inLoss: 4.055e-03 | outLoss: 1.902e-04 |Time: 0.00s | rTime: 2.842e-02h | LR: 5.000e-03\n",
      "Epoch: 290| It: 0| Loss: 1.983e-02| physLoss: 2.847e-03 | lINITIAL: 2.568e-03 | wLoss: 1.042e-02 | inLoss: 3.599e-03 | outLoss: 3.943e-04 |Time: 0.00s | rTime: 2.934e-02h | LR: 5.000e-03\n",
      "Epoch: 290| It: 0| Loss: 1.983e-02| physLoss: 2.847e-03 | lINITIAL: 2.568e-03 | wLoss: 1.042e-02 | inLoss: 3.599e-03 | outLoss: 3.943e-04 |Time: 0.00s | rTime: 2.934e-02h | LR: 5.000e-03\n",
      "Epoch: 300| It: 0| Loss: 1.956e-02| physLoss: 2.936e-03 | lINITIAL: 2.205e-03 | wLoss: 9.375e-03 | inLoss: 4.696e-03 | outLoss: 3.440e-04 |Time: 0.00s | rTime: 3.035e-02h | LR: 5.000e-03\n",
      "Epoch: 300| It: 0| Loss: 1.956e-02| physLoss: 2.936e-03 | lINITIAL: 2.205e-03 | wLoss: 9.375e-03 | inLoss: 4.696e-03 | outLoss: 3.440e-04 |Time: 0.00s | rTime: 3.035e-02h | LR: 5.000e-03\n",
      "Epoch: 310| It: 0| Loss: 1.975e-02| physLoss: 2.872e-03 | lINITIAL: 1.796e-03 | wLoss: 1.010e-02 | inLoss: 4.183e-03 | outLoss: 7.956e-04 |Time: 0.00s | rTime: 3.137e-02h | LR: 5.000e-03\n",
      "Epoch: 310| It: 0| Loss: 1.975e-02| physLoss: 2.872e-03 | lINITIAL: 1.796e-03 | wLoss: 1.010e-02 | inLoss: 4.183e-03 | outLoss: 7.956e-04 |Time: 0.00s | rTime: 3.137e-02h | LR: 5.000e-03\n",
      "Epoch: 320| It: 0| Loss: 1.928e-02| physLoss: 2.840e-03 | lINITIAL: 2.517e-03 | wLoss: 8.459e-03 | inLoss: 5.172e-03 | outLoss: 2.892e-04 |Time: 0.00s | rTime: 3.237e-02h | LR: 5.000e-03\n",
      "Epoch: 320| It: 0| Loss: 1.928e-02| physLoss: 2.840e-03 | lINITIAL: 2.517e-03 | wLoss: 8.459e-03 | inLoss: 5.172e-03 | outLoss: 2.892e-04 |Time: 0.00s | rTime: 3.237e-02h | LR: 5.000e-03\n",
      "Epoch: 330| It: 0| Loss: 1.950e-02| physLoss: 2.698e-03 | lINITIAL: 2.860e-03 | wLoss: 1.059e-02 | inLoss: 3.028e-03 | outLoss: 3.237e-04 |Time: 0.00s | rTime: 3.337e-02h | LR: 5.000e-03\n",
      "Epoch: 330| It: 0| Loss: 1.950e-02| physLoss: 2.698e-03 | lINITIAL: 2.860e-03 | wLoss: 1.059e-02 | inLoss: 3.028e-03 | outLoss: 3.237e-04 |Time: 0.00s | rTime: 3.337e-02h | LR: 5.000e-03\n",
      "Epoch: 340| It: 0| Loss: 1.910e-02| physLoss: 2.941e-03 | lINITIAL: 2.774e-03 | wLoss: 9.477e-03 | inLoss: 3.489e-03 | outLoss: 4.184e-04 |Time: 0.00s | rTime: 3.435e-02h | LR: 5.000e-03\n",
      "Epoch: 340| It: 0| Loss: 1.910e-02| physLoss: 2.941e-03 | lINITIAL: 2.774e-03 | wLoss: 9.477e-03 | inLoss: 3.489e-03 | outLoss: 4.184e-04 |Time: 0.00s | rTime: 3.435e-02h | LR: 5.000e-03\n",
      "Epoch: 350| It: 0| Loss: 1.873e-02| physLoss: 3.245e-03 | lINITIAL: 2.174e-03 | wLoss: 8.423e-03 | inLoss: 4.688e-03 | outLoss: 1.980e-04 |Time: 0.00s | rTime: 3.543e-02h | LR: 5.000e-03\n",
      "Epoch: 350| It: 0| Loss: 1.873e-02| physLoss: 3.245e-03 | lINITIAL: 2.174e-03 | wLoss: 8.423e-03 | inLoss: 4.688e-03 | outLoss: 1.980e-04 |Time: 0.00s | rTime: 3.543e-02h | LR: 5.000e-03\n",
      "Epoch: 360| It: 0| Loss: 1.796e-02| physLoss: 3.178e-03 | lINITIAL: 1.572e-03 | wLoss: 9.934e-03 | inLoss: 3.225e-03 | outLoss: 5.429e-05 |Time: 0.00s | rTime: 3.645e-02h | LR: 5.000e-03\n",
      "Epoch: 360| It: 0| Loss: 1.796e-02| physLoss: 3.178e-03 | lINITIAL: 1.572e-03 | wLoss: 9.934e-03 | inLoss: 3.225e-03 | outLoss: 5.429e-05 |Time: 0.00s | rTime: 3.645e-02h | LR: 5.000e-03\n",
      "Epoch: 370| It: 0| Loss: 1.772e-02| physLoss: 3.104e-03 | lINITIAL: 1.549e-03 | wLoss: 9.684e-03 | inLoss: 3.278e-03 | outLoss: 1.036e-04 |Time: 0.00s | rTime: 3.739e-02h | LR: 5.000e-03\n",
      "Epoch: 370| It: 0| Loss: 1.772e-02| physLoss: 3.104e-03 | lINITIAL: 1.549e-03 | wLoss: 9.684e-03 | inLoss: 3.278e-03 | outLoss: 1.036e-04 |Time: 0.00s | rTime: 3.739e-02h | LR: 5.000e-03\n",
      "Epoch: 380| It: 0| Loss: 1.786e-02| physLoss: 2.882e-03 | lINITIAL: 2.649e-03 | wLoss: 8.092e-03 | inLoss: 4.192e-03 | outLoss: 4.107e-05 |Time: 0.00s | rTime: 3.831e-02h | LR: 5.000e-03\n",
      "Epoch: 380| It: 0| Loss: 1.786e-02| physLoss: 2.882e-03 | lINITIAL: 2.649e-03 | wLoss: 8.092e-03 | inLoss: 4.192e-03 | outLoss: 4.107e-05 |Time: 0.00s | rTime: 3.831e-02h | LR: 5.000e-03\n",
      "Epoch: 390| It: 0| Loss: 1.763e-02| physLoss: 2.450e-03 | lINITIAL: 2.674e-03 | wLoss: 1.017e-02 | inLoss: 2.289e-03 | outLoss: 4.332e-05 |Time: 0.00s | rTime: 3.941e-02h | LR: 5.000e-03\n",
      "Epoch: 390| It: 0| Loss: 1.763e-02| physLoss: 2.450e-03 | lINITIAL: 2.674e-03 | wLoss: 1.017e-02 | inLoss: 2.289e-03 | outLoss: 4.332e-05 |Time: 0.00s | rTime: 3.941e-02h | LR: 5.000e-03\n",
      "Epoch: 400| It: 0| Loss: 1.763e-02| physLoss: 2.203e-03 | lINITIAL: 3.642e-03 | wLoss: 8.899e-03 | inLoss: 2.522e-03 | outLoss: 3.628e-04 |Time: 0.00s | rTime: 4.042e-02h | LR: 5.000e-03\n",
      "Epoch: 400| It: 0| Loss: 1.763e-02| physLoss: 2.203e-03 | lINITIAL: 3.642e-03 | wLoss: 8.899e-03 | inLoss: 2.522e-03 | outLoss: 3.628e-04 |Time: 0.00s | rTime: 4.042e-02h | LR: 5.000e-03\n",
      "Epoch: 410| It: 0| Loss: 1.717e-02| physLoss: 2.076e-03 | lINITIAL: 3.509e-03 | wLoss: 9.485e-03 | inLoss: 2.046e-03 | outLoss: 5.165e-05 |Time: 0.00s | rTime: 4.142e-02h | LR: 5.000e-03\n",
      "Epoch: 410| It: 0| Loss: 1.717e-02| physLoss: 2.076e-03 | lINITIAL: 3.509e-03 | wLoss: 9.485e-03 | inLoss: 2.046e-03 | outLoss: 5.165e-05 |Time: 0.00s | rTime: 4.142e-02h | LR: 5.000e-03\n",
      "Epoch: 420| It: 0| Loss: 1.688e-02| physLoss: 2.309e-03 | lINITIAL: 3.129e-03 | wLoss: 8.623e-03 | inLoss: 2.749e-03 | outLoss: 6.760e-05 |Time: 0.00s | rTime: 4.235e-02h | LR: 5.000e-03\n",
      "Epoch: 420| It: 0| Loss: 1.688e-02| physLoss: 2.309e-03 | lINITIAL: 3.129e-03 | wLoss: 8.623e-03 | inLoss: 2.749e-03 | outLoss: 6.760e-05 |Time: 0.00s | rTime: 4.235e-02h | LR: 5.000e-03\n",
      "Epoch: 430| It: 0| Loss: 1.685e-02| physLoss: 2.701e-03 | lINITIAL: 2.011e-03 | wLoss: 9.217e-03 | inLoss: 2.704e-03 | outLoss: 2.208e-04 |Time: 0.00s | rTime: 4.338e-02h | LR: 5.000e-03\n",
      "Epoch: 430| It: 0| Loss: 1.685e-02| physLoss: 2.701e-03 | lINITIAL: 2.011e-03 | wLoss: 9.217e-03 | inLoss: 2.704e-03 | outLoss: 2.208e-04 |Time: 0.00s | rTime: 4.338e-02h | LR: 5.000e-03\n",
      "Epoch: 440| It: 0| Loss: 1.679e-02| physLoss: 3.015e-03 | lINITIAL: 1.695e-03 | wLoss: 9.287e-03 | inLoss: 2.743e-03 | outLoss: 4.833e-05 |Time: 0.00s | rTime: 4.431e-02h | LR: 5.000e-03\n",
      "Epoch: 440| It: 0| Loss: 1.679e-02| physLoss: 3.015e-03 | lINITIAL: 1.695e-03 | wLoss: 9.287e-03 | inLoss: 2.743e-03 | outLoss: 4.833e-05 |Time: 0.00s | rTime: 4.431e-02h | LR: 5.000e-03\n",
      "Epoch: 450| It: 0| Loss: 1.666e-02| physLoss: 3.173e-03 | lINITIAL: 1.877e-03 | wLoss: 8.333e-03 | inLoss: 3.208e-03 | outLoss: 7.331e-05 |Time: 0.00s | rTime: 4.523e-02h | LR: 5.000e-03\n",
      "Epoch: 450| It: 0| Loss: 1.666e-02| physLoss: 3.173e-03 | lINITIAL: 1.877e-03 | wLoss: 8.333e-03 | inLoss: 3.208e-03 | outLoss: 7.331e-05 |Time: 0.00s | rTime: 4.523e-02h | LR: 5.000e-03\n",
      "Epoch: 460| It: 0| Loss: 1.640e-02| physLoss: 3.094e-03 | lINITIAL: 2.073e-03 | wLoss: 9.015e-03 | inLoss: 2.184e-03 | outLoss: 3.348e-05 |Time: 0.00s | rTime: 4.616e-02h | LR: 5.000e-03\n",
      "Epoch: 460| It: 0| Loss: 1.640e-02| physLoss: 3.094e-03 | lINITIAL: 2.073e-03 | wLoss: 9.015e-03 | inLoss: 2.184e-03 | outLoss: 3.348e-05 |Time: 0.00s | rTime: 4.616e-02h | LR: 5.000e-03\n",
      "Epoch: 470| It: 0| Loss: 1.638e-02| physLoss: 2.969e-03 | lINITIAL: 2.589e-03 | wLoss: 8.872e-03 | inLoss: 1.925e-03 | outLoss: 2.260e-05 |Time: 0.00s | rTime: 4.711e-02h | LR: 5.000e-03\n",
      "Epoch: 470| It: 0| Loss: 1.638e-02| physLoss: 2.969e-03 | lINITIAL: 2.589e-03 | wLoss: 8.872e-03 | inLoss: 1.925e-03 | outLoss: 2.260e-05 |Time: 0.00s | rTime: 4.711e-02h | LR: 5.000e-03\n",
      "Epoch: 480| It: 0| Loss: 1.623e-02| physLoss: 2.929e-03 | lINITIAL: 2.634e-03 | wLoss: 8.298e-03 | inLoss: 2.229e-03 | outLoss: 1.382e-04 |Time: 0.00s | rTime: 4.805e-02h | LR: 5.000e-03\n",
      "Epoch: 480| It: 0| Loss: 1.623e-02| physLoss: 2.929e-03 | lINITIAL: 2.634e-03 | wLoss: 8.298e-03 | inLoss: 2.229e-03 | outLoss: 1.382e-04 |Time: 0.00s | rTime: 4.805e-02h | LR: 5.000e-03\n",
      "Epoch: 490| It: 0| Loss: 1.603e-02| physLoss: 3.131e-03 | lINITIAL: 2.124e-03 | wLoss: 8.659e-03 | inLoss: 2.057e-03 | outLoss: 5.926e-05 |Time: 0.00s | rTime: 4.898e-02h | LR: 5.000e-03\n",
      "Epoch: 490| It: 0| Loss: 1.603e-02| physLoss: 3.131e-03 | lINITIAL: 2.124e-03 | wLoss: 8.659e-03 | inLoss: 2.057e-03 | outLoss: 5.926e-05 |Time: 0.00s | rTime: 4.898e-02h | LR: 5.000e-03\n",
      "Epoch: 500| It: 0| Loss: 1.598e-02| physLoss: 3.334e-03 | lINITIAL: 1.849e-03 | wLoss: 8.619e-03 | inLoss: 2.078e-03 | outLoss: 9.851e-05 |Time: 0.00s | rTime: 4.995e-02h | LR: 5.000e-03\n",
      "Epoch: 500| It: 0| Loss: 1.598e-02| physLoss: 3.334e-03 | lINITIAL: 1.849e-03 | wLoss: 8.619e-03 | inLoss: 2.078e-03 | outLoss: 9.851e-05 |Time: 0.00s | rTime: 4.995e-02h | LR: 5.000e-03\n",
      "Epoch: 510| It: 0| Loss: 1.591e-02| physLoss: 3.447e-03 | lINITIAL: 1.883e-03 | wLoss: 8.669e-03 | inLoss: 1.865e-03 | outLoss: 4.288e-05 |Time: 0.00s | rTime: 5.086e-02h | LR: 5.000e-03\n",
      "Epoch: 510| It: 0| Loss: 1.591e-02| physLoss: 3.447e-03 | lINITIAL: 1.883e-03 | wLoss: 8.669e-03 | inLoss: 1.865e-03 | outLoss: 4.288e-05 |Time: 0.00s | rTime: 5.086e-02h | LR: 5.000e-03\n",
      "Epoch: 520| It: 0| Loss: 1.597e-02| physLoss: 3.586e-03 | lINITIAL: 2.001e-03 | wLoss: 8.332e-03 | inLoss: 2.007e-03 | outLoss: 4.786e-05 |Time: 0.00s | rTime: 5.179e-02h | LR: 5.000e-03\n",
      "Epoch: 520| It: 0| Loss: 1.597e-02| physLoss: 3.586e-03 | lINITIAL: 2.001e-03 | wLoss: 8.332e-03 | inLoss: 2.007e-03 | outLoss: 4.786e-05 |Time: 0.00s | rTime: 5.179e-02h | LR: 5.000e-03\n",
      "Epoch: 530| It: 0| Loss: 1.585e-02| physLoss: 3.473e-03 | lINITIAL: 1.843e-03 | wLoss: 8.951e-03 | inLoss: 1.496e-03 | outLoss: 8.838e-05 |Time: 0.00s | rTime: 5.280e-02h | LR: 5.000e-03\n",
      "Epoch: 530| It: 0| Loss: 1.585e-02| physLoss: 3.473e-03 | lINITIAL: 1.843e-03 | wLoss: 8.951e-03 | inLoss: 1.496e-03 | outLoss: 8.838e-05 |Time: 0.00s | rTime: 5.280e-02h | LR: 5.000e-03\n",
      "Epoch: 540| It: 0| Loss: 1.571e-02| physLoss: 3.362e-03 | lINITIAL: 2.024e-03 | wLoss: 8.420e-03 | inLoss: 1.871e-03 | outLoss: 3.338e-05 |Time: 0.00s | rTime: 5.371e-02h | LR: 5.000e-03\n",
      "Epoch: 540| It: 0| Loss: 1.571e-02| physLoss: 3.362e-03 | lINITIAL: 2.024e-03 | wLoss: 8.420e-03 | inLoss: 1.871e-03 | outLoss: 3.338e-05 |Time: 0.00s | rTime: 5.371e-02h | LR: 5.000e-03\n",
      "Epoch: 550| It: 0| Loss: 1.576e-02| physLoss: 3.304e-03 | lINITIAL: 2.129e-03 | wLoss: 8.464e-03 | inLoss: 1.838e-03 | outLoss: 2.599e-05 |Time: 0.00s | rTime: 5.470e-02h | LR: 5.000e-03\n",
      "Epoch: 550| It: 0| Loss: 1.576e-02| physLoss: 3.304e-03 | lINITIAL: 2.129e-03 | wLoss: 8.464e-03 | inLoss: 1.838e-03 | outLoss: 2.599e-05 |Time: 0.00s | rTime: 5.470e-02h | LR: 5.000e-03\n",
      "Epoch: 560| It: 0| Loss: 1.565e-02| physLoss: 3.239e-03 | lINITIAL: 2.129e-03 | wLoss: 8.623e-03 | inLoss: 1.643e-03 | outLoss: 1.401e-05 |Time: 0.00s | rTime: 5.565e-02h | LR: 5.000e-03\n",
      "Epoch: 560| It: 0| Loss: 1.565e-02| physLoss: 3.239e-03 | lINITIAL: 2.129e-03 | wLoss: 8.623e-03 | inLoss: 1.643e-03 | outLoss: 1.401e-05 |Time: 0.00s | rTime: 5.565e-02h | LR: 5.000e-03\n",
      "Epoch: 570| It: 0| Loss: 1.558e-02| physLoss: 3.337e-03 | lINITIAL: 1.994e-03 | wLoss: 8.691e-03 | inLoss: 1.560e-03 | outLoss: 1.534e-06 |Time: 0.00s | rTime: 5.657e-02h | LR: 5.000e-03\n",
      "Epoch: 570| It: 0| Loss: 1.558e-02| physLoss: 3.337e-03 | lINITIAL: 1.994e-03 | wLoss: 8.691e-03 | inLoss: 1.560e-03 | outLoss: 1.534e-06 |Time: 0.00s | rTime: 5.657e-02h | LR: 5.000e-03\n",
      "Epoch: 580| It: 0| Loss: 1.562e-02| physLoss: 3.486e-03 | lINITIAL: 1.770e-03 | wLoss: 8.544e-03 | inLoss: 1.778e-03 | outLoss: 4.368e-05 |Time: 0.00s | rTime: 5.757e-02h | LR: 5.000e-03\n",
      "Epoch: 580| It: 0| Loss: 1.562e-02| physLoss: 3.486e-03 | lINITIAL: 1.770e-03 | wLoss: 8.544e-03 | inLoss: 1.778e-03 | outLoss: 4.368e-05 |Time: 0.00s | rTime: 5.757e-02h | LR: 5.000e-03\n",
      "Epoch: 590| It: 0| Loss: 1.564e-02| physLoss: 3.528e-03 | lINITIAL: 1.559e-03 | wLoss: 8.718e-03 | inLoss: 1.827e-03 | outLoss: 6.099e-06 |Time: 0.00s | rTime: 5.849e-02h | LR: 5.000e-03\n",
      "Epoch: 590| It: 0| Loss: 1.564e-02| physLoss: 3.528e-03 | lINITIAL: 1.559e-03 | wLoss: 8.718e-03 | inLoss: 1.827e-03 | outLoss: 6.099e-06 |Time: 0.00s | rTime: 5.849e-02h | LR: 5.000e-03\n",
      "Epoch: 600| It: 0| Loss: 1.559e-02| physLoss: 3.386e-03 | lINITIAL: 1.635e-03 | wLoss: 8.954e-03 | inLoss: 1.610e-03 | outLoss: 4.366e-06 |Time: 0.00s | rTime: 5.945e-02h | LR: 5.000e-03\n",
      "Epoch: 600| It: 0| Loss: 1.559e-02| physLoss: 3.386e-03 | lINITIAL: 1.635e-03 | wLoss: 8.954e-03 | inLoss: 1.610e-03 | outLoss: 4.366e-06 |Time: 0.00s | rTime: 5.945e-02h | LR: 5.000e-03\n",
      "Epoch: 610| It: 0| Loss: 1.555e-02| physLoss: 3.180e-03 | lINITIAL: 2.159e-03 | wLoss: 8.360e-03 | inLoss: 1.823e-03 | outLoss: 2.530e-05 |Time: 0.00s | rTime: 6.043e-02h | LR: 5.000e-03\n",
      "Epoch: 610| It: 0| Loss: 1.555e-02| physLoss: 3.180e-03 | lINITIAL: 2.159e-03 | wLoss: 8.360e-03 | inLoss: 1.823e-03 | outLoss: 2.530e-05 |Time: 0.00s | rTime: 6.043e-02h | LR: 5.000e-03\n",
      "Epoch: 620| It: 0| Loss: 1.555e-02| physLoss: 3.028e-03 | lINITIAL: 2.170e-03 | wLoss: 9.031e-03 | inLoss: 1.309e-03 | outLoss: 6.986e-06 |Time: 0.00s | rTime: 6.141e-02h | LR: 5.000e-03\n",
      "Epoch: 620| It: 0| Loss: 1.555e-02| physLoss: 3.028e-03 | lINITIAL: 2.170e-03 | wLoss: 9.031e-03 | inLoss: 1.309e-03 | outLoss: 6.986e-06 |Time: 0.00s | rTime: 6.141e-02h | LR: 5.000e-03\n",
      "Epoch: 630| It: 0| Loss: 1.549e-02| physLoss: 3.089e-03 | lINITIAL: 2.134e-03 | wLoss: 8.584e-03 | inLoss: 1.664e-03 | outLoss: 1.618e-05 |Time: 0.00s | rTime: 6.235e-02h | LR: 5.000e-03\n",
      "Epoch: 630| It: 0| Loss: 1.549e-02| physLoss: 3.089e-03 | lINITIAL: 2.134e-03 | wLoss: 8.584e-03 | inLoss: 1.664e-03 | outLoss: 1.618e-05 |Time: 0.00s | rTime: 6.235e-02h | LR: 5.000e-03\n",
      "Epoch: 640| It: 0| Loss: 1.547e-02| physLoss: 3.177e-03 | lINITIAL: 1.803e-03 | wLoss: 8.796e-03 | inLoss: 1.650e-03 | outLoss: 4.303e-05 |Time: 0.00s | rTime: 6.327e-02h | LR: 5.000e-03\n",
      "Epoch: 640| It: 0| Loss: 1.547e-02| physLoss: 3.177e-03 | lINITIAL: 1.803e-03 | wLoss: 8.796e-03 | inLoss: 1.650e-03 | outLoss: 4.303e-05 |Time: 0.00s | rTime: 6.327e-02h | LR: 5.000e-03\n",
      "Epoch: 650| It: 0| Loss: 1.543e-02| physLoss: 3.209e-03 | lINITIAL: 1.751e-03 | wLoss: 8.699e-03 | inLoss: 1.712e-03 | outLoss: 5.509e-05 |Time: 0.00s | rTime: 6.422e-02h | LR: 5.000e-03\n",
      "Epoch: 650| It: 0| Loss: 1.543e-02| physLoss: 3.209e-03 | lINITIAL: 1.751e-03 | wLoss: 8.699e-03 | inLoss: 1.712e-03 | outLoss: 5.509e-05 |Time: 0.00s | rTime: 6.422e-02h | LR: 5.000e-03\n",
      "Epoch: 660| It: 0| Loss: 1.541e-02| physLoss: 3.145e-03 | lINITIAL: 1.841e-03 | wLoss: 8.911e-03 | inLoss: 1.477e-03 | outLoss: 4.027e-05 |Time: 0.00s | rTime: 6.528e-02h | LR: 5.000e-03\n",
      "Epoch: 660| It: 0| Loss: 1.541e-02| physLoss: 3.145e-03 | lINITIAL: 1.841e-03 | wLoss: 8.911e-03 | inLoss: 1.477e-03 | outLoss: 4.027e-05 |Time: 0.00s | rTime: 6.528e-02h | LR: 5.000e-03\n",
      "Epoch: 670| It: 0| Loss: 1.537e-02| physLoss: 3.104e-03 | lINITIAL: 1.998e-03 | wLoss: 8.501e-03 | inLoss: 1.741e-03 | outLoss: 2.636e-05 |Time: 0.00s | rTime: 6.642e-02h | LR: 5.000e-03\n",
      "Epoch: 670| It: 0| Loss: 1.537e-02| physLoss: 3.104e-03 | lINITIAL: 1.998e-03 | wLoss: 8.501e-03 | inLoss: 1.741e-03 | outLoss: 2.636e-05 |Time: 0.00s | rTime: 6.642e-02h | LR: 5.000e-03\n",
      "Epoch: 680| It: 0| Loss: 1.533e-02| physLoss: 3.090e-03 | lINITIAL: 1.815e-03 | wLoss: 8.855e-03 | inLoss: 1.554e-03 | outLoss: 1.564e-05 |Time: 0.00s | rTime: 6.748e-02h | LR: 5.000e-03\n",
      "Epoch: 680| It: 0| Loss: 1.533e-02| physLoss: 3.090e-03 | lINITIAL: 1.815e-03 | wLoss: 8.855e-03 | inLoss: 1.554e-03 | outLoss: 1.564e-05 |Time: 0.00s | rTime: 6.748e-02h | LR: 5.000e-03\n",
      "Epoch: 690| It: 0| Loss: 1.532e-02| physLoss: 3.161e-03 | lINITIAL: 1.856e-03 | wLoss: 8.534e-03 | inLoss: 1.755e-03 | outLoss: 1.186e-05 |Time: 0.00s | rTime: 6.844e-02h | LR: 5.000e-03\n",
      "Epoch: 690| It: 0| Loss: 1.532e-02| physLoss: 3.161e-03 | lINITIAL: 1.856e-03 | wLoss: 8.534e-03 | inLoss: 1.755e-03 | outLoss: 1.186e-05 |Time: 0.00s | rTime: 6.844e-02h | LR: 5.000e-03\n",
      "Epoch: 700| It: 0| Loss: 1.530e-02| physLoss: 3.168e-03 | lINITIAL: 1.764e-03 | wLoss: 8.784e-03 | inLoss: 1.540e-03 | outLoss: 3.807e-05 |Time: 0.00s | rTime: 6.950e-02h | LR: 5.000e-03\n",
      "Epoch: 700| It: 0| Loss: 1.530e-02| physLoss: 3.168e-03 | lINITIAL: 1.764e-03 | wLoss: 8.784e-03 | inLoss: 1.540e-03 | outLoss: 3.807e-05 |Time: 0.00s | rTime: 6.950e-02h | LR: 5.000e-03\n",
      "Epoch: 710| It: 0| Loss: 1.526e-02| physLoss: 3.207e-03 | lINITIAL: 1.789e-03 | wLoss: 8.628e-03 | inLoss: 1.627e-03 | outLoss: 7.707e-06 |Time: 0.00s | rTime: 7.053e-02h | LR: 5.000e-03\n",
      "Epoch: 710| It: 0| Loss: 1.526e-02| physLoss: 3.207e-03 | lINITIAL: 1.789e-03 | wLoss: 8.628e-03 | inLoss: 1.627e-03 | outLoss: 7.707e-06 |Time: 0.00s | rTime: 7.053e-02h | LR: 5.000e-03\n",
      "Epoch: 720| It: 0| Loss: 1.524e-02| physLoss: 3.134e-03 | lINITIAL: 1.720e-03 | wLoss: 8.858e-03 | inLoss: 1.518e-03 | outLoss: 9.839e-06 |Time: 0.00s | rTime: 7.156e-02h | LR: 5.000e-03\n",
      "Epoch: 720| It: 0| Loss: 1.524e-02| physLoss: 3.134e-03 | lINITIAL: 1.720e-03 | wLoss: 8.858e-03 | inLoss: 1.518e-03 | outLoss: 9.839e-06 |Time: 0.00s | rTime: 7.156e-02h | LR: 5.000e-03\n",
      "Epoch: 730| It: 0| Loss: 1.523e-02| physLoss: 3.064e-03 | lINITIAL: 1.864e-03 | wLoss: 8.604e-03 | inLoss: 1.682e-03 | outLoss: 1.581e-05 |Time: 0.00s | rTime: 7.261e-02h | LR: 5.000e-03\n",
      "Epoch: 730| It: 0| Loss: 1.523e-02| physLoss: 3.064e-03 | lINITIAL: 1.864e-03 | wLoss: 8.604e-03 | inLoss: 1.682e-03 | outLoss: 1.581e-05 |Time: 0.00s | rTime: 7.261e-02h | LR: 5.000e-03\n",
      "Epoch: 740| It: 0| Loss: 1.519e-02| physLoss: 2.925e-03 | lINITIAL: 1.942e-03 | wLoss: 8.862e-03 | inLoss: 1.449e-03 | outLoss: 1.012e-05 |Time: 0.00s | rTime: 7.354e-02h | LR: 5.000e-03\n",
      "Epoch: 740| It: 0| Loss: 1.519e-02| physLoss: 2.925e-03 | lINITIAL: 1.942e-03 | wLoss: 8.862e-03 | inLoss: 1.449e-03 | outLoss: 1.012e-05 |Time: 0.00s | rTime: 7.354e-02h | LR: 5.000e-03\n",
      "Epoch: 750| It: 0| Loss: 1.517e-02| physLoss: 2.898e-03 | lINITIAL: 2.018e-03 | wLoss: 8.648e-03 | inLoss: 1.600e-03 | outLoss: 1.069e-05 |Time: 0.00s | rTime: 7.450e-02h | LR: 5.000e-03\n",
      "Epoch: 750| It: 0| Loss: 1.517e-02| physLoss: 2.898e-03 | lINITIAL: 2.018e-03 | wLoss: 8.648e-03 | inLoss: 1.600e-03 | outLoss: 1.069e-05 |Time: 0.00s | rTime: 7.450e-02h | LR: 5.000e-03\n",
      "Epoch: 760| It: 0| Loss: 1.516e-02| physLoss: 2.864e-03 | lINITIAL: 1.854e-03 | wLoss: 8.807e-03 | inLoss: 1.613e-03 | outLoss: 2.227e-05 |Time: 0.00s | rTime: 7.537e-02h | LR: 5.000e-03\n",
      "Epoch: 760| It: 0| Loss: 1.516e-02| physLoss: 2.864e-03 | lINITIAL: 1.854e-03 | wLoss: 8.807e-03 | inLoss: 1.613e-03 | outLoss: 2.227e-05 |Time: 0.00s | rTime: 7.537e-02h | LR: 5.000e-03\n",
      "Epoch: 770| It: 0| Loss: 1.513e-02| physLoss: 2.847e-03 | lINITIAL: 1.828e-03 | wLoss: 8.684e-03 | inLoss: 1.759e-03 | outLoss: 1.710e-05 |Time: 0.00s | rTime: 7.629e-02h | LR: 5.000e-03\n",
      "Epoch: 770| It: 0| Loss: 1.513e-02| physLoss: 2.847e-03 | lINITIAL: 1.828e-03 | wLoss: 8.684e-03 | inLoss: 1.759e-03 | outLoss: 1.710e-05 |Time: 0.00s | rTime: 7.629e-02h | LR: 5.000e-03\n",
      "Epoch: 780| It: 0| Loss: 1.511e-02| physLoss: 2.765e-03 | lINITIAL: 1.874e-03 | wLoss: 8.784e-03 | inLoss: 1.677e-03 | outLoss: 1.318e-05 |Time: 0.00s | rTime: 7.725e-02h | LR: 5.000e-03\n",
      "Epoch: 780| It: 0| Loss: 1.511e-02| physLoss: 2.765e-03 | lINITIAL: 1.874e-03 | wLoss: 8.784e-03 | inLoss: 1.677e-03 | outLoss: 1.318e-05 |Time: 0.00s | rTime: 7.725e-02h | LR: 5.000e-03\n",
      "Epoch: 790| It: 0| Loss: 1.511e-02| physLoss: 2.743e-03 | lINITIAL: 1.946e-03 | wLoss: 8.635e-03 | inLoss: 1.776e-03 | outLoss: 1.309e-05 |Time: 0.00s | rTime: 7.821e-02h | LR: 5.000e-03\n",
      "Epoch: 790| It: 0| Loss: 1.511e-02| physLoss: 2.743e-03 | lINITIAL: 1.946e-03 | wLoss: 8.635e-03 | inLoss: 1.776e-03 | outLoss: 1.309e-05 |Time: 0.00s | rTime: 7.821e-02h | LR: 5.000e-03\n",
      "Epoch: 800| It: 0| Loss: 1.509e-02| physLoss: 2.745e-03 | lINITIAL: 1.818e-03 | wLoss: 8.780e-03 | inLoss: 1.733e-03 | outLoss: 8.818e-06 |Time: 0.00s | rTime: 7.912e-02h | LR: 5.000e-03\n",
      "Epoch: 800| It: 0| Loss: 1.509e-02| physLoss: 2.745e-03 | lINITIAL: 1.818e-03 | wLoss: 8.780e-03 | inLoss: 1.733e-03 | outLoss: 8.818e-06 |Time: 0.00s | rTime: 7.912e-02h | LR: 5.000e-03\n",
      "Epoch: 810| It: 0| Loss: 1.507e-02| physLoss: 2.793e-03 | lINITIAL: 1.799e-03 | wLoss: 8.618e-03 | inLoss: 1.852e-03 | outLoss: 7.339e-06 |Time: 0.00s | rTime: 8.012e-02h | LR: 5.000e-03\n",
      "Epoch: 810| It: 0| Loss: 1.507e-02| physLoss: 2.793e-03 | lINITIAL: 1.799e-03 | wLoss: 8.618e-03 | inLoss: 1.852e-03 | outLoss: 7.339e-06 |Time: 0.00s | rTime: 8.012e-02h | LR: 5.000e-03\n",
      "Epoch: 820| It: 0| Loss: 1.505e-02| physLoss: 2.796e-03 | lINITIAL: 1.832e-03 | wLoss: 8.668e-03 | inLoss: 1.745e-03 | outLoss: 1.268e-05 |Time: 0.00s | rTime: 8.106e-02h | LR: 5.000e-03\n",
      "Epoch: 820| It: 0| Loss: 1.505e-02| physLoss: 2.796e-03 | lINITIAL: 1.832e-03 | wLoss: 8.668e-03 | inLoss: 1.745e-03 | outLoss: 1.268e-05 |Time: 0.00s | rTime: 8.106e-02h | LR: 5.000e-03\n",
      "Epoch: 830| It: 0| Loss: 1.503e-02| physLoss: 2.790e-03 | lINITIAL: 1.874e-03 | wLoss: 8.710e-03 | inLoss: 1.653e-03 | outLoss: 5.999e-06 |Time: 0.00s | rTime: 8.200e-02h | LR: 5.000e-03\n",
      "Epoch: 830| It: 0| Loss: 1.503e-02| physLoss: 2.790e-03 | lINITIAL: 1.874e-03 | wLoss: 8.710e-03 | inLoss: 1.653e-03 | outLoss: 5.999e-06 |Time: 0.00s | rTime: 8.200e-02h | LR: 5.000e-03\n",
      "Epoch: 840| It: 0| Loss: 1.501e-02| physLoss: 2.801e-03 | lINITIAL: 1.892e-03 | wLoss: 8.599e-03 | inLoss: 1.711e-03 | outLoss: 7.523e-06 |Time: 0.00s | rTime: 8.294e-02h | LR: 5.000e-03\n",
      "Epoch: 840| It: 0| Loss: 1.501e-02| physLoss: 2.801e-03 | lINITIAL: 1.892e-03 | wLoss: 8.599e-03 | inLoss: 1.711e-03 | outLoss: 7.523e-06 |Time: 0.00s | rTime: 8.294e-02h | LR: 5.000e-03\n",
      "Epoch: 850| It: 0| Loss: 1.500e-02| physLoss: 2.812e-03 | lINITIAL: 1.878e-03 | wLoss: 8.641e-03 | inLoss: 1.658e-03 | outLoss: 9.093e-06 |Time: 0.00s | rTime: 8.388e-02h | LR: 5.000e-03\n",
      "Epoch: 850| It: 0| Loss: 1.500e-02| physLoss: 2.812e-03 | lINITIAL: 1.878e-03 | wLoss: 8.641e-03 | inLoss: 1.658e-03 | outLoss: 9.093e-06 |Time: 0.00s | rTime: 8.388e-02h | LR: 5.000e-03\n",
      "Epoch: 860| It: 0| Loss: 1.498e-02| physLoss: 2.827e-03 | lINITIAL: 1.888e-03 | wLoss: 8.651e-03 | inLoss: 1.601e-03 | outLoss: 9.803e-06 |Time: 0.00s | rTime: 8.491e-02h | LR: 5.000e-03\n",
      "Epoch: 860| It: 0| Loss: 1.498e-02| physLoss: 2.827e-03 | lINITIAL: 1.888e-03 | wLoss: 8.651e-03 | inLoss: 1.601e-03 | outLoss: 9.803e-06 |Time: 0.00s | rTime: 8.491e-02h | LR: 5.000e-03\n",
      "Epoch: 870| It: 0| Loss: 1.496e-02| physLoss: 2.834e-03 | lINITIAL: 1.871e-03 | wLoss: 8.632e-03 | inLoss: 1.608e-03 | outLoss: 1.063e-05 |Time: 0.00s | rTime: 8.595e-02h | LR: 5.000e-03\n",
      "Epoch: 870| It: 0| Loss: 1.496e-02| physLoss: 2.834e-03 | lINITIAL: 1.871e-03 | wLoss: 8.632e-03 | inLoss: 1.608e-03 | outLoss: 1.063e-05 |Time: 0.00s | rTime: 8.595e-02h | LR: 5.000e-03\n",
      "Epoch: 880| It: 0| Loss: 1.494e-02| physLoss: 2.843e-03 | lINITIAL: 1.871e-03 | wLoss: 8.507e-03 | inLoss: 1.705e-03 | outLoss: 9.529e-06 |Time: 0.00s | rTime: 8.687e-02h | LR: 5.000e-03\n",
      "Epoch: 880| It: 0| Loss: 1.494e-02| physLoss: 2.843e-03 | lINITIAL: 1.871e-03 | wLoss: 8.507e-03 | inLoss: 1.705e-03 | outLoss: 9.529e-06 |Time: 0.00s | rTime: 8.687e-02h | LR: 5.000e-03\n",
      "Epoch: 890| It: 0| Loss: 1.491e-02| physLoss: 2.798e-03 | lINITIAL: 1.855e-03 | wLoss: 8.664e-03 | inLoss: 1.586e-03 | outLoss: 9.850e-06 |Time: 0.00s | rTime: 8.772e-02h | LR: 5.000e-03\n",
      "Epoch: 890| It: 0| Loss: 1.491e-02| physLoss: 2.798e-03 | lINITIAL: 1.855e-03 | wLoss: 8.664e-03 | inLoss: 1.586e-03 | outLoss: 9.850e-06 |Time: 0.00s | rTime: 8.772e-02h | LR: 5.000e-03\n",
      "Epoch: 900| It: 0| Loss: 1.489e-02| physLoss: 2.807e-03 | lINITIAL: 1.919e-03 | wLoss: 8.484e-03 | inLoss: 1.673e-03 | outLoss: 1.172e-05 |Time: 0.00s | rTime: 8.870e-02h | LR: 5.000e-03\n",
      "Epoch: 900| It: 0| Loss: 1.489e-02| physLoss: 2.807e-03 | lINITIAL: 1.919e-03 | wLoss: 8.484e-03 | inLoss: 1.673e-03 | outLoss: 1.172e-05 |Time: 0.00s | rTime: 8.870e-02h | LR: 5.000e-03\n",
      "Epoch: 910| It: 0| Loss: 1.487e-02| physLoss: 2.821e-03 | lINITIAL: 1.819e-03 | wLoss: 8.589e-03 | inLoss: 1.633e-03 | outLoss: 1.166e-05 |Time: 0.00s | rTime: 8.959e-02h | LR: 5.000e-03\n",
      "Epoch: 910| It: 0| Loss: 1.487e-02| physLoss: 2.821e-03 | lINITIAL: 1.819e-03 | wLoss: 8.589e-03 | inLoss: 1.633e-03 | outLoss: 1.166e-05 |Time: 0.00s | rTime: 8.959e-02h | LR: 5.000e-03\n",
      "Epoch: 920| It: 0| Loss: 1.485e-02| physLoss: 2.860e-03 | lINITIAL: 1.774e-03 | wLoss: 8.509e-03 | inLoss: 1.698e-03 | outLoss: 1.190e-05 |Time: 0.00s | rTime: 9.057e-02h | LR: 5.000e-03\n",
      "Epoch: 920| It: 0| Loss: 1.485e-02| physLoss: 2.860e-03 | lINITIAL: 1.774e-03 | wLoss: 8.509e-03 | inLoss: 1.698e-03 | outLoss: 1.190e-05 |Time: 0.00s | rTime: 9.057e-02h | LR: 5.000e-03\n",
      "Epoch: 930| It: 0| Loss: 1.483e-02| physLoss: 2.839e-03 | lINITIAL: 1.819e-03 | wLoss: 8.534e-03 | inLoss: 1.625e-03 | outLoss: 1.425e-05 |Time: 0.00s | rTime: 9.151e-02h | LR: 5.000e-03\n",
      "Epoch: 930| It: 0| Loss: 1.483e-02| physLoss: 2.839e-03 | lINITIAL: 1.819e-03 | wLoss: 8.534e-03 | inLoss: 1.625e-03 | outLoss: 1.425e-05 |Time: 0.00s | rTime: 9.151e-02h | LR: 5.000e-03\n",
      "Epoch: 940| It: 0| Loss: 1.481e-02| physLoss: 2.811e-03 | lINITIAL: 1.894e-03 | wLoss: 8.520e-03 | inLoss: 1.574e-03 | outLoss: 1.082e-05 |Time: 0.00s | rTime: 9.244e-02h | LR: 5.000e-03\n",
      "Epoch: 940| It: 0| Loss: 1.481e-02| physLoss: 2.811e-03 | lINITIAL: 1.894e-03 | wLoss: 8.520e-03 | inLoss: 1.574e-03 | outLoss: 1.082e-05 |Time: 0.00s | rTime: 9.244e-02h | LR: 5.000e-03\n",
      "Epoch: 950| It: 0| Loss: 1.479e-02| physLoss: 2.819e-03 | lINITIAL: 1.900e-03 | wLoss: 8.451e-03 | inLoss: 1.608e-03 | outLoss: 1.145e-05 |Time: 0.00s | rTime: 9.342e-02h | LR: 5.000e-03\n",
      "Epoch: 950| It: 0| Loss: 1.479e-02| physLoss: 2.819e-03 | lINITIAL: 1.900e-03 | wLoss: 8.451e-03 | inLoss: 1.608e-03 | outLoss: 1.145e-05 |Time: 0.00s | rTime: 9.342e-02h | LR: 5.000e-03\n",
      "Epoch: 960| It: 0| Loss: 1.477e-02| physLoss: 2.839e-03 | lINITIAL: 1.851e-03 | wLoss: 8.494e-03 | inLoss: 1.571e-03 | outLoss: 1.261e-05 |Time: 0.00s | rTime: 9.431e-02h | LR: 5.000e-03\n",
      "Epoch: 960| It: 0| Loss: 1.477e-02| physLoss: 2.839e-03 | lINITIAL: 1.851e-03 | wLoss: 8.494e-03 | inLoss: 1.571e-03 | outLoss: 1.261e-05 |Time: 0.00s | rTime: 9.431e-02h | LR: 5.000e-03\n",
      "Epoch: 970| It: 0| Loss: 1.475e-02| physLoss: 2.872e-03 | lINITIAL: 1.837e-03 | wLoss: 8.464e-03 | inLoss: 1.563e-03 | outLoss: 1.190e-05 |Time: 0.00s | rTime: 9.531e-02h | LR: 5.000e-03\n",
      "Epoch: 970| It: 0| Loss: 1.475e-02| physLoss: 2.872e-03 | lINITIAL: 1.837e-03 | wLoss: 8.464e-03 | inLoss: 1.563e-03 | outLoss: 1.190e-05 |Time: 0.00s | rTime: 9.531e-02h | LR: 5.000e-03\n",
      "Epoch: 980| It: 0| Loss: 1.473e-02| physLoss: 2.870e-03 | lINITIAL: 1.819e-03 | wLoss: 8.477e-03 | inLoss: 1.548e-03 | outLoss: 1.188e-05 |Time: 0.00s | rTime: 9.631e-02h | LR: 5.000e-03\n",
      "Epoch: 980| It: 0| Loss: 1.473e-02| physLoss: 2.870e-03 | lINITIAL: 1.819e-03 | wLoss: 8.477e-03 | inLoss: 1.548e-03 | outLoss: 1.188e-05 |Time: 0.00s | rTime: 9.631e-02h | LR: 5.000e-03\n",
      "Epoch: 990| It: 0| Loss: 1.470e-02| physLoss: 2.864e-03 | lINITIAL: 1.865e-03 | wLoss: 8.371e-03 | inLoss: 1.593e-03 | outLoss: 1.000e-05 |Time: 0.00s | rTime: 9.733e-02h | LR: 5.000e-03\n",
      "Epoch: 990| It: 0| Loss: 1.470e-02| physLoss: 2.864e-03 | lINITIAL: 1.865e-03 | wLoss: 8.371e-03 | inLoss: 1.593e-03 | outLoss: 1.000e-05 |Time: 0.00s | rTime: 9.733e-02h | LR: 5.000e-03\n"
     ]
    }
   ],
   "source": [
    "N_f = 10000\n",
    "Nu1 = 200\n",
    "\n",
    "\n",
    "\n",
    "model.train(collo[::10,:], inlet, outlet, wall, initial, tf_iter=1000,   tf_iter2=100, newton_iter2=1500)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"noslipwall_02_Z0slice_2D_innerdomain\" \n",
    "\n",
    "test_data_innerdomain = get_testing_dataset(path   , part=key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting nn solution\n",
      "\n",
      "Predicting nn solution\n",
      "\n",
      " Relative L2 ERROR:\n",
      "\n",
      " Relative L2 ERROR:\n",
      "domain U velocity  : 8.777e+01 %\n",
      "domain U velocity  : 8.777e+01 %\n",
      "domain V velocity  : 7.453e+03 %\n",
      "domain V velocity  : 7.453e+03 %\n",
      "domain P Pressure  : 1.084e+02 %\n",
      "domain P Pressure  : 1.084e+02 %\n",
      "\n",
      " Relative l1 error\n",
      "\n",
      " Relative l1 error\n",
      "domain U velocity  : 8.350e+01 %\n",
      "domain U velocity  : 8.350e+01 %\n",
      "domain V velocity  : 1.413e+04 %\n",
      "domain V velocity  : 1.413e+04 %\n",
      "domain P Pressure  : 1.045e+02 %\n",
      "domain P Pressure  : 1.045e+02 %\n",
      "/media/afrah2/MyWork/files2023/SK/dwPINNs/utilities.py:653: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n",
      "/media/afrah2/MyWork/files2023/SK/dwPINNs/utilities.py:653: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n",
      "/media/afrah2/MyWork/files2023/SK/dwPINNs/utilities.py:653: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TwoD_BFS_Slip_PINN' object has no attribute 'weight_change_per_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24705/644013940.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_result2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_24705/1325692979.py\u001b[0m in \u001b[0;36mplot_result2\u001b[0;34m(model, path)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mdraw_contourf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m17.5\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlabelsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12.5\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maxes_pad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_change_per_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TwoD_BFS_Slip_PINN' object has no attribute 'weight_change_per_layer'"
     ]
    }
   ],
   "source": [
    "plot_result2(model , path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[5.00000e+00],\n",
       "        [5.00000e+00],\n",
       "        [5.00000e+00],\n",
       "        ...,\n",
       "        [4.50000e-01],\n",
       "        [4.50000e-01],\n",
       "        [4.50000e-01]],\n",
       "\n",
       "       [[2.00500e-01],\n",
       "        [2.00500e-01],\n",
       "        [2.00500e-01],\n",
       "        ...,\n",
       "        [9.99500e-01],\n",
       "        [9.99500e-01],\n",
       "        [9.99500e-01]],\n",
       "\n",
       "       [[7.17720e-03],\n",
       "        [6.53162e-03],\n",
       "        [5.88605e-03],\n",
       "        ...,\n",
       "        [2.65096e-03],\n",
       "        [3.55705e-03],\n",
       "        [3.24514e-03]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([test_data_innerdomain[:,  0:1] , test_data_innerdomain[:,  1:2]  , test_data_innerdomain[:,  2:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_innerdomain[:, 1:2].shape\n",
    "\n",
    "test = ([test_data_innerdomain[:, 1:2] , test_data_innerdomain[:, 2:3] , test_data_innerdomain[:, 0:1]])\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.print(f\"weight_ub: {weight_initial}  weight_fu: {weight_fu}\")\n",
    "u_pred, v_pred, p_pred = model.predict(test_data_innerdomain[:,  0:1] , test_data_innerdomain[:,  1:2]  , test_data_innerdomain[:,  2:3])\n",
    "\n",
    "\n",
    "\n",
    "# lbfgs(loss_and_flat_grad, get_weights(u_model), Struct(), maxIter=newton_iter2, learningRate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error u: 8.883702e-01\n",
      "Error v: 2.271106e+02\n",
      "Error v: 1.168959e+00\n",
      "Starting L-BFGS training\n"
     ]
    }
   ],
   "source": [
    "error_u = np.linalg.norm(test_data_innerdomain[:,  3:4] - u_pred, 2) / np.linalg.norm(test_data_innerdomain[:,  3:4] , 2)\n",
    "print('Error u: %e' % (error_u))\n",
    "error_v = np.linalg.norm(test_data_innerdomain[:,  4:5] - v_pred, 2) / np.linalg.norm(test_data_innerdomain[:,  4:5], 2)\n",
    "print('Error v: %e' % (error_v))\n",
    "error_v = np.linalg.norm(test_data_innerdomain[:,  5:6] - v_pred, 2) / np.linalg.norm(test_data_innerdomain[:,  5:6], 2)\n",
    "print('Error v: %e' % (error_v))\n",
    "print(\"Starting L-BFGS training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loss_funct() missing 1 required positional argument: 'weight_fu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11589/61811525.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloss_and_flat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss_and_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_star1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_star1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_star1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mub\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvb\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mweight_ub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_fu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlbfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_and_flat_grad\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_innerdomain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/afrah2/MyWork/files2023/SK/dwPINNs/eager_lbfgs.py\u001b[0m in \u001b[0;36mlbfgs\u001b[0;34m(opfunc, x, state, maxIter, learningRate, do_verbose)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mf_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11589/705365439.py\u001b[0m in \u001b[0;36mloss_and_flat_grad\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_funct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_f_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_f_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_f_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mub_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_ub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_fu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mgrad_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: loss_funct() missing 1 required positional argument: 'weight_fu'"
     ]
    }
   ],
   "source": [
    "loss_and_flat_grad = get_loss_and_flat_grad(t_star1 , x_star1, y_star1, tb ,  xb, yb,  ub,  vb , weight_initial, weight_fu)\n",
    "\n",
    "lbfgs(loss_and_flat_grad,  get_weights(u_model), Struct(), maxIter = 500, learningRate=0.8)\n",
    "\n",
    "u_pred, v_pred, p_pred = predict(test_data_innerdomain[:,  0:3])\n",
    "error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
    "print('Error u: %e' % (error_u))\n",
    "error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
    "print('Error v: %e' % (error_v))\n",
    "\n",
    "error_p = np.linalg.norm(p_exact1 - p_pred, 2) / np.linalg.norm(p_exact1, 2)\n",
    "print('Error p: %e' % (error_p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1488.1751\n",
      "Error u: 4.266133e+01\n",
      "Error v: 4.266161e+01\n",
      "Error p: 8.245512e+01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "elapsed = time.time() - start_time\n",
    "print('Training time: %.4f' % (elapsed))\n",
    "\n",
    "u_pred, v_pred, p_pred = predict(X_star1)\n",
    "\n",
    "# U_pred = u_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
    "# V_pred = v_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
    "# P_pred = p_pred.reshape((x1.shape[0], y1.shape[0], t1.shape[0]))\n",
    "u_exact1 = np.array([X_star1[:, 3]])\n",
    "v_exact1 = np.array([X_star1[:, 4]])\n",
    "p_exact1 = np.array([X_star1[:, 5]])\n",
    "\n",
    "\n",
    "error_uu = np.abs( u_exact1 - u_pred)\n",
    "error_vv = np.abs(v_exact1 - v_pred)\n",
    "error_pp = np.abs( p_exact1 - p_pred)\n",
    "\n",
    "error_u = np.linalg.norm(u_exact1 - u_pred, 2) / np.linalg.norm(u_exact1, 2)\n",
    "print('Error u: %e' % (error_u))\n",
    "\n",
    "error_v = np.linalg.norm(v_exact1 - v_pred, 2) / np.linalg.norm(v_exact1, 2)\n",
    "print('Error v: %e' % (error_v))\n",
    "\n",
    "error_p = np.linalg.norm(p_exact1 - p_pred, 2) / np.linalg.norm(p_exact1, 2)\n",
    "print('Error p: %e' % (error_p))\n",
    "\n",
    "# dataNewNS = 'NS_hisyory.mat'\n",
    "# scipy.io.savemat(dataNewNS, {'w_MSE_b': MSE_b1, 'w_MSE_f': MSE_f1, 'weight_u': weightu,\n",
    "#                   'weight_f': weightf, 'U_pred': u_pred, 'V_pred': V_pred, 'P_pred': P_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
