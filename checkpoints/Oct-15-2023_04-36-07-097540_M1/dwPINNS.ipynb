{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_32177/2648050088.py:10: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Oct 24 13:02:32 2021\n",
    "\n",
    "@author: lenovo\n",
    "\"\"\"\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "import math\n",
    "import matplotlib.gridspec as gridspec\n",
    "from plotting import newfig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import layers, activations\n",
    "from scipy.interpolate import griddata\n",
    "from eager_lbfgs import lbfgs, Struct\n",
    "from pyDOE import lhs\n",
    "\n",
    "# from generate_dataset import *\n",
    "# from utilities import *\n",
    "import os.path\n",
    "\n",
    "import shutil\n",
    "import datetime\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_net(object):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "\n",
    "        self.layer_sizes = layers\n",
    "\n",
    "\n",
    "\n",
    "        # L-BFGS weight getting and setting from https://github.com/pierremtb/PINNs-TF2.0\n",
    "\n",
    "\n",
    "\n",
    "    def xavier_init(self, layer_sizes):\n",
    "        in_dim = layer_sizes[0]\n",
    "        out_dim = layer_sizes[1]\n",
    "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "\n",
    "    def model(self):\n",
    "\n",
    "        input_tensor = keras.Input(shape=(self.layer_sizes[0],))\n",
    "\n",
    "        hide_layer_list = []\n",
    "        flag = True\n",
    "        for width in self.layer_sizes[1:-1]:\n",
    "            if flag:\n",
    "                x = layers.Dense(\n",
    "                    width, activation=tf.nn.tanh,\n",
    "                    kernel_initializer=\"glorot_normal\")(input_tensor)\n",
    "                flag = False\n",
    "            else:\n",
    "                x = layers.Dense(\n",
    "                    width, activation=tf.nn.tanh,\n",
    "                    kernel_initializer=\"glorot_normal\")(x)\n",
    "        output_tensor = layers.Dense(self.layer_sizes[-1], activation=None,kernel_initializer=\"glorot_normal\")(x)\n",
    "        # print(\"xxxxxxxxxxxxxx\")\n",
    "        output0 = output_tensor[:, 0:1]\n",
    "        output1 = output_tensor[:, 1:2]\n",
    "        output2 = output_tensor[:, 2:3]\n",
    "\n",
    "        model_output = keras.models.Model(input_tensor, [output0, output1, output2])\n",
    "        model_output.summary()\n",
    "\n",
    "        return model_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoD_BFS_Slip_PINN:\n",
    "\n",
    "    def __init__(self, data, layers  , activFun , mode , starter_learning_rate = 1.0e-3 , ExistModel=0):\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "        self.layers = layers # \n",
    "\n",
    "        self.adpative_constant_WALL_log = []\n",
    "        \n",
    "        self.adpative_constant_INITIAL_log = []  \n",
    "\n",
    "        self.adpative_constant_INLET_log = []  \n",
    "        \n",
    "        self.adpative_constant_OUTLET_log = []  \n",
    "        self.adpative_constant_F_log = []  \n",
    "\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.90, staircase=False)\n",
    "\n",
    "        self.weight_initial = tf.Variable([1.0], dtype=tf.float32)\n",
    "        self.weight_wall = tf.Variable([1.0], dtype=tf.float32)\n",
    "        self.weight_inlet = tf.Variable([1.0], dtype=tf.float32)\n",
    "        self.weight_outlet = tf.Variable([1.0], dtype=tf.float32)\n",
    "\n",
    "        self.weight_fu = tf.Variable([1.0], dtype=tf.float32)\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=self.learning_rate , beta_1=.99)\n",
    "        self.optimizer_fu= tf.keras.optimizers.Adam(lr=0.003, beta_1=.99)\n",
    "        self.optimizer_initial = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
    "        self.optimizer_wall = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
    "        self.optimizer_inlet = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
    "        self.optimizer_outlet = tf.keras.optimizers.Adam(lr=0.03, beta_1=.99)\n",
    "\n",
    "        # initialize the NN\n",
    "        self.net_cuvwp = neural_net(layers).model()\n",
    "\n",
    "        # self.loss_tensor_list = [ self.loss_OUTLET,  self.loss_INLET , self.loss_WALL, self.loss_res, self.loss_INITIAL] \n",
    "        self.loss_list = [\"loss_OUTLET\" , \"loss_INLET\", \"loss_WALL\" , \"loss_Phys\", \"loss_Initial\" ]\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "\n",
    "        self.sizes_w = []\n",
    "        self.sizes_b = []\n",
    "        for i, width in enumerate(self.layers):\n",
    "            if i != 1:\n",
    "                self.sizes_w.append(int(width * self.layers[1]))\n",
    "                self.sizes_b.append(int(width if i != 0 else self.layers[1]))\n",
    "\n",
    "    def set_weights(self , new_weight):  # 重新设置参数\n",
    "\n",
    "        for i, layer in enumerate(self.net_cuvwp.layers[1:len(self.sizes_w) + 1]):\n",
    "            start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "            end_weights = sum(self.sizes_w[:i + 1]) + sum(self.sizes_b[:i])\n",
    "            weights = new_weight[start_weights:end_weights]\n",
    "            w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "            weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "            biases = new_weight[end_weights:end_weights + self.sizes_b[i]]\n",
    "            weights_biases = [weights, biases]\n",
    "            layer.set_weights(weights_biases)\n",
    "\n",
    "    def get_weights(self):\n",
    "        w = []\n",
    "        for layer in self.net_cuvwp.layers[1:len(self.sizes_w) + 1]:\n",
    "            weights_biases = layer.get_weights()\n",
    "            weights = weights_biases[0].flatten()\n",
    "            biases = weights_biases[1]\n",
    "            w.extend(weights)\n",
    "            w.extend(biases)\n",
    "        w = tf.convert_to_tensor(w)\n",
    "        return w\n",
    "    \n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        loss_list = [\"loss_OUTLET\" , \"loss_INLET\", \"loss_WALL\" ,\"loss_Initial\"]\n",
    "        ax = plt.subplot(1 , 2 , 1)\n",
    "    #         fig.set_size_inches([15,8])\n",
    "        for key in loss_list:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=7)\n",
    "        ax.set_ylabel(\"loss\", fontsize=7)\n",
    "        ax.set_yscale('log')\n",
    "        ax.tick_params(labelsize=7)\n",
    "        ax.legend()\n",
    "\n",
    "\n",
    "        ax = plt.subplot(1,2,2)\n",
    "        self.print(\"Final loss %s: %e\" % (\"loss_Phys\", self.loss_history[\"loss_Phys\"][-1]))\n",
    "        ax.semilogy(self.loss_history[\"loss_Phys\"], label=\"loss_Phys\")\n",
    "        ax.set_xlabel(\"epochs\", fontsize=7)\n",
    "        ax.set_ylabel(\"loss\", fontsize=7)\n",
    "        ax.set_yscale('log')\n",
    "        ax.tick_params(labelsize=7)\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        text = 'loss_history.png' \n",
    "        plt.savefig(os.path.join(self.dirname,text))\n",
    "        plt.close(\"all\")\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def net_f(self,t ,  x, y):\n",
    "\n",
    "        mu = 0.00345\n",
    "        density = 1056\n",
    "\n",
    "        u, v, p = self.net_cuvwp(tf.concat([t, x , y],1))\n",
    "\n",
    "        \n",
    "        u_t = tf.gradients(u, t)[0]\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        u_y = tf.gradients(u, y)[0]\n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "        u_yy = tf.gradients(u_y, y)[0]\n",
    "\n",
    "        v_t = tf.gradients(v, t)[0]\n",
    "        v_x = tf.gradients(v, x)[0]\n",
    "        v_y = tf.gradients(v, y)[0]\n",
    "        v_xx = tf.gradients(v_x, x)[0]\n",
    "        v_yy = tf.gradients(v_y, y)[0]\n",
    "\n",
    "        p_x = tf.gradients(p, x)[0]\n",
    "        p_y = tf.gradients(p, y)[0]\n",
    "\n",
    "        f_u = u_t + (u * u_x + v * u_y ) + 1.0/density * p_x - mu *(u_xx + u_yy )\n",
    "        f_v = v_t + (u * v_x + v * v_y ) + 1.0/density * p_y - mu *(v_xx + v_yy )\n",
    "        div = u_x + v_y \n",
    "            \n",
    "\n",
    "        return f_u, f_v, div\n",
    "\n",
    "\n",
    "    # define the loss\n",
    "    def loss_funct(self ,t_c_tf , x_c_tf, y_c_tf, t_WALL_tf , x_WALL_tf, y_WALL_tf, p_WALL_tf ,\n",
    "                    t_INLET_tf , x_INLET_tf, y_INLET_tf, u_INLET_tf ,  v_INLET_tf ,t_OUTLET_tf , x_OUTLET_tf, y_OUTLET_tf, u_OUTLET_tf , v_OUTLET_tf,\n",
    "                    t_INITIAL_tf , x_INITIAL_tf, y_INITIAL_tf , u_INITIAL_tf, v_INITIAL_tf, p_INITIAL_tf ):\n",
    "\n",
    "\n",
    "        [f_u, f_v, f_e] = self.net_f(t_c_tf ,x_c_tf ,y_c_tf)\n",
    "    \n",
    "        [u_WALL_pred, v_WALL_pred, p_WALL_pred] = self.net_cuvwp(tf.concat([t_WALL_tf, x_WALL_tf,y_WALL_tf],1))\n",
    "        \n",
    "        [u_INLET_pred, v_INLET_pred, p_INLET_pred] = self.net_cuvwp(tf.concat([t_INLET_tf, x_INLET_tf,y_INLET_tf],1))\n",
    "\n",
    "        [u_OUTLET_pred, v_OUTLET_pred,  p_OUTLET_pred ]= self.net_cuvwp(tf.concat([t_OUTLET_tf,x_OUTLET_tf, y_OUTLET_tf],1))\n",
    "\n",
    "        \n",
    "        [u_INITIAL_pred, v_INITIAL_pred,  p_INITIAL_pred ]= self.net_cuvwp( tf.concat([t_INITIAL_tf, x_INITIAL_tf, y_INITIAL_tf  ],1))\n",
    "\n",
    "        loss_res = self.weight_fu *( tf.reduce_mean(tf.square(f_u)) + 1.0 *  tf.reduce_mean(tf.square(f_v)) + tf.reduce_mean(tf.square(f_e)))\n",
    "        \n",
    "        ##############################################################################\n",
    "        # loss_res = 1.0 *tf.reduce_mean(tf.square(u_c_pred  - u_c_tf)) + 1 * tf.reduce_mean(tf.square(v_c_pred  -v_c_tf )) + 1 * tf.reduce_mean(tf.square(p_c_pred - p_c_tf)) \n",
    "    \n",
    "    # on the Wall :supervised leaning:  U_predict - Uexact , V_predict - V_exact and  P_predict - P_exact and the gradient : P_y = 0  \n",
    "        loss_wall =   self.weight_wall *  (tf.reduce_mean(tf.square(u_WALL_pred ))  + 1.0* tf.reduce_mean(tf.square(v_WALL_pred)))\n",
    "                                                                                                        # + 1000 * tf.reduce_mean(tf.square(p_WALL_pred - p_WALL_tf)) + tf.square(tf.gradients(p_WALL_pred, y_WALL_tf)[0])) \n",
    "        \n",
    "        \n",
    "    # on the INLET :supervised leaning:  U_predict - Uexact , V_predict - V_exact and  P_predict - P_exact and the gradient : P_x = 0  \n",
    "        loss_inlet =   self.weight_inlet  * ( tf.reduce_mean(tf.square(u_INLET_pred - u_INLET_tf))  +  1.0 * tf.reduce_mean(tf.square(v_INLET_pred -v_INLET_tf )))\n",
    "        #  + \\                1000.0 * tf.reduce_mean(tf.square(p_INLET_pred - p_INLET_tf)) #+ tf.square(tf.gradients(p_INLET_pred, x_INLET_tf)[0])) \n",
    "        \n",
    "    # on the OUTLET :supervised leaning:  U_predict - Uexact , V_predict - V_exact and  P_predict - P_exact and the gradient : U_x = 0 and V_x = 0   \n",
    "        loss_outlet =   self.weight_outlet * (tf.reduce_mean(tf.square(p_OUTLET_pred )) )# 50.0  *    tf.reduce_mean(tf.square(u_OUTLET_pred - u_OUTLET_tf) + tf.square(tf.gradients(u_OUTLET_pred , x_OUTLET_tf)[0])) + \\        2000.0 * tf.reduce_mean(tf.square(v_OUTLET_pred - v_OUTLET_tf) + tf.square(tf.gradients(v_OUTLET_pred , x_OUTLET_tf)[0])) +  \n",
    "        \n",
    "\n",
    "    # on the Initial condition :supervised leaning:  U_predict - Uexact , V_predict - V_exact and  P_predict - P_exact \n",
    "        loss_initial = self.weight_initial *  (tf.reduce_mean(tf.square(u_INITIAL_pred - u_INITIAL_tf)) +  tf.reduce_mean(tf.square(v_INITIAL_pred - v_INITIAL_tf)) +  tf.reduce_mean(tf.square(p_INITIAL_pred - p_INITIAL_tf)))\n",
    "\n",
    "        \n",
    "        loss = loss_wall + loss_initial + loss_res + loss_inlet + loss_outlet\n",
    "\n",
    "        return  loss_res, loss_wall , loss_inlet , loss_outlet , loss_initial , loss\n",
    "    \n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def grad(self ,t_c_tf , x_c_tf, y_c_tf,\n",
    "             t_WALL_tf , x_WALL_tf, y_WALL_tf, p_WALL_tf ,\n",
    "               t_INLET_tf , x_INLET_tf, y_INLET_tf, u_INLET_tf ,  v_INLET_tf ,\n",
    "                 t_OUTLET_tf , x_OUTLET_tf, y_OUTLET_tf, u_OUTLET_tf , v_OUTLET_tf,\n",
    "                 t_INITIAL_tf , x_INITIAL_tf, y_INITIAL_tf , u_INITIAL_tf, v_INITIAL_tf, p_INITIAL_tf ):\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            loss_res, loss_wall , loss_inlet , loss_outlet , loss_initial , loss = self.loss_funct(t_c_tf , x_c_tf, y_c_tf, t_WALL_tf , x_WALL_tf, y_WALL_tf, p_WALL_tf ,\n",
    "                                                                                                   t_INLET_tf , x_INLET_tf, y_INLET_tf, u_INLET_tf ,  v_INLET_tf ,t_OUTLET_tf , x_OUTLET_tf, y_OUTLET_tf, u_OUTLET_tf , v_OUTLET_tf,\n",
    "                                                                                                     t_INITIAL_tf , x_INITIAL_tf, y_INITIAL_tf , u_INITIAL_tf, v_INITIAL_tf, p_INITIAL_tf )\n",
    "            grads = tape.gradient(loss, self.net_cuvwp.trainable_variables)\n",
    "\n",
    "            grads_initial = tape.gradient(loss, self.weight_initial)\n",
    "            grads_fu = tape.gradient(loss, self.weight_fu)\n",
    "\n",
    "            grads_wall = tape.gradient(loss, self.weight_wall)\n",
    "            grads_inlet = tape.gradient(loss, self.weight_inlet)\n",
    "\n",
    "            grads_outlet = tape.gradient(loss, self.weight_outlet)\n",
    "\n",
    "\n",
    "        return   loss_res, loss_wall , loss_inlet , loss_outlet , loss_initial , loss, grads , grads_fu , grads_initial, grads_wall , grads_inlet , grads_outlet\n",
    "\n",
    "\n",
    "\n",
    "#model.train(collo[::10,:], inlet, outlet, wall, initial,  weight_ub, weight_fu, tf_iter=1000,   tf_iter2=100, newton_iter2=1500)\n",
    "    def train( self , collo, inlet, outlet, wall, initial ,  tf_iter, tf_iter2, newton_iter2):\n",
    "\n",
    "        batch_size =  120\n",
    "        n_batches =  collo.shape[0] // batch_size\n",
    "        start_time = time.time()\n",
    "\n",
    "        running_time = 0\n",
    "\n",
    "\n",
    "        # tf.print(f\"weight_initial: {self.weight_initial}  weight_fu: {self.weight_fu}\")\n",
    "        print(\"starting Adam training\")\n",
    "\n",
    "        a = np.random.rand(1000)\n",
    "        loss_history = list(a)\n",
    "        MSE_b0 = list(a)\n",
    "        MSE_f0 = list(a)\n",
    "\n",
    " \n",
    "        \n",
    "        # print(wall)\n",
    "        # For mini-batch (if used)\n",
    "        for epoch in range(tf_iter):\n",
    "            for it in range(n_batches):\n",
    "\n",
    "                t_WALL_tf= wall[: , 0:1]\n",
    "                x_WALL_tf= wall[: , 1:2]\n",
    "                y_WALL_tf= wall[: , 2:3]\n",
    "                p_WALL_tf= wall[: , 5:6]\n",
    "\n",
    "                t_INLET_tf= inlet[: , 0:1]\n",
    "                x_INLET_tf= inlet[: , 1:2]\n",
    "                y_INLET_tf= inlet[: , 2:3]\n",
    "                u_INLET_tf= inlet[: , 3:4]\n",
    "                v_INLET_tf= inlet[: , 4:5]\n",
    "                p_INLET_tf= inlet[: , 5:6]\n",
    "\n",
    "                t_OUTLET_tf= outlet[: , 0:1]\n",
    "                x_OUTLET_tf= outlet[: , 1:2]\n",
    "                y_OUTLET_tf= outlet[: , 2:3]\n",
    "                u_OUTLET_tf= outlet[: , 3:4]\n",
    "                v_OUTLET_tf= outlet[: , 4:5]\n",
    "\n",
    "                t_INITIAL_tf= initial[: , 0:1]\n",
    "                x_INITIAL_tf= initial[: , 1:2]\n",
    "                y_INITIAL_tf= initial[: , 2:3]\n",
    "                u_INITIAL_tf= initial[: , 3:4]\n",
    "                v_INITIAL_tf= initial[: , 4:5]\n",
    "                p_INITIAL_tf= initial[: , 5:6]\n",
    "\n",
    "\n",
    "                x_f_tf = collo[:, 1:2][it * batch_size:(it * batch_size + batch_size), ]\n",
    "                y_f_tf = collo[:, 2:3][it * batch_size:(it * batch_size + batch_size), ]\n",
    "                t_f_tf = collo[:, 0:1][it * batch_size:(it * batch_size + batch_size), ]\n",
    "                \n",
    "                elapsed = time.time() - start_time\n",
    "                running_time += elapsed\n",
    "\n",
    "                loss_res, loss_wall , loss_inlet , loss_outlet , loss_initial , loss,grads , grads_fu , grads_initial, grads_wall , grads_inlet , grads_outlet = self.grad( t_f_tf , x_f_tf, y_f_tf,\n",
    "                                                                                                                                                         t_WALL_tf , x_WALL_tf, y_WALL_tf, p_WALL_tf ,\n",
    "                                                                                                                                                           t_INLET_tf , x_INLET_tf, y_INLET_tf, u_INLET_tf ,  v_INLET_tf ,\n",
    "                                                                                                                                                             t_OUTLET_tf , x_OUTLET_tf, y_OUTLET_tf, u_OUTLET_tf , v_OUTLET_tf,\n",
    "                                                                                                                                                               t_INITIAL_tf , x_INITIAL_tf, y_INITIAL_tf , u_INITIAL_tf, v_INITIAL_tf, p_INITIAL_tf )\n",
    "                \n",
    "                \n",
    "\n",
    "                # print(\"starting Adam training\")\n",
    "        # self.loss_list = [\"loss_OUTLET\" , \"loss_INLET\", \"loss_WALL\" , \"loss_Phys\", \"loss_Initial\" ]\n",
    "\n",
    "                self.optimizer.apply_gradients(zip(grads, self.net_cuvwp.trainable_variables))\n",
    "\n",
    "\n",
    "                batch_losses =[loss_outlet.numpy()[0], loss_inlet.numpy()[0] , loss_wall.numpy()[0] , loss_res.numpy()[0] ,loss_initial.numpy()[0] ]\n",
    "\n",
    "                self.assign_batch_losses(batch_losses)\n",
    "                for key in self.loss_history:\n",
    "                    self.loss_history[key].append(self.epoch_loss[key])\n",
    "                start_time = time.time()\n",
    "                                    \n",
    " \n",
    "            if epoch % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                self.print('Epoch: %d| It: %d| Loss: %.3e| physLoss: %.3e | lINITIAL: %.3e | wLoss: %.3e | inLoss: %.3e | outLoss: %.3e |Time: %.2fs | rTime: %.3eh | LR: %.3e'\n",
    "                          %(epoch , it, loss.numpy()[0], loss_res.numpy()[0], loss_initial.numpy()[0] , loss_wall.numpy()[0] , loss_inlet.numpy()[0] , loss_outlet.numpy()[0] , elapsed, running_time, 0.005))\n",
    "                \n",
    "                self.adpative_constant_INITIAL_log.append(self.weight_initial)\n",
    "                self.adpative_constant_INLET_log.append(self.weight_inlet)\n",
    "                self.adpative_constant_OUTLET_log.append(self.weight_outlet)\n",
    "                self.adpative_constant_WALL_log.append(self.weight_wall)\n",
    "                self.adpative_constant_F_log.append(self.weight_fu)\n",
    "\n",
    "\n",
    "                # if loss_history[-1] < loss_history[-2] and loss_history[-2] < loss_history[-3] and loss_history[-1] < loss_history[-10]:\n",
    "                self.optimizer_outlet.apply_gradients(zip([-grads_outlet], [self.weight_outlet]))\n",
    "                self.optimizer_inlet.apply_gradients(zip([-grads_inlet], [self.weight_inlet]))\n",
    "                self.optimizer_wall.apply_gradients(zip([-grads_wall], [self.weight_wall]))\n",
    "                self.optimizer_initial.apply_gradients(zip([-grads_initial], [self.weight_initial]))\n",
    "                self.optimizer_fu.apply_gradients(zip([-grads_fu], [self.weight_fu]))\n",
    "\n",
    "  \n",
    "\n",
    "        #l-bfgs-b optimization\n",
    "        print(\"Starting L-BFGS training\")\n",
    "\n",
    "        loss_and_flat_grad = self.get_loss_and_flat_grad(t_f_tf , x_f_tf, y_f_tf,t_WALL_tf , x_WALL_tf, y_WALL_tf, p_WALL_tf ,\n",
    "                                                            t_INLET_tf , x_INLET_tf, y_INLET_tf, u_INLET_tf ,  v_INLET_tf ,\n",
    "                                                                t_OUTLET_tf , x_OUTLET_tf, y_OUTLET_tf, u_OUTLET_tf , v_OUTLET_tf,\n",
    "                                                                t_INITIAL_tf , x_INITIAL_tf, y_INITIAL_tf , u_INITIAL_tf, v_INITIAL_tf, p_INITIAL_tf)\n",
    "\n",
    "        lbfgs(loss_and_flat_grad, self.get_weights(), Struct(), maxIter=100, learningRate=0.8)\n",
    "\n",
    "\n",
    "        return #MSE_b1, MSE_f1,  weightu, weightf\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "            \n",
    "            \n",
    "\n",
    "    # L-BFGS implementation from https://github.com/pierremtb/PINNs-TF2.0\n",
    "    def get_loss_and_flat_grad ( self , t_c_tf , x_c_tf, y_c_tf, t_WALL_tf , x_WALL_tf, y_WALL_tf, p_WALL_tf , \n",
    "                                t_INLET_tf , x_INLET_tf, y_INLET_tf, u_INLET_tf ,  v_INLET_tf ,\n",
    "                                t_OUTLET_tf , x_OUTLET_tf, y_OUTLET_tf, u_OUTLET_tf , v_OUTLET_tf,\n",
    "                                  t_INITIAL_tf , x_INITIAL_tf, y_INITIAL_tf , u_INITIAL_tf, v_INITIAL_tf, p_INITIAL_tf):\n",
    "        \n",
    "        def loss_and_flat_grad(new_weight):\n",
    "            # print('hello' , new_weight)\n",
    "            with tf.GradientTape() as tape:\n",
    "                self.set_weights( new_weight)\n",
    "\n",
    "                loss_res, loss_wall , loss_inlet , loss_outlet , loss_initial , loss = self.loss_funct(t_c_tf , x_c_tf, y_c_tf, t_WALL_tf , x_WALL_tf, y_WALL_tf, p_WALL_tf ,\n",
    "                    t_INLET_tf , x_INLET_tf, y_INLET_tf, u_INLET_tf ,  v_INLET_tf ,t_OUTLET_tf , x_OUTLET_tf, y_OUTLET_tf, u_OUTLET_tf , v_OUTLET_tf,\n",
    "                    t_INITIAL_tf , x_INITIAL_tf, y_INITIAL_tf , u_INITIAL_tf, v_INITIAL_tf, p_INITIAL_tf)\n",
    "                \n",
    "            gradient = tape.gradient(loss, self.net_cuvwp.trainable_variables)\n",
    "            grad_flat = []\n",
    "            for grad in gradient:\n",
    "                grad_flat.append(tf.reshape(grad, [-1]))\n",
    "            grad_flat = tf.concat(grad_flat, 0)\n",
    "            # print(loss_value, grad_flat)\n",
    "            return  loss_res, loss_wall , loss_inlet , loss_outlet , loss_initial , loss, grad_flat\n",
    "\n",
    "        return loss_and_flat_grad\n",
    "\n",
    "\n",
    "    def predict(self , t , x , y):\n",
    "        X_star = tf.concat([t,x, y],1) # tf.convert_to_tensor(np.array([t , x, y]), dtype=tf.float32)\n",
    "\n",
    "        u, v, p = self.net_cuvwp(tf.concat([ X_star[:, 0:1] , X_star[:, 1:2], X_star[:, 2:3]], 1))\n",
    "\n",
    "        return u.numpy(), v.numpy(), p.numpy()\n",
    "\n",
    "############################################################\n",
    "   ############################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        text = datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode\n",
    "        dirname = os.path.abspath(os.path.join(\"checkpoints\", text))\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        shutil.copyfile('dwPINNS.ipynb', os.path.join(dirname, 'dwPINNS.ipynb'))\n",
    "\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    #  ############################################################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/noslipwall_02_Z0slice_2D/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/noslipwall_02_Z0slice_2D/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/noslipwall_02_Z0slice_2D/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/noslipwall_02_Z0slice_2D/M1.py', os.path.join(dirname, 'M1.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "    \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "\n",
    "    def plot_lambda(self):\n",
    "\n",
    "            fig_3 = plt.figure(3)\n",
    "            ax = fig_3.add_subplot(1, 1, 1)\n",
    "            ax.plot(self.adpative_constant_WALL_log, label='$\\lambda_{u_{WALL}}$')\n",
    "            ax.plot(self.adpative_constant_INITIAL_log, label='$\\lambda_{u_{INITIAL}}$')\n",
    "            ax.plot(self.adpative_constant_INLET_log, label='$\\lambda_{u_{INLET}}$')\n",
    "            ax.plot(self.adpative_constant_OUTLET_log, label='$\\lambda_{u_{OUTLET}}$')\n",
    "            ax.plot(self.adpative_constant_F_log, label='$\\lambda_{u_{Fun}}$')\n",
    "\n",
    "            ax.set_xlabel('iterations')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            text = 'lambda.png' \n",
    "            plt.savefig(os.path.join(self.dirname,text))\n",
    "            plt.close(\"all\")\n",
    "            #######################\n",
    "            #         self.loss_list = [\"Total_loss\", \"loss_OUTLET\" , \"loss_INLET\", \"loss_WALL\" , \"loss_res\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "\n",
    "def get_training_dataset(path , pINLET , pOUTLET , pWALL , pDomain , pInitial , dist):\n",
    "    \n",
    "    data = h5py.File(path , 'r')  # load dataset from matlab\n",
    "    WALL = np.transpose(data['noslipwall_02_Z0slice_2D_wall'], axes=range(len(data['noslipwall_02_Z0slice_2D_wall'].shape) - 1,-1, -1)).astype(np.float32)\n",
    "    \n",
    "    WALL = np.delete(WALL , np.where(WALL[:,0] == WALL[:,0].min())[0] , 0)  \n",
    "    # WALL = np.delete(WALL , np.where(WALL[:,1] <= 0.2)[0] , 0)  \n",
    "\n",
    "\n",
    "    domain = np.transpose(data['noslipwall_02_Z0slice_2D_innerdomain'], axes=range(len(data['noslipwall_02_Z0slice_2D_innerdomain'].shape) - 1,-1, -1)).astype(np.float32)\n",
    "    \n",
    "    domain = np.delete(domain , np.where(domain[:,0] == domain[:,0].min())[0] , 0)  \n",
    "    # domain = np.delete(domain , np.where(domain[:,1] <= 0.2)[0] , 0)  \n",
    "\n",
    "\n",
    "    # INLET = np.transpose(data['noslipwall_02_Z0slice_2D_inlet'],  axes=range(len(data['noslipwall_02_Z0slice_2D_inlet'].shape) - 1,-1, -1)).astype(np.float32)\n",
    "        \n",
    "    INLET = domain[np.where(domain[:,1] == domain[:,1].min())[0],:] #np.delete(INLET , np.where(INLET[:,0] == INLET[:,0].min())[0] , 0)  \n",
    "\n",
    "    OUTLET = np.transpose(data['noslipwall_02_Z0slice_2D_outlet'], axes=range(len(data['noslipwall_02_Z0slice_2D_outlet'].shape) - 1,-1, -1)).astype(np.float32)\n",
    "    \n",
    "    OUTLET = np.delete(OUTLET , np.where(OUTLET[:,0] == OUTLET[:,0].min())[0] , 0)  \n",
    "\n",
    "    total = INLET.shape[0] + OUTLET.shape[0] + domain.shape[0] +  WALL.shape[0] \n",
    "    \n",
    "    np.random.seed(1234)\n",
    "\n",
    "    # initial domain ux is 0.2 and other values are zero. FOr wall, all values are zero\n",
    "    \n",
    "    INITIALd = domain[np.where(domain[:,0] == domain[:,0].min())[0],:] # initial doamin corressponds to all values where t is zero\n",
    "\n",
    "    INITIALw = WALL[np.where(WALL[:,0] == WALL[:,0].min())[0],:] # initial doamin corressponds to all values where t is zero\n",
    "\n",
    "    INITIALi = INLET[np.where(INLET[:,0] == INLET[:,0].min())[0],:] # initial doamin corressponds to all values where t is zero\n",
    "\n",
    "    INITIALo = OUTLET[np.where(OUTLET[:,0] == OUTLET[:,0].min())[0],:] # initial doamin corressponds to all values where t is zero\n",
    "\n",
    "    #random selection of training data\n",
    "    INITIAL = np.concatenate([INITIALd , INITIALo],0)\n",
    "    \n",
    "    # domain = np.concatenate([domain , WALL , INLET , OUTLET],0)\n",
    "\n",
    "    #INLET = np.delete(INLET , idx_initi , 0)  \n",
    "    #OUTLET = np.delete(OUTLET , idx_inito  , 0)  \n",
    "    #domain = np.delete(domain , idx_initd , 0)  \n",
    "    #WALL = np.delete(WALL , idx_initw , 0)  \n",
    "    \n",
    "    if dist == \"Sobol\":\n",
    "        idxi = generate_sobol_sequence(0 , INLET.shape[0] ,  int(INLET.shape[0] * pINLET)) \n",
    "        INLET = INLET[idxi, :]\n",
    "        idxi = generate_sobol_sequence(0 , OUTLET.shape[0] ,  int(OUTLET.shape[0] * pOUTLET)) \n",
    "        OUTLET = OUTLET[idxi, :]\n",
    "        idxi = generate_sobol_sequence(0 , INITIAL.shape[0] ,  int(INITIAL.shape[0] * pInitial)) \n",
    "        INITIAL = INITIAL[idxi, :]\n",
    "        idxi = generate_sobol_sequence(0 , WALL.shape[0] ,  int(WALL.shape[0] * pWALL)) \n",
    "        WALL = WALL[idxi, :]\n",
    "        idxi = generate_sobol_sequence(0 , domain.shape[0] ,  int(domain.shape[0] * pDomain)) \n",
    "        domain = domain[idxi, :]\n",
    "    else:\n",
    "        idxi = np.random.choice(INLET.shape[0], int(INLET.shape[0] * pINLET), replace=False)\n",
    "        INLET = INLET[idxi, :]\n",
    "        idxi = np.random.choice(OUTLET.shape[0], int(OUTLET.shape[0] * pOUTLET), replace=False)\n",
    "        OUTLET = OUTLET[idxi, :]\n",
    "        idxi = np.random.choice(INITIAL.shape[0], int(INITIAL.shape[0] * pInitial), replace=False)\n",
    "        INITIAL = INITIAL[idxi, :]\n",
    "        idxi = np.random.choice(WALL.shape[0], int(WALL.shape[0] * pWALL), replace=False)\n",
    "        WALL = WALL[idxi, :]\n",
    "        idxi = np.random.choice(domain.shape[0], int(domain.shape[0] * pDomain), replace=False)\n",
    "        domain = domain[idxi, :]\n",
    "\n",
    "    #########################################\n",
    "    return [domain , INLET , OUTLET, WALL, INITIAL , total]\n",
    "################################################################\n",
    "\n",
    "\n",
    "\n",
    "def plot_result2(model ,path ):\n",
    "\n",
    "\n",
    "    ######################################\n",
    "    # model.save_NN()\n",
    "\n",
    "    # list_ = [ \"noslipwall_02_Z0slice_2D_wall\", \"noslipwall_02_Z0slice_2D_innerdomain\" ]\n",
    "    # N_data = [ 1600 , 20800 ]\n",
    "\n",
    "    # peotDic = dict((key,value) for key,value in zip(list_,N_data))\n",
    "\n",
    "    # tstep = [ 100 , 100 , ]\n",
    "    # tstepList = dict((key,value) for key,value in zip(list_,tstep))\n",
    "\n",
    "    # for key,value in peotDic.items():\n",
    "    #     # print(key, value)\n",
    "    #     test_data_inlet = get_testing_dataset(path   , part=key)\n",
    "    #     test_data_inlet = test_data_inlet[np.argsort(test_data_inlet[:, 0])]\n",
    "\n",
    "    #     [_ ,xf , _ , ufa , vfa  , pfa, u_pred , v_pred, p_pred]  = predict_result(model , test_data_inlet ,  value ,  tstep  ,  text=key , stm = False)\n",
    "    #     peotDic[key] =  error_over_time(tstepList[key] ,value , ufa , vfa , pfa , u_pred , v_pred , p_pred )\n",
    "    #     l1l2Erorr(key , u_pred ,v_pred , p_pred , ufa , vfa , pfa , model)\n",
    "\n",
    "    # draw_error_over_time(peotDic , model.dirname)\n",
    "\n",
    "    #     ## drawing velocity profile\n",
    "    # xValues = [5.000e-04 , 0.2275, 9.995e-01]\n",
    "    # # UVelocity_profile(model.dirname , 0.2 , xValues , xf , u_pred)\n",
    "\n",
    "    \n",
    "    tstep =100\n",
    "    N_data = 20800\n",
    "    #def predict_result(model , test_data , N_data , tstep  , text = 'all'  , stm = False):\n",
    "    part = 'noslipwall_02_Z0slice_2D_innerdomain'\n",
    "    test_data_innerdomain = get_testing_dataset(path    ,  part=part)\n",
    "    [tf , xf , yf , ufa , vfa  , pfa, u_pred , v_pred, p_pred]   = predict_result(model , test_data_innerdomain , N_data ,  tstep ,  text='domain' ,  stm = False)\n",
    "\n",
    "    \n",
    "\n",
    "    x = xf.reshape(tstep,N_data)[0,:]\n",
    "    y = yf.reshape(tstep,N_data)[0,:]\n",
    "    t = tf.reshape(tstep,N_data)[:,0].T\n",
    "    ufa = ufa.reshape(tstep,N_data)\n",
    "    vfa = vfa.reshape(tstep,N_data)\n",
    "    pfa = pfa.reshape(tstep,N_data)\n",
    "    u_pred = u_pred.reshape(tstep,N_data)\n",
    "    v_pred = v_pred.reshape(tstep,N_data)\n",
    "    p_pred = p_pred.reshape(tstep,N_data)\n",
    "    error_u =  np.abs(ufa - u_pred) # (u - u_pred) / u # np.abs(u - u_pred)\n",
    "    error_v =  np.abs(vfa - v_pred) # (v - v_pred) / v # np.abs(v - v_pred)\n",
    "    error_p =  np.abs(pfa - p_pred) # (p - p_pred) / p # np.abs(p - p_pred)\n",
    "\n",
    "    data = [u_pred, v_pred , p_pred , ufa, vfa , pfa , error_u ,  error_v ,error_p ]\n",
    "\n",
    "    plot_time_profile(model.dirname , x , y , t , ufa , u_pred , '$u$')\n",
    "    plot_time_profile(model.dirname , x , y , t , vfa , v_pred , '$v$')\n",
    "    plot_time_profile(model.dirname , x , y , t , pfa , p_pred , '$p$')\n",
    "\n",
    "    draw_contourf(t , x , y , data , 1.0 ,10 , model.dirname , 5 , fontsize=17.5 , labelsize=12.5 , axes_pad=1.2)\n",
    "\n",
    "    # model.weight_change_per_layer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50)           200         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           2550        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 50)           2550        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            153         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1)]          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 1)]          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 1)]          0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,453\n",
      "Trainable params: 5,453\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from generate_dataset import * \n",
    "# path = '/okyanus/users/afarea/dataDir/noslipwall_02_Z0slice_2D/noslipwall_02_Z0slice_2D.mat'\n",
    "path = '/media/afrah2/MyWork/files2023/dataset/noslipwall_02_Z0slice_2D.mat'\n",
    "\n",
    "modelPath = ''\n",
    "\n",
    "\n",
    "pINLET = 1\n",
    "pOUTLET= 0.3\n",
    "pWALL =  0.006\n",
    "pDomain = 0.0007\n",
    "pInitial = 0.07\n",
    "\n",
    "dist =  \"Sobol\"\n",
    "activFun = 'hard_swish'\n",
    "\n",
    "[collo , inlet , outlet, wall, initial , total] = get_training_dataset(path , pINLET , pOUTLET , pWALL , pDomain , pInitial , dist)\n",
    "\n",
    "XY_c = np.concatenate([collo , wall, inlet , outlet], 0) \n",
    "\n",
    "\n",
    "\n",
    "# # Define model\n",
    "mode = 'M1'\n",
    "\n",
    "input_dimension = 3\n",
    "output_dimension = 3\n",
    "n_hidden_layers = 3\n",
    "neurons = 50 # [3, 20, 20, 20, 20, 20, 20, 20, 3]\n",
    "\n",
    "starter_learning_rate = float(0.005)\n",
    "epochs = 5\n",
    "model_layers = [input_dimension] + n_hidden_layers*[neurons] + [output_dimension]\n",
    "\n",
    "batch_size= 300\n",
    "\n",
    "iterations = 40000\n",
    "\n",
    "method  = \"mini_batch\"\n",
    "\n",
    "\n",
    "###############################################################\n",
    "model = TwoD_BFS_Slip_PINN(XY_c , model_layers  , activFun , mode , starter_learning_rate , ExistModel = modelPath)\n",
    "\n",
    "\n",
    "# model.print(\"Using mode: \" , model.mode)\n",
    "# model.print(\"neural network: \" , model.layers )\n",
    "# model.print(\"collocation Training data size : \" ,collo.shape[0], \" (\" ,str(pDomain*100) , \"%)\")\n",
    "# model.print(\"Wall Training data size: \" ,wall.shape[0], \" (\" ,str(pWALL*100) , \"%)\")\n",
    "# model.print(\"Initial Training data size: \" ,initial.shape[0], \" (\" ,str(pInitial*100) , \"%)\")\n",
    "# model.print(\"INLET Training data size: \" ,inlet.shape[0], \" (\" ,str(pINLET*100) , \"%)\")\n",
    "# model.print(\"OUTLET Training data size: \" ,outlet.shape[0], \" (\" ,str(pOUTLET*100) , \"%)\")\n",
    "\n",
    "# model.print(\"Total training datset size: \" ,XY_c.shape[0], \" (\" ,str((XY_c.shape[0] / total)*100) , \"%)\")\n",
    "# model.print(\"Activation function: \" , activFun)\n",
    "# model.print(\"number of iterations: \" , iterations)\n",
    "\n",
    "# model.print(\"Method desciption : learning rates: Exponential decay  with initial value: \", starter_learning_rate)\n",
    "\n",
    "\n",
    "# model.print(\"File directory: \" , model.dirname)\n",
    "\n",
    "# #save_data_to_matlab(os.path.join(model.dirname,'slipwall_02_Z0slice_2D_training.mat') ,coll , INLET , OUTLET , WALL ,INITIAL )\n",
    "\n",
    "# plot_dataset( model.dirname , wall , inlet , outlet , collo , initial , dist)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Adam training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0| It: 0| Loss: 5.096e-01| physLoss: 3.564e-02 | lINITIAL: 1.740e-01 | wLoss: 8.506e-03 | inLoss: 5.214e-02 | outLoss: 2.393e-01 |Time: 0.00s | rTime: 2.997e-04h | LR: 5.000e-03\n",
      "Epoch: 100| It: 0| Loss: 5.009e-02| physLoss: 5.525e-03 | lINITIAL: 9.178e-03 | wLoss: 5.518e-03 | inLoss: 1.787e-02 | outLoss: 1.200e-02 |Time: 0.00s | rTime: 5.761e-02h | LR: 5.000e-03\n",
      "Epoch: 200| It: 0| Loss: 2.374e-02| physLoss: 2.502e-03 | lINITIAL: 3.115e-03 | wLoss: 1.319e-02 | inLoss: 3.795e-03 | outLoss: 1.140e-03 |Time: 0.00s | rTime: 8.032e-02h | LR: 5.000e-03\n",
      "Epoch: 300| It: 0| Loss: 2.396e-02| physLoss: 2.473e-03 | lINITIAL: 6.313e-03 | wLoss: 1.000e-02 | inLoss: 3.704e-03 | outLoss: 1.467e-03 |Time: 0.00s | rTime: 1.027e-01h | LR: 5.000e-03\n",
      "Epoch: 400| It: 0| Loss: 2.045e-02| physLoss: 3.087e-03 | lINITIAL: 3.291e-03 | wLoss: 1.003e-02 | inLoss: 3.977e-03 | outLoss: 6.859e-05 |Time: 0.00s | rTime: 1.275e-01h | LR: 5.000e-03\n",
      "Epoch: 500| It: 0| Loss: 1.926e-02| physLoss: 2.325e-03 | lINITIAL: 3.415e-03 | wLoss: 9.653e-03 | inLoss: 3.777e-03 | outLoss: 9.497e-05 |Time: 0.00s | rTime: 1.496e-01h | LR: 5.000e-03\n",
      "Epoch: 600| It: 0| Loss: 1.856e-02| physLoss: 2.602e-03 | lINITIAL: 2.908e-03 | wLoss: 1.026e-02 | inLoss: 2.762e-03 | outLoss: 2.391e-05 |Time: 0.00s | rTime: 1.725e-01h | LR: 5.000e-03\n",
      "Epoch: 700| It: 0| Loss: 1.828e-02| physLoss: 2.808e-03 | lINITIAL: 2.391e-03 | wLoss: 1.044e-02 | inLoss: 2.629e-03 | outLoss: 1.231e-05 |Time: 0.00s | rTime: 1.937e-01h | LR: 5.000e-03\n",
      "Epoch: 800| It: 0| Loss: 1.820e-02| physLoss: 2.817e-03 | lINITIAL: 2.368e-03 | wLoss: 1.037e-02 | inLoss: 2.644e-03 | outLoss: 1.106e-05 |Time: 0.00s | rTime: 2.167e-01h | LR: 5.000e-03\n",
      "Epoch: 900| It: 0| Loss: 1.825e-02| physLoss: 3.017e-03 | lINITIAL: 2.059e-03 | wLoss: 1.088e-02 | inLoss: 2.294e-03 | outLoss: 3.147e-06 |Time: 0.00s | rTime: 2.382e-01h | LR: 5.000e-03\n",
      "Epoch: 1000| It: 0| Loss: 1.838e-02| physLoss: 3.003e-03 | lINITIAL: 2.145e-03 | wLoss: 1.109e-02 | inLoss: 2.148e-03 | outLoss: 3.718e-06 |Time: 0.00s | rTime: 2.602e-01h | LR: 5.000e-03\n",
      "Epoch: 1100| It: 0| Loss: 1.852e-02| physLoss: 3.052e-03 | lINITIAL: 2.180e-03 | wLoss: 1.114e-02 | inLoss: 2.145e-03 | outLoss: 4.184e-06 |Time: 0.00s | rTime: 2.820e-01h | LR: 5.000e-03\n",
      "Epoch: 1200| It: 0| Loss: 1.860e-02| physLoss: 3.078e-03 | lINITIAL: 2.266e-03 | wLoss: 1.109e-02 | inLoss: 2.163e-03 | outLoss: 3.410e-06 |Time: 0.00s | rTime: 3.032e-01h | LR: 5.000e-03\n",
      "Epoch: 1300| It: 0| Loss: 1.856e-02| physLoss: 3.146e-03 | lINITIAL: 2.329e-03 | wLoss: 1.098e-02 | inLoss: 2.101e-03 | outLoss: 4.313e-06 |Time: 0.00s | rTime: 3.248e-01h | LR: 5.000e-03\n",
      "Epoch: 1400| It: 0| Loss: 1.826e-02| physLoss: 3.380e-03 | lINITIAL: 2.367e-03 | wLoss: 1.043e-02 | inLoss: 2.077e-03 | outLoss: 6.050e-06 |Time: 0.00s | rTime: 3.470e-01h | LR: 5.000e-03\n",
      "Epoch: 1500| It: 0| Loss: 1.795e-02| physLoss: 3.859e-03 | lINITIAL: 2.474e-03 | wLoss: 9.676e-03 | inLoss: 1.938e-03 | outLoss: 7.521e-06 |Time: 0.00s | rTime: 3.683e-01h | LR: 5.000e-03\n",
      "Epoch: 1600| It: 0| Loss: 1.782e-02| physLoss: 3.926e-03 | lINITIAL: 2.565e-03 | wLoss: 9.497e-03 | inLoss: 1.824e-03 | outLoss: 9.437e-06 |Time: 0.00s | rTime: 3.898e-01h | LR: 5.000e-03\n",
      "Epoch: 1700| It: 0| Loss: 1.717e-02| physLoss: 3.019e-03 | lINITIAL: 2.532e-03 | wLoss: 9.745e-03 | inLoss: 1.861e-03 | outLoss: 1.608e-05 |Time: 0.00s | rTime: 4.109e-01h | LR: 5.000e-03\n",
      "Epoch: 1800| It: 0| Loss: 1.588e-02| physLoss: 1.642e-03 | lINITIAL: 2.600e-03 | wLoss: 9.843e-03 | inLoss: 1.757e-03 | outLoss: 3.731e-05 |Time: 0.00s | rTime: 4.322e-01h | LR: 5.000e-03\n",
      "Epoch: 1900| It: 0| Loss: 1.521e-02| physLoss: 1.325e-03 | lINITIAL: 2.917e-03 | wLoss: 9.593e-03 | inLoss: 1.331e-03 | outLoss: 4.899e-05 |Time: 0.00s | rTime: 4.534e-01h | LR: 5.000e-03\n",
      "Epoch: 2000| It: 0| Loss: 1.489e-02| physLoss: 1.528e-03 | lINITIAL: 3.038e-03 | wLoss: 9.045e-03 | inLoss: 1.254e-03 | outLoss: 2.785e-05 |Time: 0.00s | rTime: 4.760e-01h | LR: 5.000e-03\n",
      "Epoch: 2100| It: 0| Loss: 1.465e-02| physLoss: 1.468e-03 | lINITIAL: 2.818e-03 | wLoss: 9.066e-03 | inLoss: 1.278e-03 | outLoss: 1.743e-05 |Time: 0.00s | rTime: 4.972e-01h | LR: 5.000e-03\n",
      "Epoch: 2200| It: 0| Loss: 1.441e-02| physLoss: 1.366e-03 | lINITIAL: 2.854e-03 | wLoss: 8.957e-03 | inLoss: 1.219e-03 | outLoss: 1.448e-05 |Time: 0.00s | rTime: 5.188e-01h | LR: 5.000e-03\n",
      "Epoch: 2300| It: 0| Loss: 1.418e-02| physLoss: 1.417e-03 | lINITIAL: 2.813e-03 | wLoss: 8.737e-03 | inLoss: 1.200e-03 | outLoss: 9.840e-06 |Time: 0.00s | rTime: 5.401e-01h | LR: 5.000e-03\n",
      "Epoch: 2400| It: 0| Loss: 1.385e-02| physLoss: 1.460e-03 | lINITIAL: 2.705e-03 | wLoss: 8.510e-03 | inLoss: 1.169e-03 | outLoss: 6.276e-06 |Time: 0.00s | rTime: 5.620e-01h | LR: 5.000e-03\n",
      "Epoch: 2500| It: 0| Loss: 1.338e-02| physLoss: 1.407e-03 | lINITIAL: 2.583e-03 | wLoss: 8.315e-03 | inLoss: 1.075e-03 | outLoss: 4.437e-06 |Time: 0.00s | rTime: 5.851e-01h | LR: 5.000e-03\n",
      "Epoch: 2600| It: 0| Loss: 1.279e-02| physLoss: 1.395e-03 | lINITIAL: 2.362e-03 | wLoss: 8.045e-03 | inLoss: 9.794e-04 | outLoss: 3.851e-06 |Time: 0.00s | rTime: 6.069e-01h | LR: 5.000e-03\n",
      "Epoch: 2700| It: 0| Loss: 1.201e-02| physLoss: 1.426e-03 | lINITIAL: 2.048e-03 | wLoss: 7.579e-03 | inLoss: 9.511e-04 | outLoss: 3.992e-06 |Time: 0.00s | rTime: 6.284e-01h | LR: 5.000e-03\n",
      "Epoch: 2800| It: 0| Loss: 1.073e-02| physLoss: 1.361e-03 | lINITIAL: 1.680e-03 | wLoss: 6.757e-03 | inLoss: 9.297e-04 | outLoss: 4.480e-06 |Time: 0.00s | rTime: 6.535e-01h | LR: 5.000e-03\n",
      "Epoch: 2900| It: 0| Loss: 9.892e-03| physLoss: 1.535e-03 | lINITIAL: 1.338e-03 | wLoss: 6.175e-03 | inLoss: 8.373e-04 | outLoss: 6.462e-06 |Time: 0.00s | rTime: 6.763e-01h | LR: 5.000e-03\n",
      "Epoch: 3000| It: 0| Loss: 9.442e-03| physLoss: 1.437e-03 | lINITIAL: 1.299e-03 | wLoss: 5.881e-03 | inLoss: 8.176e-04 | outLoss: 7.552e-06 |Time: 0.00s | rTime: 6.989e-01h | LR: 5.000e-03\n",
      "Epoch: 3100| It: 0| Loss: 9.017e-03| physLoss: 1.288e-03 | lINITIAL: 1.338e-03 | wLoss: 5.627e-03 | inLoss: 7.602e-04 | outLoss: 3.348e-06 |Time: 0.00s | rTime: 7.201e-01h | LR: 5.000e-03\n",
      "Epoch: 3200| It: 0| Loss: 8.626e-03| physLoss: 1.221e-03 | lINITIAL: 1.300e-03 | wLoss: 5.420e-03 | inLoss: 6.803e-04 | outLoss: 4.555e-06 |Time: 0.00s | rTime: 7.418e-01h | LR: 5.000e-03\n",
      "Epoch: 3300| It: 0| Loss: 8.224e-03| physLoss: 1.133e-03 | lINITIAL: 1.262e-03 | wLoss: 5.196e-03 | inLoss: 6.277e-04 | outLoss: 4.888e-06 |Time: 0.00s | rTime: 7.643e-01h | LR: 5.000e-03\n",
      "Epoch: 3400| It: 0| Loss: 7.840e-03| physLoss: 1.073e-03 | lINITIAL: 1.235e-03 | wLoss: 4.955e-03 | inLoss: 5.712e-04 | outLoss: 5.970e-06 |Time: 0.00s | rTime: 7.857e-01h | LR: 5.000e-03\n",
      "Epoch: 3500| It: 0| Loss: 7.495e-03| physLoss: 1.081e-03 | lINITIAL: 1.193e-03 | wLoss: 4.691e-03 | inLoss: 5.232e-04 | outLoss: 7.372e-06 |Time: 0.00s | rTime: 8.072e-01h | LR: 5.000e-03\n",
      "Epoch: 3600| It: 0| Loss: 6.949e-03| physLoss: 9.363e-04 | lINITIAL: 1.177e-03 | wLoss: 4.407e-03 | inLoss: 4.197e-04 | outLoss: 8.777e-06 |Time: 0.00s | rTime: 8.290e-01h | LR: 5.000e-03\n",
      "Epoch: 3700| It: 0| Loss: 6.552e-03| physLoss: 9.292e-04 | lINITIAL: 1.158e-03 | wLoss: 4.106e-03 | inLoss: 3.487e-04 | outLoss: 1.099e-05 |Time: 0.00s | rTime: 8.508e-01h | LR: 5.000e-03\n",
      "Epoch: 3800| It: 0| Loss: 6.195e-03| physLoss: 9.246e-04 | lINITIAL: 1.113e-03 | wLoss: 3.854e-03 | inLoss: 2.899e-04 | outLoss: 1.315e-05 |Time: 0.00s | rTime: 8.720e-01h | LR: 5.000e-03\n",
      "Epoch: 3900| It: 0| Loss: 5.614e-03| physLoss: 6.632e-04 | lINITIAL: 1.013e-03 | wLoss: 3.639e-03 | inLoss: 2.860e-04 | outLoss: 1.363e-05 |Time: 0.00s | rTime: 8.934e-01h | LR: 5.000e-03\n",
      "Epoch: 4000| It: 0| Loss: 5.343e-03| physLoss: 6.368e-04 | lINITIAL: 9.555e-04 | wLoss: 3.444e-03 | inLoss: 2.946e-04 | outLoss: 1.197e-05 |Time: 0.00s | rTime: 9.148e-01h | LR: 5.000e-03\n",
      "Epoch: 4100| It: 0| Loss: 5.058e-03| physLoss: 5.741e-04 | lINITIAL: 9.383e-04 | wLoss: 3.264e-03 | inLoss: 2.734e-04 | outLoss: 8.420e-06 |Time: 0.00s | rTime: 9.365e-01h | LR: 5.000e-03\n",
      "Epoch: 4200| It: 0| Loss: 4.692e-03| physLoss: 4.290e-04 | lINITIAL: 9.199e-04 | wLoss: 3.104e-03 | inLoss: 2.349e-04 | outLoss: 4.200e-06 |Time: 0.00s | rTime: 9.620e-01h | LR: 5.000e-03\n",
      "Epoch: 4300| It: 0| Loss: 4.445e-03| physLoss: 3.577e-04 | lINITIAL: 9.018e-04 | wLoss: 2.947e-03 | inLoss: 2.365e-04 | outLoss: 2.115e-06 |Time: 0.00s | rTime: 9.834e-01h | LR: 5.000e-03\n",
      "Epoch: 4400| It: 0| Loss: 4.431e-03| physLoss: 4.468e-04 | lINITIAL: 8.741e-04 | wLoss: 2.991e-03 | inLoss: 1.174e-04 | outLoss: 1.511e-06 |Time: 0.00s | rTime: 1.006e+00h | LR: 5.000e-03\n",
      "Epoch: 4500| It: 0| Loss: 4.138e-03| physLoss: 3.425e-04 | lINITIAL: 9.306e-04 | wLoss: 2.705e-03 | inLoss: 1.582e-04 | outLoss: 1.352e-06 |Time: 0.00s | rTime: 1.028e+00h | LR: 5.000e-03\n",
      "Epoch: 4600| It: 0| Loss: 3.838e-03| physLoss: 2.223e-04 | lINITIAL: 8.634e-04 | wLoss: 2.565e-03 | inLoss: 1.851e-04 | outLoss: 1.995e-06 |Time: 0.00s | rTime: 1.049e+00h | LR: 5.000e-03\n",
      "Epoch: 4700| It: 0| Loss: 3.698e-03| physLoss: 2.256e-04 | lINITIAL: 8.591e-04 | wLoss: 2.472e-03 | inLoss: 1.406e-04 | outLoss: 1.417e-06 |Time: 0.00s | rTime: 1.071e+00h | LR: 5.000e-03\n",
      "Epoch: 4800| It: 0| Loss: 3.570e-03| physLoss: 2.205e-04 | lINITIAL: 8.240e-04 | wLoss: 2.406e-03 | inLoss: 1.185e-04 | outLoss: 1.328e-06 |Time: 0.00s | rTime: 1.092e+00h | LR: 5.000e-03\n",
      "Epoch: 4900| It: 0| Loss: 3.528e-03| physLoss: 2.515e-04 | lINITIAL: 8.150e-04 | wLoss: 2.387e-03 | inLoss: 7.242e-05 | outLoss: 1.935e-06 |Time: 0.00s | rTime: 1.114e+00h | LR: 5.000e-03\n",
      "Epoch: 5000| It: 0| Loss: 3.559e-03| physLoss: 3.351e-04 | lINITIAL: 8.126e-04 | wLoss: 2.342e-03 | inLoss: 6.754e-05 | outLoss: 1.374e-06 |Time: 0.00s | rTime: 1.136e+00h | LR: 5.000e-03\n",
      "Epoch: 5100| It: 0| Loss: 3.421e-03| physLoss: 2.419e-04 | lINITIAL: 7.762e-04 | wLoss: 2.351e-03 | inLoss: 4.996e-05 | outLoss: 2.414e-06 |Time: 0.00s | rTime: 1.157e+00h | LR: 5.000e-03\n",
      "Epoch: 5200| It: 0| Loss: 3.373e-03| physLoss: 2.637e-04 | lINITIAL: 7.628e-04 | wLoss: 2.267e-03 | inLoss: 7.813e-05 | outLoss: 1.060e-06 |Time: 0.00s | rTime: 1.180e+00h | LR: 5.000e-03\n",
      "Epoch: 5300| It: 0| Loss: 3.352e-03| physLoss: 2.645e-04 | lINITIAL: 7.619e-04 | wLoss: 2.209e-03 | inLoss: 1.153e-04 | outLoss: 9.542e-07 |Time: 0.00s | rTime: 1.202e+00h | LR: 5.000e-03\n",
      "Epoch: 5400| It: 0| Loss: 3.352e-03| physLoss: 2.677e-04 | lINITIAL: 7.607e-04 | wLoss: 2.276e-03 | inLoss: 4.539e-05 | outLoss: 2.169e-06 |Time: 0.00s | rTime: 1.223e+00h | LR: 5.000e-03\n",
      "Epoch: 5500| It: 0| Loss: 3.361e-03| physLoss: 3.006e-04 | lINITIAL: 7.664e-04 | wLoss: 2.255e-03 | inLoss: 3.703e-05 | outLoss: 1.543e-06 |Time: 0.00s | rTime: 1.245e+00h | LR: 5.000e-03\n",
      "Epoch: 5600| It: 0| Loss: 3.206e-03| physLoss: 1.829e-04 | lINITIAL: 7.597e-04 | wLoss: 2.205e-03 | inLoss: 5.692e-05 | outLoss: 9.923e-07 |Time: 0.00s | rTime: 1.267e+00h | LR: 5.000e-03\n",
      "Epoch: 5700| It: 0| Loss: 3.329e-03| physLoss: 3.003e-04 | lINITIAL: 7.510e-04 | wLoss: 2.195e-03 | inLoss: 8.120e-05 | outLoss: 8.855e-07 |Time: 0.00s | rTime: 1.289e+00h | LR: 5.000e-03\n",
      "Epoch: 5800| It: 0| Loss: 3.296e-03| physLoss: 2.750e-04 | lINITIAL: 7.525e-04 | wLoss: 2.205e-03 | inLoss: 6.262e-05 | outLoss: 1.194e-06 |Time: 0.00s | rTime: 1.310e+00h | LR: 5.000e-03\n",
      "Epoch: 5900| It: 0| Loss: 3.154e-03| physLoss: 1.697e-04 | lINITIAL: 7.740e-04 | wLoss: 2.125e-03 | inLoss: 8.454e-05 | outLoss: 4.808e-07 |Time: 0.00s | rTime: 1.332e+00h | LR: 5.000e-03\n",
      "Epoch: 6000| It: 0| Loss: 3.160e-03| physLoss: 1.973e-04 | lINITIAL: 7.660e-04 | wLoss: 2.160e-03 | inLoss: 3.603e-05 | outLoss: 6.180e-07 |Time: 0.00s | rTime: 1.353e+00h | LR: 5.000e-03\n",
      "Epoch: 6100| It: 0| Loss: 3.061e-03| physLoss: 1.152e-04 | lINITIAL: 7.637e-04 | wLoss: 2.128e-03 | inLoss: 5.337e-05 | outLoss: 4.761e-07 |Time: 0.00s | rTime: 1.375e+00h | LR: 5.000e-03\n",
      "Epoch: 6200| It: 0| Loss: 3.116e-03| physLoss: 1.800e-04 | lINITIAL: 7.631e-04 | wLoss: 2.109e-03 | inLoss: 6.320e-05 | outLoss: 4.379e-07 |Time: 0.00s | rTime: 1.396e+00h | LR: 5.000e-03\n",
      "Epoch: 6300| It: 0| Loss: 3.206e-03| physLoss: 2.683e-04 | lINITIAL: 7.599e-04 | wLoss: 2.107e-03 | inLoss: 7.017e-05 | outLoss: 6.547e-07 |Time: 0.00s | rTime: 1.417e+00h | LR: 5.000e-03\n",
      "Epoch: 6400| It: 0| Loss: 3.112e-03| physLoss: 1.902e-04 | lINITIAL: 7.692e-04 | wLoss: 2.125e-03 | inLoss: 2.662e-05 | outLoss: 9.482e-07 |Time: 0.00s | rTime: 1.439e+00h | LR: 5.000e-03\n",
      "Epoch: 6500| It: 0| Loss: 3.070e-03| physLoss: 1.569e-04 | lINITIAL: 7.596e-04 | wLoss: 2.093e-03 | inLoss: 6.005e-05 | outLoss: 3.137e-07 |Time: 0.00s | rTime: 1.460e+00h | LR: 5.000e-03\n",
      "Epoch: 6600| It: 0| Loss: 3.160e-03| physLoss: 2.158e-04 | lINITIAL: 7.791e-04 | wLoss: 2.018e-03 | inLoss: 1.439e-04 | outLoss: 2.677e-06 |Time: 0.00s | rTime: 1.483e+00h | LR: 5.000e-03\n",
      "Epoch: 6700| It: 0| Loss: 3.183e-03| physLoss: 2.381e-04 | lINITIAL: 7.698e-04 | wLoss: 2.150e-03 | inLoss: 2.140e-05 | outLoss: 3.031e-06 |Time: 0.00s | rTime: 1.505e+00h | LR: 5.000e-03\n",
      "Epoch: 6800| It: 0| Loss: 3.070e-03| physLoss: 1.646e-04 | lINITIAL: 7.611e-04 | wLoss: 2.089e-03 | inLoss: 5.547e-05 | outLoss: 2.834e-07 |Time: 0.00s | rTime: 1.533e+00h | LR: 5.000e-03\n",
      "Epoch: 6900| It: 0| Loss: 3.069e-03| physLoss: 1.603e-04 | lINITIAL: 7.777e-04 | wLoss: 2.091e-03 | inLoss: 3.915e-05 | outLoss: 7.756e-07 |Time: 0.00s | rTime: 1.554e+00h | LR: 5.000e-03\n",
      "Epoch: 7000| It: 0| Loss: 3.153e-03| physLoss: 2.498e-04 | lINITIAL: 8.017e-04 | wLoss: 2.058e-03 | inLoss: 4.270e-05 | outLoss: 6.744e-07 |Time: 0.00s | rTime: 1.577e+00h | LR: 5.000e-03\n",
      "Epoch: 7100| It: 0| Loss: 3.115e-03| physLoss: 1.804e-04 | lINITIAL: 7.940e-04 | wLoss: 2.010e-03 | inLoss: 1.271e-04 | outLoss: 3.414e-06 |Time: 0.00s | rTime: 1.600e+00h | LR: 5.000e-03\n",
      "Epoch: 7200| It: 0| Loss: 3.101e-03| physLoss: 2.060e-04 | lINITIAL: 7.799e-04 | wLoss: 2.068e-03 | inLoss: 4.642e-05 | outLoss: 5.058e-07 |Time: 0.00s | rTime: 1.622e+00h | LR: 5.000e-03\n",
      "Epoch: 7300| It: 0| Loss: 3.043e-03| physLoss: 1.402e-04 | lINITIAL: 7.727e-04 | wLoss: 2.092e-03 | inLoss: 3.664e-05 | outLoss: 1.043e-06 |Time: 0.00s | rTime: 1.645e+00h | LR: 5.000e-03\n",
      "Epoch: 7400| It: 0| Loss: 3.116e-03| physLoss: 1.763e-04 | lINITIAL: 8.385e-04 | wLoss: 2.024e-03 | inLoss: 7.491e-05 | outLoss: 2.178e-06 |Time: 0.00s | rTime: 1.667e+00h | LR: 5.000e-03\n",
      "Epoch: 7500| It: 0| Loss: 3.082e-03| physLoss: 1.749e-04 | lINITIAL: 7.882e-04 | wLoss: 2.059e-03 | inLoss: 5.939e-05 | outLoss: 3.703e-07 |Time: 0.00s | rTime: 1.689e+00h | LR: 5.000e-03\n",
      "Epoch: 7600| It: 0| Loss: 3.022e-03| physLoss: 1.213e-04 | lINITIAL: 8.045e-04 | wLoss: 2.046e-03 | inLoss: 4.939e-05 | outLoss: 5.249e-07 |Time: 0.00s | rTime: 1.711e+00h | LR: 5.000e-03\n",
      "Epoch: 7700| It: 0| Loss: 3.170e-03| physLoss: 2.481e-04 | lINITIAL: 7.704e-04 | wLoss: 2.059e-03 | inLoss: 9.114e-05 | outLoss: 9.662e-07 |Time: 0.00s | rTime: 1.733e+00h | LR: 5.000e-03\n",
      "Epoch: 7800| It: 0| Loss: 3.184e-03| physLoss: 2.385e-04 | lINITIAL: 7.766e-04 | wLoss: 2.140e-03 | inLoss: 2.436e-05 | outLoss: 4.430e-06 |Time: 0.00s | rTime: 1.755e+00h | LR: 5.000e-03\n",
      "Epoch: 7900| It: 0| Loss: 3.079e-03| physLoss: 1.663e-04 | lINITIAL: 7.697e-04 | wLoss: 2.087e-03 | inLoss: 5.494e-05 | outLoss: 3.300e-07 |Time: 0.00s | rTime: 1.777e+00h | LR: 5.000e-03\n",
      "Epoch: 8000| It: 0| Loss: 3.152e-03| physLoss: 2.213e-04 | lINITIAL: 8.120e-04 | wLoss: 2.059e-03 | inLoss: 5.940e-05 | outLoss: 4.952e-07 |Time: 0.00s | rTime: 1.798e+00h | LR: 5.000e-03\n",
      "Epoch: 8100| It: 0| Loss: 3.113e-03| physLoss: 1.984e-04 | lINITIAL: 7.706e-04 | wLoss: 2.115e-03 | inLoss: 2.606e-05 | outLoss: 2.507e-06 |Time: 0.00s | rTime: 1.820e+00h | LR: 5.000e-03\n",
      "Epoch: 8200| It: 0| Loss: 3.105e-03| physLoss: 1.737e-04 | lINITIAL: 8.455e-04 | wLoss: 2.025e-03 | inLoss: 5.855e-05 | outLoss: 2.114e-06 |Time: 0.00s | rTime: 1.842e+00h | LR: 5.000e-03\n",
      "Epoch: 8300| It: 0| Loss: 3.212e-03| physLoss: 2.951e-04 | lINITIAL: 8.422e-04 | wLoss: 2.034e-03 | inLoss: 3.844e-05 | outLoss: 1.952e-06 |Time: 0.00s | rTime: 1.863e+00h | LR: 5.000e-03\n",
      "Epoch: 8400| It: 0| Loss: 2.969e-03| physLoss: 8.079e-05 | lINITIAL: 7.743e-04 | wLoss: 2.073e-03 | inLoss: 4.096e-05 | outLoss: 2.741e-07 |Time: 0.00s | rTime: 1.885e+00h | LR: 5.000e-03\n",
      "Epoch: 8500| It: 0| Loss: 3.046e-03| physLoss: 1.535e-04 | lINITIAL: 7.776e-04 | wLoss: 2.086e-03 | inLoss: 2.848e-05 | outLoss: 9.221e-07 |Time: 0.00s | rTime: 1.909e+00h | LR: 5.000e-03\n",
      "Epoch: 8600| It: 0| Loss: 3.169e-03| physLoss: 2.434e-04 | lINITIAL: 7.822e-04 | wLoss: 2.118e-03 | inLoss: 2.349e-05 | outLoss: 2.464e-06 |Time: 0.00s | rTime: 1.930e+00h | LR: 5.000e-03\n",
      "Epoch: 8700| It: 0| Loss: 3.116e-03| physLoss: 2.117e-04 | lINITIAL: 7.883e-04 | wLoss: 2.033e-03 | inLoss: 8.263e-05 | outLoss: 8.391e-07 |Time: 0.00s | rTime: 1.952e+00h | LR: 5.000e-03\n",
      "Epoch: 8800| It: 0| Loss: 3.007e-03| physLoss: 1.028e-04 | lINITIAL: 8.235e-04 | wLoss: 2.029e-03 | inLoss: 5.115e-05 | outLoss: 7.520e-07 |Time: 0.00s | rTime: 1.975e+00h | LR: 5.000e-03\n",
      "Epoch: 8900| It: 0| Loss: 3.576e-03| physLoss: 5.114e-04 | lINITIAL: 8.074e-04 | wLoss: 2.161e-03 | inLoss: 8.105e-05 | outLoss: 1.470e-05 |Time: 0.00s | rTime: 1.996e+00h | LR: 5.000e-03\n",
      "Epoch: 9000| It: 0| Loss: 3.260e-03| physLoss: 3.033e-04 | lINITIAL: 7.840e-04 | wLoss: 2.111e-03 | inLoss: 6.056e-05 | outLoss: 1.192e-06 |Time: 0.00s | rTime: 2.018e+00h | LR: 5.000e-03\n",
      "Epoch: 9100| It: 0| Loss: 3.059e-03| physLoss: 1.076e-04 | lINITIAL: 7.691e-04 | wLoss: 2.124e-03 | inLoss: 5.433e-05 | outLoss: 4.464e-06 |Time: 0.00s | rTime: 2.041e+00h | LR: 5.000e-03\n",
      "Epoch: 9200| It: 0| Loss: 3.015e-03| physLoss: 9.559e-05 | lINITIAL: 7.785e-04 | wLoss: 2.106e-03 | inLoss: 3.467e-05 | outLoss: 5.521e-07 |Time: 0.00s | rTime: 2.064e+00h | LR: 5.000e-03\n",
      "Epoch: 9300| It: 0| Loss: 2.975e-03| physLoss: 7.082e-05 | lINITIAL: 7.882e-04 | wLoss: 2.088e-03 | inLoss: 2.722e-05 | outLoss: 2.513e-07 |Time: 0.00s | rTime: 2.086e+00h | LR: 5.000e-03\n",
      "Epoch: 9400| It: 0| Loss: 2.972e-03| physLoss: 8.292e-05 | lINITIAL: 7.844e-04 | wLoss: 2.079e-03 | inLoss: 2.584e-05 | outLoss: 2.506e-07 |Time: 0.00s | rTime: 2.108e+00h | LR: 5.000e-03\n",
      "Epoch: 9500| It: 0| Loss: 3.222e-03| physLoss: 3.120e-04 | lINITIAL: 7.825e-04 | wLoss: 2.083e-03 | inLoss: 4.241e-05 | outLoss: 2.072e-06 |Time: 0.00s | rTime: 2.130e+00h | LR: 5.000e-03\n",
      "Epoch: 9600| It: 0| Loss: 2.986e-03| physLoss: 8.050e-05 | lINITIAL: 7.791e-04 | wLoss: 2.099e-03 | inLoss: 2.763e-05 | outLoss: 6.661e-07 |Time: 0.00s | rTime: 2.153e+00h | LR: 5.000e-03\n",
      "Epoch: 9700| It: 0| Loss: 3.385e-03| physLoss: 3.816e-04 | lINITIAL: 8.086e-04 | wLoss: 2.121e-03 | inLoss: 6.001e-05 | outLoss: 1.442e-05 |Time: 0.00s | rTime: 2.175e+00h | LR: 5.000e-03\n",
      "Epoch: 9800| It: 0| Loss: 2.982e-03| physLoss: 6.004e-05 | lINITIAL: 7.885e-04 | wLoss: 2.108e-03 | inLoss: 2.543e-05 | outLoss: 2.011e-07 |Time: 0.00s | rTime: 2.197e+00h | LR: 5.000e-03\n",
      "Epoch: 9900| It: 0| Loss: 2.994e-03| physLoss: 8.337e-05 | lINITIAL: 7.920e-04 | wLoss: 2.079e-03 | inLoss: 3.877e-05 | outLoss: 1.316e-06 |Time: 0.00s | rTime: 2.219e+00h | LR: 5.000e-03\n",
      "Epoch: 10000| It: 0| Loss: 2.960e-03| physLoss: 7.240e-05 | lINITIAL: 8.016e-04 | wLoss: 2.058e-03 | inLoss: 2.833e-05 | outLoss: 4.196e-07 |Time: 0.00s | rTime: 2.241e+00h | LR: 5.000e-03\n",
      "Epoch: 10100| It: 0| Loss: 4.770e-03| physLoss: 1.535e-03 | lINITIAL: 1.094e-03 | wLoss: 2.118e-03 | inLoss: 2.285e-05 | outLoss: 9.266e-07 |Time: 0.00s | rTime: 2.262e+00h | LR: 5.000e-03\n",
      "Epoch: 10200| It: 0| Loss: 3.000e-03| physLoss: 8.026e-05 | lINITIAL: 8.483e-04 | wLoss: 1.991e-03 | inLoss: 7.916e-05 | outLoss: 1.599e-06 |Time: 0.00s | rTime: 2.284e+00h | LR: 5.000e-03\n",
      "Epoch: 10300| It: 0| Loss: 2.948e-03| physLoss: 6.263e-05 | lINITIAL: 8.120e-04 | wLoss: 2.043e-03 | inLoss: 2.857e-05 | outLoss: 1.605e-06 |Time: 0.00s | rTime: 2.306e+00h | LR: 5.000e-03\n",
      "Epoch: 10400| It: 0| Loss: 2.948e-03| physLoss: 6.461e-05 | lINITIAL: 8.305e-04 | wLoss: 1.997e-03 | inLoss: 5.541e-05 | outLoss: 4.261e-07 |Time: 0.00s | rTime: 2.328e+00h | LR: 5.000e-03\n",
      "Epoch: 10500| It: 0| Loss: 5.207e-03| physLoss: 1.994e-03 | lINITIAL: 8.397e-04 | wLoss: 2.226e-03 | inLoss: 1.343e-04 | outLoss: 1.210e-05 |Time: 0.00s | rTime: 2.350e+00h | LR: 5.000e-03\n",
      "Epoch: 10600| It: 0| Loss: 3.471e-03| physLoss: 4.231e-04 | lINITIAL: 8.223e-04 | wLoss: 2.136e-03 | inLoss: 6.907e-05 | outLoss: 2.121e-05 |Time: 0.00s | rTime: 2.372e+00h | LR: 5.000e-03\n",
      "Epoch: 10700| It: 0| Loss: 3.169e-03| physLoss: 1.597e-04 | lINITIAL: 7.951e-04 | wLoss: 2.184e-03 | inLoss: 1.768e-05 | outLoss: 1.243e-05 |Time: 0.00s | rTime: 2.394e+00h | LR: 5.000e-03\n",
      "Epoch: 10800| It: 0| Loss: 2.977e-03| physLoss: 5.739e-05 | lINITIAL: 7.973e-04 | wLoss: 2.106e-03 | inLoss: 1.537e-05 | outLoss: 5.007e-07 |Time: 0.00s | rTime: 2.417e+00h | LR: 5.000e-03\n",
      "Epoch: 10900| It: 0| Loss: 2.952e-03| physLoss: 5.925e-05 | lINITIAL: 8.199e-04 | wLoss: 2.047e-03 | inLoss: 2.405e-05 | outLoss: 1.070e-06 |Time: 0.00s | rTime: 2.442e+00h | LR: 5.000e-03\n",
      "Epoch: 11000| It: 0| Loss: 2.921e-03| physLoss: 4.803e-05 | lINITIAL: 8.035e-04 | wLoss: 2.053e-03 | inLoss: 1.596e-05 | outLoss: 2.433e-07 |Time: 0.00s | rTime: 2.464e+00h | LR: 5.000e-03\n",
      "Epoch: 11100| It: 0| Loss: 3.051e-03| physLoss: 1.659e-04 | lINITIAL: 8.045e-04 | wLoss: 2.046e-03 | inLoss: 3.456e-05 | outLoss: 3.353e-07 |Time: 0.00s | rTime: 2.487e+00h | LR: 5.000e-03\n",
      "Epoch: 11200| It: 0| Loss: 3.541e-03| physLoss: 4.814e-04 | lINITIAL: 8.424e-04 | wLoss: 2.207e-03 | inLoss: 9.791e-06 | outLoss: 3.431e-07 |Time: 0.00s | rTime: 2.510e+00h | LR: 5.000e-03\n",
      "Epoch: 11300| It: 0| Loss: 3.137e-03| physLoss: 1.260e-04 | lINITIAL: 7.998e-04 | wLoss: 2.205e-03 | inLoss: 5.235e-06 | outLoss: 1.301e-06 |Time: 0.00s | rTime: 2.532e+00h | LR: 5.000e-03\n",
      "Epoch: 11400| It: 0| Loss: 2.933e-03| physLoss: 5.506e-05 | lINITIAL: 8.026e-04 | wLoss: 2.063e-03 | inLoss: 1.122e-05 | outLoss: 1.008e-06 |Time: 0.00s | rTime: 2.554e+00h | LR: 5.000e-03\n",
      "Epoch: 11500| It: 0| Loss: 3.710e-03| physLoss: 7.892e-04 | lINITIAL: 8.050e-04 | wLoss: 2.024e-03 | inLoss: 9.096e-05 | outLoss: 3.315e-07 |Time: 0.00s | rTime: 2.577e+00h | LR: 5.000e-03\n",
      "Epoch: 11600| It: 0| Loss: 3.078e-03| physLoss: 1.935e-04 | lINITIAL: 8.525e-04 | wLoss: 2.001e-03 | inLoss: 2.551e-05 | outLoss: 5.970e-06 |Time: 0.00s | rTime: 2.599e+00h | LR: 5.000e-03\n",
      "Epoch: 11700| It: 0| Loss: 3.086e-03| physLoss: 1.529e-04 | lINITIAL: 8.033e-04 | wLoss: 2.123e-03 | inLoss: 6.037e-06 | outLoss: 2.729e-07 |Time: 0.00s | rTime: 2.621e+00h | LR: 5.000e-03\n",
      "Epoch: 11800| It: 0| Loss: 3.000e-03| physLoss: 1.032e-04 | lINITIAL: 8.065e-04 | wLoss: 2.085e-03 | inLoss: 4.918e-06 | outLoss: 3.532e-07 |Time: 0.00s | rTime: 2.643e+00h | LR: 5.000e-03\n",
      "Epoch: 11900| It: 0| Loss: 2.949e-03| physLoss: 8.862e-05 | lINITIAL: 8.477e-04 | wLoss: 1.961e-03 | inLoss: 5.096e-05 | outLoss: 5.023e-07 |Time: 0.00s | rTime: 2.664e+00h | LR: 5.000e-03\n",
      "Epoch: 12000| It: 0| Loss: 3.135e-03| physLoss: 2.791e-04 | lINITIAL: 8.281e-04 | wLoss: 2.014e-03 | inLoss: 1.286e-05 | outLoss: 4.249e-07 |Time: 0.00s | rTime: 2.686e+00h | LR: 5.000e-03\n",
      "Epoch: 12100| It: 0| Loss: 3.096e-03| physLoss: 2.488e-04 | lINITIAL: 8.191e-04 | wLoss: 1.988e-03 | inLoss: 3.983e-05 | outLoss: 4.468e-07 |Time: 0.00s | rTime: 2.709e+00h | LR: 5.000e-03\n",
      "Epoch: 12200| It: 0| Loss: 2.883e-03| physLoss: 4.819e-05 | lINITIAL: 8.197e-04 | wLoss: 2.004e-03 | inLoss: 1.013e-05 | outLoss: 4.741e-07 |Time: 0.00s | rTime: 2.730e+00h | LR: 5.000e-03\n",
      "Epoch: 12300| It: 0| Loss: 2.925e-03| physLoss: 1.012e-04 | lINITIAL: 8.206e-04 | wLoss: 1.973e-03 | inLoss: 2.994e-05 | outLoss: 4.879e-07 |Time: 0.00s | rTime: 2.752e+00h | LR: 5.000e-03\n",
      "Epoch: 12400| It: 0| Loss: 3.154e-03| physLoss: 2.965e-04 | lINITIAL: 8.199e-04 | wLoss: 2.009e-03 | inLoss: 2.900e-05 | outLoss: 2.607e-07 |Time: 0.00s | rTime: 2.773e+00h | LR: 5.000e-03\n",
      "Epoch: 12500| It: 0| Loss: 3.570e-03| physLoss: 3.432e-04 | lINITIAL: 8.250e-04 | wLoss: 2.342e-03 | inLoss: 4.995e-05 | outLoss: 1.002e-05 |Time: 0.00s | rTime: 2.796e+00h | LR: 5.000e-03\n",
      "Epoch: 12600| It: 0| Loss: 3.274e-03| physLoss: 2.977e-04 | lINITIAL: 8.186e-04 | wLoss: 2.059e-03 | inLoss: 9.749e-05 | outLoss: 6.409e-07 |Time: 0.00s | rTime: 2.817e+00h | LR: 5.000e-03\n",
      "Epoch: 12700| It: 0| Loss: 3.024e-03| physLoss: 1.016e-04 | lINITIAL: 8.139e-04 | wLoss: 2.075e-03 | inLoss: 2.495e-05 | outLoss: 8.166e-06 |Time: 0.00s | rTime: 2.839e+00h | LR: 5.000e-03\n",
      "Epoch: 12800| It: 0| Loss: 2.908e-03| physLoss: 4.972e-05 | lINITIAL: 8.363e-04 | wLoss: 1.997e-03 | inLoss: 2.349e-05 | outLoss: 1.599e-06 |Time: 0.00s | rTime: 2.860e+00h | LR: 5.000e-03\n",
      "Epoch: 12900| It: 0| Loss: 2.863e-03| physLoss: 3.849e-05 | lINITIAL: 8.366e-04 | wLoss: 1.962e-03 | inLoss: 2.558e-05 | outLoss: 1.870e-07 |Time: 0.00s | rTime: 2.881e+00h | LR: 5.000e-03\n",
      "Epoch: 13000| It: 0| Loss: 2.835e-03| physLoss: 3.595e-05 | lINITIAL: 8.424e-04 | wLoss: 1.939e-03 | inLoss: 1.778e-05 | outLoss: 3.775e-07 |Time: 0.00s | rTime: 2.904e+00h | LR: 5.000e-03\n",
      "Epoch: 13100| It: 0| Loss: 3.518e-03| physLoss: 6.467e-04 | lINITIAL: 8.329e-04 | wLoss: 1.963e-03 | inLoss: 7.455e-05 | outLoss: 2.717e-07 |Time: 0.00s | rTime: 2.925e+00h | LR: 5.000e-03\n",
      "Epoch: 13200| It: 0| Loss: 3.036e-03| physLoss: 2.059e-04 | lINITIAL: 8.442e-04 | wLoss: 1.926e-03 | inLoss: 5.918e-05 | outLoss: 2.486e-07 |Time: 0.00s | rTime: 2.947e+00h | LR: 5.000e-03\n",
      "Epoch: 13300| It: 0| Loss: 2.895e-03| physLoss: 9.621e-05 | lINITIAL: 8.586e-04 | wLoss: 1.926e-03 | inLoss: 1.343e-05 | outLoss: 4.644e-07 |Time: 0.00s | rTime: 2.968e+00h | LR: 5.000e-03\n",
      "Epoch: 13400| It: 0| Loss: 2.889e-03| physLoss: 9.913e-05 | lINITIAL: 8.362e-04 | wLoss: 1.910e-03 | inLoss: 4.294e-05 | outLoss: 2.018e-07 |Time: 0.00s | rTime: 2.990e+00h | LR: 5.000e-03\n",
      "Epoch: 13500| It: 0| Loss: 2.867e-03| physLoss: 9.942e-05 | lINITIAL: 8.528e-04 | wLoss: 1.891e-03 | inLoss: 2.407e-05 | outLoss: 2.290e-07 |Time: 0.00s | rTime: 3.011e+00h | LR: 5.000e-03\n",
      "Epoch: 13600| It: 0| Loss: 2.896e-03| physLoss: 1.276e-04 | lINITIAL: 8.532e-04 | wLoss: 1.889e-03 | inLoss: 2.550e-05 | outLoss: 5.870e-07 |Time: 0.00s | rTime: 3.033e+00h | LR: 5.000e-03\n",
      "Epoch: 13700| It: 0| Loss: 2.810e-03| physLoss: 6.275e-05 | lINITIAL: 8.508e-04 | wLoss: 1.863e-03 | inLoss: 3.304e-05 | outLoss: 5.080e-07 |Time: 0.00s | rTime: 3.054e+00h | LR: 5.000e-03\n",
      "Epoch: 13800| It: 0| Loss: 3.052e-03| physLoss: 3.050e-04 | lINITIAL: 8.581e-04 | wLoss: 1.855e-03 | inLoss: 3.354e-05 | outLoss: 3.120e-07 |Time: 0.00s | rTime: 3.075e+00h | LR: 5.000e-03\n",
      "Epoch: 13900| It: 0| Loss: 2.835e-03| physLoss: 1.080e-04 | lINITIAL: 8.468e-04 | wLoss: 1.868e-03 | inLoss: 1.071e-05 | outLoss: 1.379e-06 |Time: 0.00s | rTime: 3.099e+00h | LR: 5.000e-03\n",
      "Epoch: 14000| It: 0| Loss: 3.519e-03| physLoss: 7.567e-04 | lINITIAL: 8.668e-04 | wLoss: 1.860e-03 | inLoss: 3.588e-05 | outLoss: 4.747e-07 |Time: 0.00s | rTime: 3.120e+00h | LR: 5.000e-03\n",
      "Epoch: 14100| It: 0| Loss: 2.891e-03| physLoss: 1.375e-04 | lINITIAL: 9.360e-04 | wLoss: 1.773e-03 | inLoss: 4.340e-05 | outLoss: 4.204e-07 |Time: 0.00s | rTime: 3.142e+00h | LR: 5.000e-03\n",
      "Epoch: 14200| It: 0| Loss: 2.997e-03| physLoss: 3.003e-04 | lINITIAL: 8.681e-04 | wLoss: 1.809e-03 | inLoss: 1.919e-05 | outLoss: 3.109e-07 |Time: 0.00s | rTime: 3.163e+00h | LR: 5.000e-03\n",
      "Epoch: 14300| It: 0| Loss: 2.731e-03| physLoss: 5.585e-05 | lINITIAL: 8.703e-04 | wLoss: 1.767e-03 | inLoss: 3.739e-05 | outLoss: 3.306e-07 |Time: 0.00s | rTime: 3.185e+00h | LR: 5.000e-03\n",
      "Epoch: 14400| It: 0| Loss: 2.985e-03| physLoss: 3.021e-04 | lINITIAL: 9.037e-04 | wLoss: 1.766e-03 | inLoss: 1.218e-05 | outLoss: 4.284e-07 |Time: 0.00s | rTime: 3.206e+00h | LR: 5.000e-03\n",
      "Epoch: 14500| It: 0| Loss: 2.949e-03| physLoss: 2.806e-04 | lINITIAL: 8.619e-04 | wLoss: 1.769e-03 | inLoss: 3.672e-05 | outLoss: 4.186e-07 |Time: 0.00s | rTime: 3.228e+00h | LR: 5.000e-03\n",
      "Epoch: 14600| It: 0| Loss: 2.694e-03| physLoss: 6.223e-05 | lINITIAL: 8.730e-04 | wLoss: 1.732e-03 | inLoss: 2.636e-05 | outLoss: 3.941e-07 |Time: 0.00s | rTime: 3.249e+00h | LR: 5.000e-03\n",
      "Epoch: 14700| It: 0| Loss: 2.718e-03| physLoss: 9.875e-05 | lINITIAL: 9.020e-04 | wLoss: 1.691e-03 | inLoss: 2.560e-05 | outLoss: 3.658e-07 |Time: 0.00s | rTime: 3.271e+00h | LR: 5.000e-03\n",
      "Epoch: 14800| It: 0| Loss: 2.837e-03| physLoss: 1.923e-04 | lINITIAL: 8.654e-04 | wLoss: 1.738e-03 | inLoss: 4.026e-05 | outLoss: 8.071e-07 |Time: 0.00s | rTime: 3.292e+00h | LR: 5.000e-03\n",
      "Epoch: 14900| It: 0| Loss: 2.726e-03| physLoss: 1.145e-04 | lINITIAL: 8.926e-04 | wLoss: 1.680e-03 | inLoss: 3.493e-05 | outLoss: 4.253e-06 |Time: 0.00s | rTime: 3.313e+00h | LR: 5.000e-03\n",
      "Epoch: 15000| It: 0| Loss: 3.099e-03| physLoss: 4.719e-04 | lINITIAL: 9.555e-04 | wLoss: 1.658e-03 | inLoss: 1.339e-05 | outLoss: 4.649e-07 |Time: 0.00s | rTime: 3.335e+00h | LR: 5.000e-03\n",
      "Epoch: 15100| It: 0| Loss: 2.612e-03| physLoss: 5.247e-05 | lINITIAL: 8.827e-04 | wLoss: 1.654e-03 | inLoss: 2.173e-05 | outLoss: 1.106e-06 |Time: 0.00s | rTime: 3.358e+00h | LR: 5.000e-03\n",
      "Epoch: 15200| It: 0| Loss: 2.666e-03| physLoss: 9.944e-05 | lINITIAL: 8.797e-04 | wLoss: 1.660e-03 | inLoss: 2.588e-05 | outLoss: 7.654e-07 |Time: 0.00s | rTime: 3.382e+00h | LR: 5.000e-03\n",
      "Epoch: 15300| It: 0| Loss: 3.024e-03| physLoss: 4.784e-04 | lINITIAL: 8.904e-04 | wLoss: 1.630e-03 | inLoss: 2.487e-05 | outLoss: 8.193e-07 |Time: 0.00s | rTime: 3.403e+00h | LR: 5.000e-03\n",
      "Epoch: 15400| It: 0| Loss: 2.574e-03| physLoss: 5.573e-05 | lINITIAL: 8.857e-04 | wLoss: 1.620e-03 | inLoss: 1.232e-05 | outLoss: 5.161e-07 |Time: 0.00s | rTime: 3.425e+00h | LR: 5.000e-03\n",
      "Epoch: 15500| It: 0| Loss: 2.784e-03| physLoss: 2.488e-04 | lINITIAL: 9.280e-04 | wLoss: 1.553e-03 | inLoss: 5.437e-05 | outLoss: 5.262e-07 |Time: 0.00s | rTime: 3.446e+00h | LR: 5.000e-03\n",
      "Epoch: 15600| It: 0| Loss: 2.595e-03| physLoss: 1.010e-04 | lINITIAL: 9.043e-04 | wLoss: 1.556e-03 | inLoss: 3.306e-05 | outLoss: 9.711e-07 |Time: 0.00s | rTime: 3.468e+00h | LR: 5.000e-03\n",
      "Epoch: 15700| It: 0| Loss: 2.577e-03| physLoss: 9.398e-05 | lINITIAL: 8.922e-04 | wLoss: 1.566e-03 | inLoss: 2.282e-05 | outLoss: 1.679e-06 |Time: 0.00s | rTime: 3.490e+00h | LR: 5.000e-03\n",
      "Epoch: 15800| It: 0| Loss: 2.493e-03| physLoss: 3.322e-05 | lINITIAL: 8.905e-04 | wLoss: 1.552e-03 | inLoss: 1.578e-05 | outLoss: 6.239e-07 |Time: 0.00s | rTime: 3.511e+00h | LR: 5.000e-03\n",
      "Epoch: 15900| It: 0| Loss: 2.707e-03| physLoss: 2.296e-04 | lINITIAL: 8.890e-04 | wLoss: 1.572e-03 | inLoss: 1.609e-05 | outLoss: 7.010e-07 |Time: 0.00s | rTime: 3.533e+00h | LR: 5.000e-03\n",
      "Epoch: 16000| It: 0| Loss: 2.727e-03| physLoss: 2.450e-04 | lINITIAL: 9.445e-04 | wLoss: 1.483e-03 | inLoss: 5.157e-05 | outLoss: 3.065e-06 |Time: 0.00s | rTime: 3.555e+00h | LR: 5.000e-03\n",
      "Epoch: 16100| It: 0| Loss: 2.560e-03| physLoss: 1.207e-04 | lINITIAL: 8.958e-04 | wLoss: 1.514e-03 | inLoss: 2.803e-05 | outLoss: 1.945e-06 |Time: 0.00s | rTime: 3.576e+00h | LR: 5.000e-03\n",
      "Epoch: 16200| It: 0| Loss: 2.523e-03| physLoss: 7.935e-05 | lINITIAL: 8.943e-04 | wLoss: 1.534e-03 | inLoss: 1.415e-05 | outLoss: 1.239e-06 |Time: 0.00s | rTime: 3.597e+00h | LR: 5.000e-03\n",
      "Epoch: 16300| It: 0| Loss: 2.504e-03| physLoss: 9.291e-05 | lINITIAL: 9.659e-04 | wLoss: 1.427e-03 | inLoss: 1.783e-05 | outLoss: 5.942e-07 |Time: 0.00s | rTime: 3.618e+00h | LR: 5.000e-03\n",
      "Epoch: 16400| It: 0| Loss: 2.474e-03| physLoss: 7.331e-05 | lINITIAL: 9.577e-04 | wLoss: 1.417e-03 | inLoss: 2.549e-05 | outLoss: 7.085e-07 |Time: 0.00s | rTime: 3.640e+00h | LR: 5.000e-03\n",
      "Epoch: 16500| It: 0| Loss: 2.764e-03| physLoss: 3.695e-04 | lINITIAL: 9.439e-04 | wLoss: 1.431e-03 | inLoss: 1.621e-05 | outLoss: 3.439e-06 |Time: 0.00s | rTime: 3.661e+00h | LR: 5.000e-03\n",
      "Epoch: 16600| It: 0| Loss: 2.468e-03| physLoss: 1.050e-04 | lINITIAL: 9.390e-04 | wLoss: 1.412e-03 | inLoss: 1.093e-05 | outLoss: 1.046e-06 |Time: 0.00s | rTime: 3.682e+00h | LR: 5.000e-03\n",
      "Epoch: 16700| It: 0| Loss: 2.683e-03| physLoss: 3.134e-04 | lINITIAL: 9.265e-04 | wLoss: 1.416e-03 | inLoss: 2.409e-05 | outLoss: 3.084e-06 |Time: 0.00s | rTime: 3.704e+00h | LR: 5.000e-03\n",
      "Epoch: 16800| It: 0| Loss: 2.585e-03| physLoss: 2.388e-04 | lINITIAL: 9.268e-04 | wLoss: 1.403e-03 | inLoss: 1.555e-05 | outLoss: 1.238e-06 |Time: 0.00s | rTime: 3.725e+00h | LR: 5.000e-03\n",
      "Epoch: 16900| It: 0| Loss: 2.652e-03| physLoss: 2.758e-04 | lINITIAL: 9.048e-04 | wLoss: 1.443e-03 | inLoss: 2.723e-05 | outLoss: 1.462e-06 |Time: 0.00s | rTime: 3.746e+00h | LR: 5.000e-03\n",
      "Epoch: 17000| It: 0| Loss: 2.468e-03| physLoss: 1.233e-04 | lINITIAL: 9.009e-04 | wLoss: 1.429e-03 | inLoss: 1.391e-05 | outLoss: 6.138e-07 |Time: 0.00s | rTime: 3.768e+00h | LR: 5.000e-03\n",
      "Epoch: 17100| It: 0| Loss: 2.396e-03| physLoss: 8.070e-05 | lINITIAL: 9.202e-04 | wLoss: 1.366e-03 | inLoss: 2.660e-05 | outLoss: 2.752e-06 |Time: 0.00s | rTime: 3.790e+00h | LR: 5.000e-03\n",
      "Epoch: 17200| It: 0| Loss: 2.360e-03| physLoss: 6.009e-05 | lINITIAL: 9.204e-04 | wLoss: 1.359e-03 | inLoss: 1.998e-05 | outLoss: 6.985e-07 |Time: 0.00s | rTime: 3.812e+00h | LR: 5.000e-03\n",
      "Epoch: 17300| It: 0| Loss: 2.697e-03| physLoss: 3.762e-04 | lINITIAL: 9.817e-04 | wLoss: 1.323e-03 | inLoss: 1.393e-05 | outLoss: 1.508e-06 |Time: 0.00s | rTime: 3.833e+00h | LR: 5.000e-03\n",
      "Epoch: 17400| It: 0| Loss: 2.553e-03| physLoss: 2.528e-04 | lINITIAL: 9.741e-04 | wLoss: 1.314e-03 | inLoss: 1.080e-05 | outLoss: 5.630e-07 |Time: 0.00s | rTime: 3.855e+00h | LR: 5.000e-03\n",
      "Epoch: 17500| It: 0| Loss: 2.573e-03| physLoss: 2.731e-04 | lINITIAL: 9.882e-04 | wLoss: 1.301e-03 | inLoss: 8.299e-06 | outLoss: 2.892e-06 |Time: 0.00s | rTime: 3.876e+00h | LR: 5.000e-03\n",
      "Epoch: 17600| It: 0| Loss: 2.475e-03| physLoss: 1.829e-04 | lINITIAL: 9.184e-04 | wLoss: 1.356e-03 | inLoss: 1.600e-05 | outLoss: 9.374e-07 |Time: 0.00s | rTime: 3.900e+00h | LR: 5.000e-03\n",
      "Epoch: 17700| It: 0| Loss: 2.412e-03| physLoss: 1.441e-04 | lINITIAL: 9.455e-04 | wLoss: 1.292e-03 | inLoss: 2.859e-05 | outLoss: 1.346e-06 |Time: 0.00s | rTime: 3.922e+00h | LR: 5.000e-03\n",
      "Epoch: 17800| It: 0| Loss: 2.350e-03| physLoss: 9.883e-05 | lINITIAL: 9.515e-04 | wLoss: 1.284e-03 | inLoss: 1.515e-05 | outLoss: 5.414e-07 |Time: 0.00s | rTime: 3.943e+00h | LR: 5.000e-03\n",
      "Epoch: 17900| It: 0| Loss: 2.456e-03| physLoss: 1.955e-04 | lINITIAL: 9.262e-04 | wLoss: 1.307e-03 | inLoss: 2.425e-05 | outLoss: 2.946e-06 |Time: 0.00s | rTime: 3.965e+00h | LR: 5.000e-03\n",
      "Epoch: 18000| It: 0| Loss: 2.422e-03| physLoss: 1.705e-04 | lINITIAL: 9.324e-04 | wLoss: 1.303e-03 | inLoss: 1.578e-05 | outLoss: 6.203e-07 |Time: 0.00s | rTime: 3.988e+00h | LR: 5.000e-03\n",
      "Epoch: 18100| It: 0| Loss: 2.526e-03| physLoss: 2.768e-04 | lINITIAL: 9.755e-04 | wLoss: 1.258e-03 | inLoss: 1.452e-05 | outLoss: 1.452e-06 |Time: 0.00s | rTime: 4.009e+00h | LR: 5.000e-03\n",
      "Epoch: 18200| It: 0| Loss: 2.380e-03| physLoss: 1.499e-04 | lINITIAL: 9.721e-04 | wLoss: 1.251e-03 | inLoss: 6.269e-06 | outLoss: 6.536e-07 |Time: 0.00s | rTime: 4.031e+00h | LR: 5.000e-03\n",
      "Epoch: 18300| It: 0| Loss: 2.323e-03| physLoss: 1.003e-04 | lINITIAL: 9.676e-04 | wLoss: 1.244e-03 | inLoss: 8.587e-06 | outLoss: 1.974e-06 |Time: 0.00s | rTime: 4.052e+00h | LR: 5.000e-03\n",
      "Epoch: 18400| It: 0| Loss: 2.429e-03| physLoss: 2.042e-04 | lINITIAL: 9.447e-04 | wLoss: 1.263e-03 | inLoss: 1.543e-05 | outLoss: 1.336e-06 |Time: 0.00s | rTime: 4.074e+00h | LR: 5.000e-03\n",
      "Epoch: 18500| It: 0| Loss: 2.333e-03| physLoss: 1.070e-04 | lINITIAL: 9.291e-04 | wLoss: 1.289e-03 | inLoss: 7.873e-06 | outLoss: 5.253e-07 |Time: 0.00s | rTime: 4.095e+00h | LR: 5.000e-03\n",
      "Epoch: 18600| It: 0| Loss: 2.575e-03| physLoss: 3.322e-04 | lINITIAL: 9.342e-04 | wLoss: 1.254e-03 | inLoss: 5.308e-05 | outLoss: 1.617e-06 |Time: 0.00s | rTime: 4.116e+00h | LR: 5.000e-03\n",
      "Epoch: 18700| It: 0| Loss: 2.482e-03| physLoss: 2.527e-04 | lINITIAL: 9.749e-04 | wLoss: 1.215e-03 | inLoss: 3.576e-05 | outLoss: 3.862e-06 |Time: 0.00s | rTime: 4.137e+00h | LR: 5.000e-03\n",
      "Epoch: 18800| It: 0| Loss: 2.325e-03| physLoss: 1.206e-04 | lINITIAL: 9.811e-04 | wLoss: 1.208e-03 | inLoss: 1.420e-05 | outLoss: 1.277e-06 |Time: 0.00s | rTime: 4.159e+00h | LR: 5.000e-03\n",
      "Epoch: 18900| It: 0| Loss: 2.346e-03| physLoss: 1.425e-04 | lINITIAL: 9.812e-04 | wLoss: 1.204e-03 | inLoss: 1.742e-05 | outLoss: 6.381e-07 |Time: 0.00s | rTime: 4.180e+00h | LR: 5.000e-03\n",
      "Epoch: 19000| It: 0| Loss: 2.310e-03| physLoss: 1.172e-04 | lINITIAL: 9.755e-04 | wLoss: 1.203e-03 | inLoss: 1.365e-05 | outLoss: 9.679e-07 |Time: 0.00s | rTime: 4.202e+00h | LR: 5.000e-03\n",
      "Epoch: 19100| It: 0| Loss: 2.242e-03| physLoss: 5.891e-05 | lINITIAL: 9.660e-04 | wLoss: 1.207e-03 | inLoss: 4.994e-06 | outLoss: 4.399e-06 |Time: 0.00s | rTime: 4.224e+00h | LR: 5.000e-03\n",
      "Epoch: 19200| It: 0| Loss: 2.221e-03| physLoss: 4.067e-05 | lINITIAL: 9.510e-04 | wLoss: 1.212e-03 | inLoss: 1.643e-05 | outLoss: 6.098e-07 |Time: 0.00s | rTime: 4.246e+00h | LR: 5.000e-03\n",
      "Epoch: 19300| It: 0| Loss: 2.282e-03| physLoss: 9.974e-05 | lINITIAL: 9.862e-04 | wLoss: 1.190e-03 | inLoss: 5.242e-06 | outLoss: 9.105e-07 |Time: 0.00s | rTime: 4.267e+00h | LR: 5.000e-03\n",
      "Epoch: 19400| It: 0| Loss: 2.217e-03| physLoss: 4.739e-05 | lINITIAL: 9.521e-04 | wLoss: 1.206e-03 | inLoss: 1.113e-05 | outLoss: 3.991e-07 |Time: 0.00s | rTime: 4.288e+00h | LR: 5.000e-03\n",
      "Epoch: 19500| It: 0| Loss: 2.362e-03| physLoss: 1.786e-04 | lINITIAL: 9.624e-04 | wLoss: 1.188e-03 | inLoss: 3.228e-05 | outLoss: 6.580e-07 |Time: 0.00s | rTime: 4.311e+00h | LR: 5.000e-03\n",
      "Epoch: 19600| It: 0| Loss: 2.287e-03| physLoss: 1.174e-04 | lINITIAL: 9.487e-04 | wLoss: 1.207e-03 | inLoss: 1.315e-05 | outLoss: 7.022e-07 |Time: 0.00s | rTime: 4.332e+00h | LR: 5.000e-03\n",
      "Epoch: 19700| It: 0| Loss: 2.215e-03| physLoss: 5.357e-05 | lINITIAL: 9.726e-04 | wLoss: 1.182e-03 | inLoss: 6.297e-06 | outLoss: 5.382e-07 |Time: 0.00s | rTime: 4.354e+00h | LR: 5.000e-03\n",
      "Epoch: 19800| It: 0| Loss: 2.234e-03| physLoss: 6.722e-05 | lINITIAL: 9.476e-04 | wLoss: 1.212e-03 | inLoss: 6.424e-06 | outLoss: 8.249e-07 |Time: 0.00s | rTime: 4.375e+00h | LR: 5.000e-03\n",
      "Epoch: 19900| It: 0| Loss: 2.395e-03| physLoss: 2.191e-04 | lINITIAL: 1.004e-03 | wLoss: 1.164e-03 | inLoss: 7.849e-06 | outLoss: 4.452e-07 |Time: 0.00s | rTime: 4.396e+00h | LR: 5.000e-03\n",
      "Epoch: 20000| It: 0| Loss: 2.275e-03| physLoss: 1.014e-04 | lINITIAL: 9.457e-04 | wLoss: 1.219e-03 | inLoss: 7.543e-06 | outLoss: 7.903e-07 |Time: 0.00s | rTime: 4.417e+00h | LR: 5.000e-03\n",
      "Epoch: 20100| It: 0| Loss: 2.398e-03| physLoss: 2.291e-04 | lINITIAL: 9.670e-04 | wLoss: 1.180e-03 | inLoss: 2.091e-05 | outLoss: 7.251e-07 |Time: 0.00s | rTime: 4.438e+00h | LR: 5.000e-03\n",
      "Epoch: 20200| It: 0| Loss: 2.322e-03| physLoss: 1.473e-04 | lINITIAL: 9.483e-04 | wLoss: 1.216e-03 | inLoss: 9.919e-06 | outLoss: 3.744e-07 |Time: 0.00s | rTime: 4.460e+00h | LR: 5.000e-03\n",
      "Epoch: 20300| It: 0| Loss: 2.261e-03| physLoss: 1.036e-04 | lINITIAL: 9.524e-04 | wLoss: 1.196e-03 | inLoss: 8.585e-06 | outLoss: 4.495e-07 |Time: 0.00s | rTime: 4.481e+00h | LR: 5.000e-03\n",
      "Epoch: 20400| It: 0| Loss: 2.353e-03| physLoss: 1.899e-04 | lINITIAL: 9.723e-04 | wLoss: 1.174e-03 | inLoss: 1.505e-05 | outLoss: 2.122e-06 |Time: 0.00s | rTime: 4.502e+00h | LR: 5.000e-03\n",
      "Epoch: 20500| It: 0| Loss: 2.409e-03| physLoss: 2.450e-04 | lINITIAL: 9.808e-04 | wLoss: 1.165e-03 | inLoss: 1.783e-05 | outLoss: 3.068e-07 |Time: 0.00s | rTime: 4.524e+00h | LR: 5.000e-03\n",
      "Epoch: 20600| It: 0| Loss: 2.329e-03| physLoss: 1.724e-04 | lINITIAL: 9.697e-04 | wLoss: 1.172e-03 | inLoss: 1.364e-05 | outLoss: 1.095e-06 |Time: 0.00s | rTime: 4.545e+00h | LR: 5.000e-03\n",
      "Epoch: 20700| It: 0| Loss: 2.566e-03| physLoss: 3.687e-04 | lINITIAL: 9.518e-04 | wLoss: 1.219e-03 | inLoss: 2.556e-05 | outLoss: 2.911e-07 |Time: 0.00s | rTime: 4.567e+00h | LR: 5.000e-03\n",
      "Epoch: 20800| It: 0| Loss: 2.256e-03| physLoss: 7.825e-05 | lINITIAL: 1.003e-03 | wLoss: 1.152e-03 | inLoss: 1.532e-05 | outLoss: 7.708e-06 |Time: 0.00s | rTime: 4.588e+00h | LR: 5.000e-03\n",
      "Epoch: 20900| It: 0| Loss: 2.262e-03| physLoss: 9.984e-05 | lINITIAL: 9.571e-04 | wLoss: 1.195e-03 | inLoss: 1.004e-05 | outLoss: 2.898e-07 |Time: 0.00s | rTime: 4.610e+00h | LR: 5.000e-03\n",
      "Epoch: 21000| It: 0| Loss: 2.313e-03| physLoss: 1.588e-04 | lINITIAL: 9.767e-04 | wLoss: 1.170e-03 | inLoss: 7.765e-06 | outLoss: 3.854e-07 |Time: 0.00s | rTime: 4.631e+00h | LR: 5.000e-03\n",
      "Epoch: 21100| It: 0| Loss: 2.310e-03| physLoss: 1.619e-04 | lINITIAL: 9.901e-04 | wLoss: 1.148e-03 | inLoss: 1.048e-05 | outLoss: 2.705e-07 |Time: 0.00s | rTime: 4.653e+00h | LR: 5.000e-03\n",
      "Epoch: 21200| It: 0| Loss: 2.418e-03| physLoss: 1.957e-04 | lINITIAL: 9.478e-04 | wLoss: 1.251e-03 | inLoss: 2.043e-05 | outLoss: 3.222e-06 |Time: 0.00s | rTime: 4.674e+00h | LR: 5.000e-03\n",
      "Epoch: 21300| It: 0| Loss: 2.258e-03| physLoss: 8.928e-05 | lINITIAL: 9.471e-04 | wLoss: 1.208e-03 | inLoss: 1.312e-05 | outLoss: 2.549e-07 |Time: 0.00s | rTime: 4.695e+00h | LR: 5.000e-03\n",
      "Epoch: 21400| It: 0| Loss: 2.221e-03| physLoss: 5.752e-05 | lINITIAL: 1.009e-03 | wLoss: 1.144e-03 | inLoss: 4.131e-06 | outLoss: 6.076e-06 |Time: 0.00s | rTime: 4.717e+00h | LR: 5.000e-03\n",
      "Epoch: 21500| It: 0| Loss: 2.242e-03| physLoss: 9.166e-05 | lINITIAL: 9.856e-04 | wLoss: 1.155e-03 | inLoss: 6.508e-06 | outLoss: 3.271e-06 |Time: 0.00s | rTime: 4.738e+00h | LR: 5.000e-03\n",
      "Epoch: 21600| It: 0| Loss: 2.181e-03| physLoss: 4.361e-05 | lINITIAL: 9.896e-04 | wLoss: 1.142e-03 | inLoss: 5.749e-06 | outLoss: 4.457e-07 |Time: 0.00s | rTime: 4.760e+00h | LR: 5.000e-03\n",
      "Epoch: 21700| It: 0| Loss: 2.190e-03| physLoss: 5.176e-05 | lINITIAL: 9.768e-04 | wLoss: 1.155e-03 | inLoss: 6.275e-06 | outLoss: 1.913e-07 |Time: 0.00s | rTime: 4.783e+00h | LR: 5.000e-03\n",
      "Epoch: 21800| It: 0| Loss: 2.262e-03| physLoss: 1.207e-04 | lINITIAL: 9.886e-04 | wLoss: 1.144e-03 | inLoss: 8.482e-06 | outLoss: 4.137e-07 |Time: 0.00s | rTime: 4.805e+00h | LR: 5.000e-03\n",
      "Epoch: 21900| It: 0| Loss: 2.346e-03| physLoss: 1.945e-04 | lINITIAL: 9.779e-04 | wLoss: 1.151e-03 | inLoss: 2.269e-05 | outLoss: 2.380e-07 |Time: 0.00s | rTime: 4.827e+00h | LR: 5.000e-03\n",
      "Epoch: 22000| It: 0| Loss: 2.230e-03| physLoss: 9.391e-05 | lINITIAL: 9.988e-04 | wLoss: 1.132e-03 | inLoss: 5.658e-06 | outLoss: 1.794e-07 |Time: 0.00s | rTime: 4.849e+00h | LR: 5.000e-03\n",
      "Epoch: 22100| It: 0| Loss: 2.283e-03| physLoss: 1.420e-04 | lINITIAL: 9.873e-04 | wLoss: 1.140e-03 | inLoss: 1.367e-05 | outLoss: 2.354e-07 |Time: 0.00s | rTime: 4.871e+00h | LR: 5.000e-03\n",
      "Epoch: 22200| It: 0| Loss: 2.304e-03| physLoss: 1.569e-04 | lINITIAL: 9.718e-04 | wLoss: 1.158e-03 | inLoss: 1.729e-05 | outLoss: 1.884e-07 |Time: 0.00s | rTime: 4.892e+00h | LR: 5.000e-03\n",
      "Epoch: 22300| It: 0| Loss: 2.289e-03| physLoss: 1.463e-04 | lINITIAL: 1.024e-03 | wLoss: 1.115e-03 | inLoss: 3.286e-06 | outLoss: 1.818e-07 |Time: 0.00s | rTime: 4.915e+00h | LR: 5.000e-03\n",
      "Epoch: 22400| It: 0| Loss: 2.374e-03| physLoss: 2.241e-04 | lINITIAL: 9.753e-04 | wLoss: 1.154e-03 | inLoss: 2.030e-05 | outLoss: 1.813e-07 |Time: 0.00s | rTime: 4.936e+00h | LR: 5.000e-03\n",
      "Epoch: 22500| It: 0| Loss: 2.158e-03| physLoss: 2.657e-05 | lINITIAL: 1.012e-03 | wLoss: 1.115e-03 | inLoss: 4.996e-06 | outLoss: 1.909e-07 |Time: 0.00s | rTime: 4.961e+00h | LR: 5.000e-03\n",
      "Epoch: 22600| It: 0| Loss: 2.177e-03| physLoss: 4.836e-05 | lINITIAL: 1.002e-03 | wLoss: 1.122e-03 | inLoss: 4.023e-06 | outLoss: 2.979e-07 |Time: 0.00s | rTime: 4.983e+00h | LR: 5.000e-03\n",
      "Epoch: 22700| It: 0| Loss: 2.222e-03| physLoss: 8.491e-05 | lINITIAL: 9.793e-04 | wLoss: 1.150e-03 | inLoss: 7.207e-06 | outLoss: 1.805e-07 |Time: 0.00s | rTime: 5.005e+00h | LR: 5.000e-03\n",
      "Epoch: 22800| It: 0| Loss: 2.296e-03| physLoss: 1.453e-04 | lINITIAL: 9.751e-04 | wLoss: 1.166e-03 | inLoss: 7.752e-06 | outLoss: 2.111e-06 |Time: 0.00s | rTime: 5.028e+00h | LR: 5.000e-03\n",
      "Epoch: 22900| It: 0| Loss: 2.357e-03| physLoss: 1.834e-04 | lINITIAL: 1.065e-03 | wLoss: 1.097e-03 | inLoss: 1.140e-05 | outLoss: 9.704e-07 |Time: 0.00s | rTime: 5.049e+00h | LR: 5.000e-03\n"
     ]
    }
   ],
   "source": [
    "N_f = 10000\n",
    "Nu1 = 200\n",
    "\n",
    "\n",
    "\n",
    "model.train(collo[::10,:], inlet, outlet, wall, initial, tf_iter=40000,   tf_iter2=100, newton_iter2=1500)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final loss loss_OUTLET: 1.530280e-06\n",
      "Final loss loss_INLET: 3.779459e-05\n",
      "Final loss loss_WALL: 1.845452e-03\n",
      "Final loss loss_Initial: 8.548543e-04\n",
      "Final loss loss_Phys: 2.069821e-04\n"
     ]
    }
   ],
   "source": [
    "key = \"noslipwall_02_Z0slice_2D_innerdomain\" \n",
    "\n",
    "test_data_innerdomain = get_testing_dataset(path   , part=key)\n",
    "model.plot_loss_history(model.dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting nn solution\n",
      "\n",
      " Relative L2 ERROR:\n",
      "domain U velocity  : 9.972e+01 %\n",
      "domain V velocity  : 1.177e+03 %\n",
      "domain P Pressure  : 1.112e+02 %\n",
      "\n",
      " Relative l1 error\n",
      "domain U velocity  : 9.925e+01 %\n",
      "domain V velocity  : 1.727e+03 %\n",
      "domain P Pressure  : 1.002e+02 %\n",
      "/media/afrah2/MyWork/files2023/SK/dwPINNs/utilities.py:653: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n",
      "/media/afrah2/MyWork/files2023/SK/dwPINNs/utilities.py:653: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n",
      "/media/afrah2/MyWork/files2023/SK/dwPINNs/utilities.py:653: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "plot_result2(model , path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1.6908718], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([2.7705307], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
